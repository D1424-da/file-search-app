#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
100%ä»•æ§˜é©åˆåº¦é”æˆ - å³åº§å®Ÿè£…ç‰ˆ
ç¾åœ¨åˆ©ç”¨å¯èƒ½ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§æœ€é«˜é€Ÿå…¨æ–‡æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ å®Œæˆ
Wordãƒ»Excelãƒ»PDFãƒ»ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ».tifç”»åƒãƒ•ã‚¡ã‚¤ãƒ«(OCR) + è¶…é«˜é€Ÿãƒ©ã‚¤ãƒ–å…¨æ–‡æ¤œç´¢ã‚¢ãƒ—ãƒª å®Œå…¨é©åˆ
è‡ªå‹•ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ©Ÿèƒ½ + ä¸¦åˆ—å‡¦ç†æœ€é©åŒ–ç‰ˆ
"""

# åŸºæœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆé«˜é€Ÿèµ·å‹•å„ªå…ˆé †ï¼‰
import time
import sys
import os
import threading
import concurrent.futures
from concurrent.futures import ThreadPoolExecutor, as_completed
import subprocess
import sqlite3

import hashlib
import json
import logging
import pickle
import platform
from pathlib import Path
from datetime import datetime
from typing import Any, Dict, List, Optional

# GUIãƒ»ãã®ä»–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆé…å»¶ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¯¾å¿œï¼‰
import tkinter as tk
from tkinter import filedialog, messagebox, ttk
import tkinter.filedialog  # æ˜ç¤ºçš„ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import asyncio
import gzip
import mmap
import queue
import re
import unicodedata
import xml.etree.ElementTree as ET
import zipfile

# å¤–éƒ¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆæ¡ä»¶ä»˜ãã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼‰
try:
    import psutil
except ImportError:
    psutil = None

try:
    import fitz  # PyMuPDF
except ImportError:
    fitz = None

class ProgressTracker:
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²æ—ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°"""
    def __init__(self):
        self.reset()
        self._lock = threading.Lock()
        
    def reset(self):
        """é€²æ—ã‚’ãƒªã‚»ãƒƒãƒˆ"""
        with getattr(self, '_lock', threading.Lock()):
            self.total_files = 0
            self.processed_files = 0
            self.successful_files = 0
            self.error_files = 0
            self.current_file = ""
            self.start_time = time.time()
            self.last_update_time = time.time()
            self.category_progress = {"light": 0, "medium": 0, "heavy": 0}
            self.category_totals = {"light": 0, "medium": 0, "heavy": 0}
            self.processing_speed = 0.0
            self.estimated_remaining_time = 0.0
            
    def set_total_files(self, total: int, category_breakdown: dict = None):
        """ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’è¨­å®š"""
        with self._lock:
            self.total_files = total
            if category_breakdown:
                self.category_totals.update(category_breakdown)
                
    def update_progress(self, current_file: str = "", category: str = "", success: bool = True):
        """é€²æ—ã‚’æ›´æ–°"""
        with self._lock:
            if success:
                self.successful_files += 1
            else:
                self.error_files += 1
                
            self.processed_files += 1
            if current_file:
                self.current_file = current_file
                
            if category:
                self.category_progress[category] = self.category_progress.get(category, 0) + 1
                
            # å‡¦ç†é€Ÿåº¦è¨ˆç®—
            current_time = time.time()
            elapsed = current_time - self.start_time
            if elapsed > 0:
                self.processing_speed = self.processed_files / elapsed
                
                # æ®‹ã‚Šæ™‚é–“æ¨å®š
                remaining_files = self.total_files - self.processed_files
                if self.processing_speed > 0:
                    self.estimated_remaining_time = remaining_files / self.processing_speed
            
            self.last_update_time = current_time
            
    def get_progress_info(self) -> dict:
        """é€²æ—æƒ…å ±ã‚’å–å¾—"""
        with self._lock:
            progress_percent = (self.processed_files / self.total_files * 100) if self.total_files > 0 else 0
            
            return {
                'total_files': self.total_files,
                'processed_files': self.processed_files,
                'successful_files': self.successful_files,
                'error_files': self.error_files,
                'current_file': self.current_file,
                'progress_percent': progress_percent,
                'processing_speed': self.processing_speed,
                'estimated_remaining_time': self.estimated_remaining_time,
                'category_progress': self.category_progress.copy(),
                'category_totals': self.category_totals.copy(),
                'elapsed_time': time.time() - self.start_time
            }

try:
    import openpyxl
except ImportError:
    openpyxl = None

try:
    import docx
except ImportError:
    docx = None

try:
    import xlrd  # å¤ã„å½¢å¼ã®Excelãƒ•ã‚¡ã‚¤ãƒ«(.xls)ç”¨
except ImportError:
    xlrd = None

try:
    import docx2txt  # å¤ã„å½¢å¼ã®Wordãƒ•ã‚¡ã‚¤ãƒ«(.doc)ç”¨
except ImportError:
    docx2txt = None

try:
    import olefile  # å¤ã„Officeãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã®è§£æç”¨
except ImportError:
    olefile = None

try:
    import chardet
except ImportError:
    chardet = None

try:
    from PIL import Image
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False
    Image = None

# UTF-8å¯¾å¿œã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°ï¼ˆæ—¥æœ¬èªå¯¾å¿œå¼·åŒ–ï¼‰
def safe_truncate_utf8(text: str, max_length: int) -> str:
    """UTF-8æ–‡å­—åˆ—ã‚’å®‰å…¨ã«åˆ‡ã‚Šå–ã‚‹ï¼ˆæ—¥æœ¬èªãƒ»ãƒãƒ«ãƒãƒã‚¤ãƒˆæ–‡å­—å¯¾å¿œï¼‰"""
    if not text or len(text) <= max_length:
        return text
    
    # æ–‡å­—å¢ƒç•Œã§å®‰å…¨ã«åˆ‡ã‚Šå–ã‚Š
    truncated = text[:max_length]
    
    # UTF-8ãƒã‚¤ãƒˆåˆ—ã¨ã—ã¦æ­£å¸¸ã‹ãƒã‚§ãƒƒã‚¯
    try:
        truncated.encode('utf-8')
        return truncated
    except UnicodeEncodeError:
        # æœ€å¾Œã®æ–‡å­—ãŒä¸å®Œå…¨ãªå ´åˆã€1æ–‡å­—ãšã¤å‰Šã£ã¦ã„ã
        for i in range(1, min(4, max_length) + 1):  # æœ€å¤§4ãƒã‚¤ãƒˆã¾ã§ãƒã‚§ãƒƒã‚¯
            try:
                safe_text = text[:max_length - i]
                safe_text.encode('utf-8')
                return safe_text
            except UnicodeEncodeError:
                continue
        
        # ãã‚Œã§ã‚‚ãƒ€ãƒ¡ãªã‚‰ç©ºæ–‡å­—åˆ—
        return ""


def normalize_extracted_text(text: str, max_length: int = 100000) -> str:
    """
    æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’æ­£è¦åŒ–ï¼ˆãƒã‚¤ã‚ºé™¤å»ãƒ»èª­ã¿ã‚„ã™ã•å‘ä¸Šï¼‰
    
    Args:
        text: æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        max_length: æœ€å¤§æ–‡å­—æ•°
        
    Returns:
        æ­£è¦åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
    """
    if not text:
        return ""
    
    # åˆ¶å¾¡æ–‡å­—ã‚’é™¤å»ï¼ˆã‚¿ãƒ–ãƒ»æ”¹è¡Œãƒ»ã‚¹ãƒšãƒ¼ã‚¹ã¯ä¿æŒï¼‰
    import re
    cleaned = ''.join(char for char in text if char.isprintable() or char in '\t\n\r ')
    
    # é€£ç¶šã™ã‚‹ç©ºç™½ã‚’1ã¤ã«çµ±ä¸€
    cleaned = re.sub(r'[ \t]+', ' ', cleaned)
    
    # é€£ç¶šã™ã‚‹æ”¹è¡Œã‚’æœ€å¤§2ã¤ã¾ã§ã«åˆ¶é™
    cleaned = re.sub(r'\n{3,}', '\n\n', cleaned)
    
    # è¡Œé ­ãƒ»è¡Œæœ«ã®ç©ºç™½ã‚’å‰Šé™¤
    lines = [line.strip() for line in cleaned.split('\n')]
    cleaned = '\n'.join(line for line in lines if line)
    
    # å…¨ä½“ã®å‰å¾Œã®ç©ºç™½ã‚’å‰Šé™¤
    cleaned = cleaned.strip()
    
    # æœ€å¤§æ–‡å­—æ•°ã§åˆ‡ã‚Šè©°ã‚
    if len(cleaned) > max_length:
        cleaned = safe_truncate_utf8(cleaned, max_length)
    
    return cleaned


try:
    import pytesseract
    TESSERACT_AVAILABLE = True
except ImportError:
    TESSERACT_AVAILABLE = False
    pytesseract = None

try:
    import cv2
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False
    cv2 = None


# å¿…è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®é«˜é€Ÿãƒã‚§ãƒƒã‚¯æ©Ÿèƒ½ï¼ˆèµ·å‹•æ™‚é–“çŸ­ç¸®ç‰ˆï¼‰
def load_auto_install_settings():
    """è‡ªå‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«è¨­å®šã‚’èª­ã¿è¾¼ã¿"""
    try:
        # EXEåŒ–å¯¾å¿œ: å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’åŸºæº–ã«ã™ã‚‹
        if getattr(sys, 'frozen', False):
            base_path = Path(sys.executable).parent
        else:
            base_path = Path(__file__).parent
        
        settings_path = base_path / "config" / "auto_install_settings.json"
        if settings_path.exists():
            with open(settings_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
            return {
                "auto_install": {
                    "enabled": True,
                    "ask_permission": True,
                    "python_libraries": {"auto_install": True},
                    "tesseract_ocr": {"auto_install": True, "ask_permission": True}
                }
            }
    except Exception as e:
        print(f"âš ï¸ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return {"auto_install": {"enabled": False}}


def ensure_required_libraries():
    """å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’è¶…é«˜é€Ÿãƒã‚§ãƒƒã‚¯ãƒ»è‡ªå‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆèµ·å‹•é…å»¶ã‚¼ãƒ­ç‰ˆï¼‰"""
    # è¨­å®šã‚’èª­ã¿è¾¼ã¿
    settings = load_auto_install_settings()
    auto_install_enabled = settings.get("auto_install", {}).get("enabled", True)
    
    # æ—¢ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ¸ˆã¿ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªçŠ¶æ…‹ã‚’å³åº§ã«ç¢ºèª
    installed_libraries = []
    missing_libraries = []
    
    # äº‹å‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆã•ã‚ŒãŸãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®çŠ¶æ…‹ç¢ºèªï¼ˆ0.001ç§’ä»¥å†…ï¼‰
    library_checks = [
        ('psutil', psutil is not None),
        ('PyMuPDF', fitz is not None),
        ('openpyxl', openpyxl is not None),
        ('python-docx', docx is not None),
        ('xlrd', xlrd is not None),
        ('docx2txt', docx2txt is not None),
        ('olefile', olefile is not None),
        ('chardet', chardet is not None),
        ('Pillow', PIL_AVAILABLE),
        ('pytesseract', TESSERACT_AVAILABLE),
        ('opencv-python', CV2_AVAILABLE)
    ]
    
    # é«˜é€ŸçŠ¶æ…‹åˆ¤å®š
    for lib_name, is_available in library_checks:
        if is_available:
            installed_libraries.append(lib_name)
        else:
            missing_libraries.append(lib_name)
    
    # è‡ªå‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒæœ‰åŠ¹ãªå ´åˆã®ã¿å®Ÿè¡Œ
    if auto_install_enabled and missing_libraries:
        print(f"ğŸ“¦ {len(missing_libraries)}å€‹ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’éåŒæœŸã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...")
        
        def background_install():
            """ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰éåŒæœŸã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"""
            for lib in missing_libraries:
                print(f"ğŸ“¦ {lib} ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...")
                
                pip_cmd = [sys.executable, '-m', 'pip', 'install', lib, 
                          '--quiet', '--disable-pip-version-check', '--no-warn-script-location']
                result, error = safe_subprocess_run(pip_cmd, f"{lib}ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«", timeout=60)
                
                if result and result.returncode == 0:
                    print(f"âœ… {lib} ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†")
                    # ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¾Œã«ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å†èª­ã¿è¾¼ã¿ã‚’è©¦è¡Œ
                    try_reimport_library(lib)
                elif error:
                    print(f"âš ï¸ {lib} ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¤±æ•—ï¼ˆæ©Ÿèƒ½ã¯åˆ¶é™ã•ã‚Œã¾ã™ï¼‰: {error}")
                elif result:
                    print(f"âš ï¸ {lib} ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¤±æ•—ï¼ˆæ©Ÿèƒ½ã¯åˆ¶é™ã•ã‚Œã¾ã™ï¼‰ - çµ‚äº†ã‚³ãƒ¼ãƒ‰: {result.returncode}")
                    if result.stderr:
                        error_msg = result.stderr[:200] if len(result.stderr) > 200 else result.stderr
                        print(f"   è©³ç´°: {error_msg}...")
                else:
                    print(f"âš ï¸ {lib} ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­ã«äºˆæœŸã—ãªã„å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸ")
        
        # ãƒ‡ãƒ¼ãƒ¢ãƒ³ã‚¹ãƒ¬ãƒƒãƒ‰ã§éåŒæœŸå®Ÿè¡Œï¼ˆèµ·å‹•ã‚’å¾…æ©Ÿã•ã›ãªã„ï¼‰
        threading.Thread(target=background_install, daemon=True).start()
    elif not auto_install_enabled and missing_libraries:
        print(f"â„¹ï¸ {len(missing_libraries)}å€‹ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒä¸è¶³ã—ã¦ã„ã¾ã™ãŒã€è‡ªå‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¯ç„¡åŠ¹ã§ã™")
        print(f"   ä¸è¶³ãƒ©ã‚¤ãƒ–ãƒ©ãƒª: {', '.join(missing_libraries)}")
    else:
        print(f"âœ… å…¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªåˆ©ç”¨å¯èƒ½ ({len(installed_libraries)}å€‹) - æœ€å¤§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¢ãƒ¼ãƒ‰")
    
    return installed_libraries, missing_libraries


def try_reimport_library(lib_name):
    """ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®å‹•çš„å†ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è©¦è¡Œ"""
    global PIL_AVAILABLE, TESSERACT_AVAILABLE, CV2_AVAILABLE, psutil, fitz, openpyxl, docx, xlrd, docx2txt, olefile, chardet
    
    try:
        if lib_name == 'Pillow':
            from PIL import Image
            PIL_AVAILABLE = True
            print(f"ğŸ”„ {lib_name} å‹•çš„èª­ã¿è¾¼ã¿æˆåŠŸ - OCRç”»åƒå‡¦ç†æ©Ÿèƒ½ãŒåˆ©ç”¨å¯èƒ½ã«ãªã‚Šã¾ã—ãŸ")
        elif lib_name == 'pytesseract':
            import pytesseract
            TESSERACT_AVAILABLE = True
            print(f"ğŸ”„ {lib_name} å‹•çš„èª­ã¿è¾¼ã¿æˆåŠŸ - OCRãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨å¯èƒ½ã«ãªã‚Šã¾ã—ãŸ")
        elif lib_name == 'opencv-python':
            import cv2
            CV2_AVAILABLE = True
            print(f"ğŸ”„ {lib_name} å‹•çš„èª­ã¿è¾¼ã¿æˆåŠŸ - ç”»åƒå‰å‡¦ç†æ©Ÿèƒ½ãŒåˆ©ç”¨å¯èƒ½ã«ãªã‚Šã¾ã—ãŸ")
        elif lib_name == 'psutil':
            import psutil
            print(f"ğŸ”„ {lib_name} å‹•çš„èª­ã¿è¾¼ã¿æˆåŠŸ - ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–æ©Ÿèƒ½ãŒåˆ©ç”¨å¯èƒ½ã«ãªã‚Šã¾ã—ãŸ")
        elif lib_name == 'PyMuPDF':
            import fitz
            print(f"ğŸ”„ {lib_name} å‹•çš„èª­ã¿è¾¼ã¿æˆåŠŸ - PDFå‡¦ç†æ©Ÿèƒ½ãŒåˆ©ç”¨å¯èƒ½ã«ãªã‚Šã¾ã—ãŸ")
        elif lib_name == 'openpyxl':
            import openpyxl
            print(f"ğŸ”„ {lib_name} å‹•çš„èª­ã¿è¾¼ã¿æˆåŠŸ - Excelå‡¦ç†æ©Ÿèƒ½ãŒåˆ©ç”¨å¯èƒ½ã«ãªã‚Šã¾ã—ãŸ")
        elif lib_name == 'python-docx':
            import docx
            print(f"ğŸ”„ {lib_name} å‹•çš„èª­ã¿è¾¼ã¿æˆåŠŸ - Wordå‡¦ç†æ©Ÿèƒ½ãŒåˆ©ç”¨å¯èƒ½ã«ãªã‚Šã¾ã—ãŸ")
        elif lib_name == 'xlrd':
            import xlrd
            print(f"ğŸ”„ {lib_name} å‹•çš„èª­ã¿è¾¼ã¿æˆåŠŸ - å¤ã„å½¢å¼ã®Excelå‡¦ç†æ©Ÿèƒ½ãŒåˆ©ç”¨å¯èƒ½ã«ãªã‚Šã¾ã—ãŸ")
        elif lib_name == 'docx2txt':
            import docx2txt
            print(f"ğŸ”„ {lib_name} å‹•çš„èª­ã¿è¾¼ã¿æˆåŠŸ - å¤ã„å½¢å¼ã®Wordå‡¦ç†æ©Ÿèƒ½ãŒåˆ©ç”¨å¯èƒ½ã«ãªã‚Šã¾ã—ãŸ")
        elif lib_name == 'olefile':
            import olefile
            print(f"ğŸ”„ {lib_name} å‹•çš„èª­ã¿è¾¼ã¿æˆåŠŸ - å¤ã„Officeå½¢å¼è§£ææ©Ÿèƒ½ãŒåˆ©ç”¨å¯èƒ½ã«ãªã‚Šã¾ã—ãŸ")
        elif lib_name == 'chardet':
            import chardet
            print(f"ğŸ”„ {lib_name} å‹•çš„èª­ã¿è¾¼ã¿æˆåŠŸ - æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¤œå‡ºæ©Ÿèƒ½ãŒåˆ©ç”¨å¯èƒ½ã«ãªã‚Šã¾ã—ãŸ")
        
    except ImportError as e:
        print(f"ğŸ”„ {lib_name} å‹•çš„èª­ã¿è¾¼ã¿å¤±æ•— - ã¾ã ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå®Œäº†ã—ã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™: {e}")
    except Exception as e:
        print(f"ğŸ”„ {lib_name} å‹•çš„èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")


def safe_subprocess_run(cmd, description="ã‚³ãƒãƒ³ãƒ‰", timeout=30, **kwargs):
    """ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚»ãƒ¼ãƒ•ãªsubprocesså®Ÿè¡Œ"""
    try:
        # Windowsç’°å¢ƒã§ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å•é¡Œã‚’å›é¿
        result = subprocess.run(
            cmd, 
            capture_output=True, 
            text=True, 
            timeout=timeout,
            encoding='utf-8',
            errors='ignore',  # ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼ã‚’ç„¡è¦–
            **kwargs
        )
        return result, None
        
    except subprocess.TimeoutExpired:
        error_msg = f"{description}ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆ{timeout}ç§’ï¼‰ã—ã¾ã—ãŸ"
        return None, error_msg
        
    except FileNotFoundError:
        error_msg = f"{description}ã®ã‚³ãƒãƒ³ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
        return None, error_msg
        
    except UnicodeDecodeError as e:
        error_msg = f"{description}ã®å‡ºåŠ›ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {str(e)[:100]}..."
        return None, error_msg
        
    except Exception as e:
        error_msg = f"{description}å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)[:100]}..."
        return None, error_msg


def auto_install_tesseract_engine():
    """Tesseractã‚¨ãƒ³ã‚¸ãƒ³ã®è‡ªå‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆWindowsï¼‰"""
    print("ğŸ” Tesseractã‚¨ãƒ³ã‚¸ãƒ³ã®è‡ªå‹•ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚’ç¢ºèªä¸­...")
    
    try:
        # æ—¢ã«TesseractãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if TESSERACT_AVAILABLE:
            try:
                pytesseract.get_tesseract_version()
                print("âœ… Tesseractã‚¨ãƒ³ã‚¸ãƒ³ã¯æ—¢ã«åˆ©ç”¨å¯èƒ½ã§ã™")
                return True
            except:
                pass
        
        # Windowsç’°å¢ƒã§ã®Tesseractè‡ªå‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã®è©¦è¡Œ
        import platform
        if platform.system() == "Windows":
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«è¨±å¯ã‚’æ±‚ã‚ã‚‹
            if ask_user_permission_for_install():
                return auto_install_tesseract_windows()
            else:
                print("â„¹ï¸ Tesseractè‡ªå‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ")
                return False
        else:
            print("â„¹ï¸ Windowsä»¥å¤–ã®ç’°å¢ƒã§ã¯æ‰‹å‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå¿…è¦ã§ã™")
            return False
            
    except Exception as e:
        print(f"âš ï¸ Tesseractè‡ªå‹•ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def ask_user_permission_for_install():
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«è¨±å¯ã‚’æ±‚ã‚ã‚‹"""
    try:
        import tkinter as tk
        from tkinter import messagebox
        
        # ä¸€æ™‚çš„ãªãƒ«ãƒ¼ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’ä½œæˆï¼ˆéè¡¨ç¤ºï¼‰
        root = tk.Tk()
        root.withdraw()
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ç¢ºèª
        result = messagebox.askyesno(
            "OCRæ©Ÿèƒ½ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—",
            "OCRæ©Ÿèƒ½ï¼ˆç”»åƒå†…ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ï¼‰ã‚’åˆ©ç”¨ã™ã‚‹ãŸã‚ã€\n"
            "Tesseract OCRã‚¨ãƒ³ã‚¸ãƒ³ã®è‡ªå‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚’è©¦è¡Œã—ã¾ã™ã‹ï¼Ÿ\n\n"
            "ãƒ»Windows Package Manager (winget) ã¾ãŸã¯ Chocolatey ã‚’ä½¿ç”¨\n"
            "ãƒ»ç®¡ç†è€…æ¨©é™ãŒå¿…è¦ãªå ´åˆãŒã‚ã‚Šã¾ã™\n"
            "ãƒ»ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ™‚é–“: 1-3åˆ†ç¨‹åº¦\n\n"
            "æ‰‹å‹•ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã™ã€‚",
            icon='question'
        )
        
        root.destroy()
        return result
        
    except Exception as e:
        print(f"âš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ç¢ºèªãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
        # GUI ãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã§ç¢ºèª
        try:
            response = input("Tesseract OCRã‚¨ãƒ³ã‚¸ãƒ³ã‚’è‡ªå‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã‹ï¼Ÿ (y/N): ").lower()
            return response in ['y', 'yes', 'ã¯ã„']
        except:
            return False


def auto_install_tesseract_windows():
    """Windowsç”¨Tesseractè‡ªå‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"""
    # 1. Windowsãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼(winget)ã§ã®è©¦è¡Œ
    print("ğŸ“¦ wingetã§Tesseractã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚’è©¦è¡Œä¸­...")
    
    winget_cmd = ['winget', 'install', '--id=UB-Mannheim.TesseractOCR', '--silent', '--accept-source-agreements']
    result, error = safe_subprocess_run(winget_cmd, "winget Tesseractã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«", timeout=120)
    
    if result and result.returncode == 0:
        print("âœ… wingetçµŒç”±ã§Tesseractã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†")
        if setup_tesseract_path():
            return True
    elif error:
        print(f"âš ï¸ {error}")
    elif result:
        print(f"âš ï¸ wingetã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¤±æ•— (çµ‚äº†ã‚³ãƒ¼ãƒ‰: {result.returncode})")
        if result.stderr:
            error_msg = result.stderr[:200] if len(result.stderr) > 200 else result.stderr
            print(f"   ã‚¨ãƒ©ãƒ¼: {error_msg}...")
    
    # 2. Chocolateyã§ã®è©¦è¡Œ
    print("ğŸ“¦ Chocolateyã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚’è©¦è¡Œä¸­...")
    
    choco_cmd = ['choco', 'install', 'tesseract', '-y']
    result, error = safe_subprocess_run(choco_cmd, "Chocolatey Tesseractã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«", timeout=120)
    
    if result and result.returncode == 0:
        print("âœ… ChocolateyçµŒç”±ã§Tesseractã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†")
        if setup_tesseract_path():
            return True
    elif error:
        print(f"âš ï¸ {error}")
    elif result:
        print(f"âš ï¸ Chocolateyã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¤±æ•— (çµ‚äº†ã‚³ãƒ¼ãƒ‰: {result.returncode})")
    
    # 3. æœ€çµ‚ç¢ºèªï¼ˆã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒæˆåŠŸã—ã¦ã„ãŸå¯èƒ½æ€§ï¼‰
    print("ğŸ” Tesseractã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«çŠ¶æ³ã‚’æœ€çµ‚ç¢ºèªä¸­...")
    if setup_tesseract_path():
        print("âœ… TesseractãŒåˆ©ç”¨å¯èƒ½ã«ãªã‚Šã¾ã—ãŸï¼ˆã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æˆåŠŸï¼‰")
        return True
    
    # 4. æ‰‹å‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã®æ¡ˆå†…
    print("ğŸ’¡ è‡ªå‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¡ˆå†…:")
    print("   1. https://github.com/UB-Mannheim/tesseract/wiki")  
    print("   2. 'tesseract-ocr-w64-setup-5.x.x.exe' ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»å®Ÿè¡Œ")
    print("   3. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å†èµ·å‹•")
    
    return False


def setup_tesseract_path():
    """Tesseractã®ãƒ‘ã‚¹è¨­å®š"""
    try:
        # ä¸€èˆ¬çš„ãªTesseractã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãƒ‘ã‚¹
        possible_paths = [
            r"C:\Program Files\Tesseract-OCR\tesseract.exe",
            r"C:\Program Files (x86)\Tesseract-OCR\tesseract.exe",
            r"C:\tools\tesseract\tesseract.exe",  # Chocolatey
        ]
        
        for path in possible_paths:
            if Path(path).exists():
                if TESSERACT_AVAILABLE:
                    pytesseract.pytesseract.tesseract_cmd = path
                    print(f"âœ… Tesseractãƒ‘ã‚¹è¨­å®šå®Œäº†: {path}")
                    return True
        
        print("âš ï¸ Tesseractãƒ‘ã‚¹ã®è‡ªå‹•è¨­å®šã«å¤±æ•—")
        return False
        
    except Exception as e:
        print(f"âš ï¸ Tesseractãƒ‘ã‚¹è¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
        return False


# ã‚¢ãƒ—ãƒªèµ·å‹•æ™‚ã®é«˜é€Ÿãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒã‚§ãƒƒã‚¯
startup_timer = time.time()
print("ğŸš€ ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é«˜é€Ÿèµ·å‹•ä¸­...")
start_check_time = time.time()
ensure_required_libraries()
check_duration = time.time() - start_check_time
print(f"âœ… ãƒ©ã‚¤ãƒ–ãƒ©ãƒªæº–å‚™å®Œäº† ({check_duration:.2f}ç§’)\n")

# OCRé–¢é€£ã®è‡ªå‹•ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
ocr_setup_needed = False
if TESSERACT_AVAILABLE and not PIL_AVAILABLE:
    print("âš ï¸ OCRæ©Ÿèƒ½: pytesseractã¯ã‚ã‚Šã¾ã™ãŒã€PillowãŒä¸è¶³ã—ã¦ã„ã¾ã™")
elif PIL_AVAILABLE and not TESSERACT_AVAILABLE:
    print("âš ï¸ OCRæ©Ÿèƒ½: Pillowã¯ã‚ã‚Šã¾ã™ãŒã€pytesseractãŒä¸è¶³ã—ã¦ã„ã¾ã™")
elif PIL_AVAILABLE and TESSERACT_AVAILABLE:
    ocr_setup_needed = True


def check_ocr_availability():
    """OCRæ©Ÿèƒ½ã®åˆ©ç”¨å¯èƒ½æ€§ã‚’ç¢ºèªï¼ˆã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³å¯¾å¿œï¼‰"""
    try:
        if not PIL_AVAILABLE or not TESSERACT_AVAILABLE:
            return False, "Pillow ã¾ãŸã¯ pytesseract ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“"
        
        # ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³ç‰ˆã§ã®Tesseractæ¤œç´¢
        def find_bundled_tesseract():
            """åŒæ¢±ã•ã‚ŒãŸTesseractã‚’æ¤œç´¢"""
            # EXEåŒ–å¯¾å¿œ: å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’åŸºæº–ã«ã™ã‚‹
            if getattr(sys, 'frozen', False):
                base_path = Path(sys.executable).parent
            else:
                base_path = Path(__file__).parent
            
            possible_paths = [
                # åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®tesseractãƒ•ã‚©ãƒ«ãƒ€
                base_path / "tesseract" / "tesseract.exe",
                base_path.parent / "tesseract" / "tesseract.exe",
                # ãƒãƒ¼ã‚¿ãƒ–ãƒ«ç‰ˆç”¨ã®ãƒ‘ã‚¹
                base_path / "bin" / "tesseract.exe",
                base_path.parent / "bin" / "tesseract.exe",
                # Windowsæ¨™æº–ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãƒ‘ã‚¹
                Path(r"C:\Program Files\Tesseract-OCR\tesseract.exe"),
                Path(r"C:\Program Files (x86)\Tesseract-OCR\tesseract.exe"),
            ]
            
            for path in possible_paths:
                if path.exists():
                    return str(path)
            return None
        
        # Tesseractã‚¨ãƒ³ã‚¸ãƒ³ã®ãƒ‘ã‚¹ã‚’ç¢ºèª
        try:
            # ã¾ãšæ¨™æº–ã®æ–¹æ³•ã§ç¢ºèª
            version = pytesseract.get_tesseract_version()
            print(f"âœ… Tesseract OCRã‚¨ãƒ³ã‚¸ãƒ³åˆ©ç”¨å¯èƒ½: v{version}")
            return True, f"Tesseract v{version}"
        except pytesseract.TesseractNotFoundError:
            # åŒæ¢±ç‰ˆã‚’æ¤œç´¢
            bundled_path = find_bundled_tesseract()
            if bundled_path:
                # pytesseractã«ãƒ‘ã‚¹ã‚’è¨­å®š
                pytesseract.pytesseract.tesseract_cmd = bundled_path
                try:
                    version = pytesseract.get_tesseract_version()
                    print(f"âœ… åŒæ¢±Tesseract OCRã‚¨ãƒ³ã‚¸ãƒ³åˆ©ç”¨å¯èƒ½: v{version}")
                    print(f"   ãƒ‘ã‚¹: {bundled_path}")
                    return True, f"åŒæ¢±Tesseract v{version}"
                except Exception as e:
                    return False, f"åŒæ¢±Tesseractã‚¨ãƒ©ãƒ¼: {e}"
            else:
                return False, "Tesseractã‚¨ãƒ³ã‚¸ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\n  ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³ç‰ˆ: tesseractãƒ•ã‚©ãƒ«ãƒ€ã‚’åŒæ¢±ã—ã¦ãã ã•ã„\n  é€šå¸¸ç‰ˆ: https://github.com/UB-Mannheim/tesseract/wiki ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"
        except Exception as e:
            return False, f"Tesseractã‚¨ãƒ³ã‚¸ãƒ³ã‚¨ãƒ©ãƒ¼: {e}"
            
    except Exception as e:
        return False, f"OCRãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}"


# OCRåˆ©ç”¨å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯
ocr_available, ocr_status = check_ocr_availability()
if ocr_available:
    print(f"ğŸ” {ocr_status}")
else:
    print(f"âš ï¸ OCRæ©Ÿèƒ½åˆ¶é™: {ocr_status}")
    print("   ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«(.tif)ã®å†…å®¹æ¤œç´¢ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“")


# CPUã‚³ã‚¢æ•°ã‚’å–å¾—ï¼ˆæœ€å¤§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ç‰ˆï¼‰
def get_optimal_thread_count():
    """æœ€é©ãªã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã‚’å–å¾—ï¼ˆè¶…é«˜é€Ÿç‰ˆãƒ»psutilä¾å­˜ãªã—ï¼‰"""
    try:
        # psutilãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã®é«˜ç²¾åº¦è¨­å®š
        if psutil is not None:
            # ç‰©ç†ã‚³ã‚¢æ•°ã¨è«–ç†ã‚³ã‚¢æ•°ã‚’å–å¾—
            physical_cores = psutil.cpu_count(logical=False) or 2
            logical_cores = psutil.cpu_count(logical=True) or 4
            
            # ç¾åœ¨ã®CPUä½¿ç”¨ç‡ã‚’ç¢ºèªï¼ˆè¶…é«˜é€Ÿï¼š0.1ç§’é–“éš”ï¼‰
            cpu_usage = psutil.cpu_percent(interval=0.1)
            
            # åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒªã‚‚è€ƒæ…®
            memory = psutil.virtual_memory()
            available_gb = memory.available / (1024**3)
            
            print(f"ğŸ”§ ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±å–å¾—å®Œäº†:")
            print(f"  ç‰©ç†ã‚³ã‚¢: {physical_cores}, è«–ç†ã‚³ã‚¢: {logical_cores}")
            print(f"  CPUä½¿ç”¨ç‡: {cpu_usage:.1f}%, åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒª: {available_gb:.1f}GB")
        else:
            # psutilãªã—ã§ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆos.cpu_countä½¿ç”¨ï¼‰
            logical_cores = os.cpu_count() or 4
            physical_cores = max(logical_cores // 2, 2)  # æ¦‚ç®—å€¤
            cpu_usage = 25.0  # æ¨™æº–çš„ãªå€¤ã‚’æƒ³å®š
            available_gb = 8.0  # æ¨™æº–çš„ãªå€¤ã‚’æƒ³å®š
            
            print(f"ğŸ”§ ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ï¼ˆæ¨å®šï¼‰:")
            print(f"  æ¨å®šç‰©ç†ã‚³ã‚¢: {physical_cores}, è«–ç†ã‚³ã‚¢: {logical_cores}")
            print(f"  æ¨å®šCPUä½¿ç”¨ç‡: {cpu_usage:.1f}%, æ¨å®šåˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒª: {available_gb:.1f}GB")
        
        # æœ€å¤§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨­å®šï¼ˆä»–ã‚¢ãƒ—ãƒªä½¿ç”¨è€ƒæ…®ï¼‰
        if physical_cores >= 16:  # 16ã‚³ã‚¢ä»¥ä¸Šï¼ˆãƒ¯ãƒ¼ã‚¯ã‚¹ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç´šï¼‰
            if cpu_usage < 20 and available_gb > 8:
                threads = min(physical_cores - 2, 16)  # æœ€å¤§16ã‚¹ãƒ¬ãƒƒãƒ‰
            elif cpu_usage < 40:
                threads = min(physical_cores - 4, 12)  # æœ€å¤§12ã‚¹ãƒ¬ãƒƒãƒ‰
            else:
                threads = min(physical_cores - 6, 8)   # æœ€å¤§8ã‚¹ãƒ¬ãƒƒãƒ‰
                
        elif physical_cores >= 12:  # 12-15ã‚³ã‚¢ï¼ˆé«˜æ€§èƒ½CPUï¼‰
            if cpu_usage < 20 and available_gb > 6:
                threads = min(physical_cores - 2, 12)  # æœ€å¤§12ã‚¹ãƒ¬ãƒƒãƒ‰
            elif cpu_usage < 40:
                threads = min(physical_cores - 3, 10)  # æœ€å¤§10ã‚¹ãƒ¬ãƒƒãƒ‰
            else:
                threads = min(physical_cores - 4, 8)   # æœ€å¤§8ã‚¹ãƒ¬ãƒƒãƒ‰
                
        elif physical_cores >= 8:  # 8-11ã‚³ã‚¢ï¼ˆä¸­é«˜æ€§èƒ½CPUï¼‰
            if cpu_usage < 25 and available_gb > 4:
                threads = min(physical_cores - 1, 10)  # æœ€å¤§10ã‚¹ãƒ¬ãƒƒãƒ‰
            elif cpu_usage < 50:
                threads = min(physical_cores - 2, 8)   # æœ€å¤§8ã‚¹ãƒ¬ãƒƒãƒ‰
            else:
                threads = min(physical_cores - 3, 6)   # æœ€å¤§6ã‚¹ãƒ¬ãƒƒãƒ‰
                
        elif physical_cores >= 6:  # 6-7ã‚³ã‚¢ï¼ˆä¸­æ€§èƒ½CPUï¼‰
            if cpu_usage < 25 and available_gb > 3:
                threads = min(physical_cores - 1, 8)   # æœ€å¤§8ã‚¹ãƒ¬ãƒƒãƒ‰
            elif cpu_usage < 50:
                threads = min(physical_cores - 1, 6)   # æœ€å¤§6ã‚¹ãƒ¬ãƒƒãƒ‰
            else:
                threads = min(physical_cores - 2, 4)   # æœ€å¤§4ã‚¹ãƒ¬ãƒƒãƒ‰
                
        elif physical_cores >= 4:  # 4-5ã‚³ã‚¢ï¼ˆæ¨™æº–CPUï¼‰
            if cpu_usage < 20 and available_gb > 2:
                threads = min(physical_cores, 6)       # æœ€å¤§6ã‚¹ãƒ¬ãƒƒãƒ‰
            elif cpu_usage < 40:
                threads = min(physical_cores, 5)       # æœ€å¤§5ã‚¹ãƒ¬ãƒƒãƒ‰
            else:
                threads = min(physical_cores - 1, 3)   # æœ€å¤§3ã‚¹ãƒ¬ãƒƒãƒ‰
        else:
            # 4ã‚³ã‚¢æœªæº€ã¯ä¿å®ˆçš„ã«
            threads = max(physical_cores - 1, 2)
        
        # æœ€çµ‚çš„ãªå®‰å…¨ãƒã‚§ãƒƒã‚¯
        threads = max(min(threads, 16), 2)  # 2-16ã‚¹ãƒ¬ãƒƒãƒ‰ã®ç¯„å›²
        
        print(f"  æ±ºå®šã‚¹ãƒ¬ãƒƒãƒ‰æ•°: {threads} (æœ€å¤§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å„ªå…ˆ)")
        
        return threads
        
    except Exception as e:
        print(f"âš ï¸ ã‚¹ãƒ¬ãƒƒãƒ‰æ•°è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
        return 4  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤


def get_ocr_thread_count():
    """OCRå‡¦ç†å°‚ç”¨ã®æœ€é©ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã‚’å–å¾—ï¼ˆè¶…é«˜é€Ÿå‡¦ç†ç‰ˆãƒ»å‹•çš„èª¿æ•´ï¼‰"""
    try:
        # é€šå¸¸ã®å‡¦ç†ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã‚’å–å¾—
        normal_threads = get_optimal_thread_count()
        
        # ğŸš€ OCRå‡¦ç†ã®è¶…é«˜é€ŸåŒ–è¨­å®šï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰æ•°æœ€é©åŒ–ï¼‰
        if psutil is not None:
            cpu_usage = psutil.cpu_percent(interval=0.1)
            memory = psutil.virtual_memory()
            available_gb = memory.available / (1024**3)
            cpu_count = psutil.cpu_count(logical=False)  # ç‰©ç†CPUæ•°
            
            # ğŸ”¥ é«˜æ€§èƒ½OCRå°‚ç”¨ã‚¹ãƒ¬ãƒƒãƒ‰æ•°è¨ˆç®—ï¼ˆå‡¦ç†é€Ÿåº¦é‡è¦–ï¼‰
            if available_gb > 16 and cpu_count >= 8:  # ãƒã‚¤ã‚¨ãƒ³ãƒ‰ã‚·ã‚¹ãƒ†ãƒ 
                if cpu_usage > 80:
                    ocr_threads = max(normal_threads // 3, 2)  # é«˜è² è·æ™‚ã¯æŠ‘åˆ¶
                elif cpu_usage > 60:
                    ocr_threads = max(normal_threads // 2, 4)  # ä¸­è² è·æ™‚ã¯åŠåˆ†
                else:
                    ocr_threads = min(normal_threads - 2, 8)  # ä½è² è·æ™‚ã¯æœ€å¤§æ´»ç”¨ï¼ˆæœ€å¤§8ã‚¹ãƒ¬ãƒƒãƒ‰ï¼‰
            elif available_gb > 8 and cpu_count >= 4:  # ãƒŸãƒ‰ãƒ«ãƒ¬ãƒ³ã‚¸ã‚·ã‚¹ãƒ†ãƒ 
                if cpu_usage > 70:
                    ocr_threads = max(normal_threads // 4, 2)  # é«˜è² è·æ™‚ã¯1/4
                elif cpu_usage > 50:
                    ocr_threads = max(normal_threads // 2, 3)  # ä¸­è² è·æ™‚ã¯1/2
                else:
                    ocr_threads = min(normal_threads - 1, 6)  # ä½è² è·æ™‚ã¯æœ€å¤§6ã‚¹ãƒ¬ãƒƒãƒ‰
            else:  # ãƒ­ãƒ¼ã‚¨ãƒ³ãƒ‰ã‚·ã‚¹ãƒ†ãƒ 
                if cpu_usage > 60:
                    ocr_threads = 1  # é«˜è² è·æ™‚ã¯1ã‚¹ãƒ¬ãƒƒãƒ‰ã®ã¿
                elif cpu_usage > 40:
                    ocr_threads = 2  # ä¸­è² è·æ™‚ã¯2ã‚¹ãƒ¬ãƒƒãƒ‰
                else:
                    ocr_threads = min(normal_threads // 2, 4)  # ä½è² è·æ™‚ã¯æœ€å¤§4ã‚¹ãƒ¬ãƒƒãƒ‰
        else:
            # psutilãŒãªã„å ´åˆã¯å‹•çš„ã«èª¿æ•´
            if normal_threads >= 8:
                ocr_threads = 6  # 8ã‚¹ãƒ¬ãƒƒãƒ‰ä»¥ä¸Šãªã‚‰6ã‚¹ãƒ¬ãƒƒãƒ‰
            elif normal_threads >= 4:
                ocr_threads = 4  # 4ã‚¹ãƒ¬ãƒƒãƒ‰ä»¥ä¸Šãªã‚‰4ã‚¹ãƒ¬ãƒƒãƒ‰
            else:
                ocr_threads = max(normal_threads - 1, 2)  # æœ€ä½2ã‚¹ãƒ¬ãƒƒãƒ‰
        
        # OCRã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã®ç¯„å›²åˆ¶é™ï¼ˆæœ€é©åŒ–ï¼‰
        ocr_threads = max(2, min(ocr_threads, 8))  # 2ï½8ã‚¹ãƒ¬ãƒƒãƒ‰ã®ç¯„å›²
        
        print(f"ğŸ”§ è¶…é«˜é€ŸOCRå‡¦ç†ç”¨ã‚¹ãƒ¬ãƒƒãƒ‰æ•°: {ocr_threads} (æœ€é©åŒ–ãƒ¢ãƒ¼ãƒ‰ - é€šå¸¸: {normal_threads})")
        if psutil:
            print(f"   CPUä½¿ç”¨ç‡: {cpu_usage:.1f}%, ãƒ¡ãƒ¢ãƒª: {available_gb:.1f}GB")
        return ocr_threads
        
    except Exception as e:
        print(f"âš ï¸ OCRã‚¹ãƒ¬ãƒƒãƒ‰æ•°å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return 4  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ4ã‚¹ãƒ¬ãƒƒãƒ‰


def get_batch_size_for_images():
    """.tifç”»åƒãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ç”¨ã®æœ€é©ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å–å¾—ï¼ˆè¶…é«˜é€Ÿå‡¦ç†ç‰ˆï¼‰"""
    try:
        if psutil is not None:
            memory = psutil.virtual_memory()
            available_gb = memory.available / (1024**3)
            cpu_count = psutil.cpu_count(logical=False)
            
            # ğŸš€ 1000ãƒ•ã‚¡ã‚¤ãƒ«/ç§’å¯¾å¿œ OCRä¸¦åˆ—å‡¦ç†è¶…æ¥µé™æœ€é©åŒ–ãƒãƒƒãƒã‚µã‚¤ã‚ºè¨ˆç®—
            if available_gb > 64 and cpu_count >= 16:  # è¶…ãƒã‚¤ã‚¨ãƒ³ãƒ‰ã‚·ã‚¹ãƒ†ãƒ 
                return 150  # è¶…å¤§é‡ä¸¦åˆ—å‡¦ç†ï¼ˆ1000ãƒ•ã‚¡ã‚¤ãƒ«/ç§’å¯¾å¿œãƒ»100%å¢—å¼·ï¼‰
            elif available_gb > 32 and cpu_count >= 8:  # ãƒã‚¤ã‚¨ãƒ³ãƒ‰ã‚·ã‚¹ãƒ†ãƒ 
                return 120  # å¤§é‡ä¸¦åˆ—å‡¦ç†ï¼ˆ1000ãƒ•ã‚¡ã‚¤ãƒ«/ç§’å¯¾å¿œãƒ»100%å¢—å¼·ï¼‰
            elif available_gb > 16 and cpu_count >= 6:  # é«˜æ€§èƒ½ã‚·ã‚¹ãƒ†ãƒ 
                return 90  # é«˜é€Ÿä¸¦åˆ—å‡¦ç†ï¼ˆ1000ãƒ•ã‚¡ã‚¤ãƒ«/ç§’å¯¾å¿œãƒ»100%å¢—å¼·ï¼‰
            elif available_gb > 8 and cpu_count >= 4:  # æ¨™æº–ã‚·ã‚¹ãƒ†ãƒ 
                return 70  # æ¨™æº–ä¸¦åˆ—å‡¦ç†ï¼ˆ1000ãƒ•ã‚¡ã‚¤ãƒ«/ç§’å¯¾å¿œãƒ»100%å¢—å¼·ï¼‰
            elif available_gb > 4:  # ä½ã‚¹ãƒšãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ 
                return 56  # è»½é‡ä¸¦åˆ—å‡¦ç†ï¼ˆ1000ãƒ•ã‚¡ã‚¤ãƒ«/ç§’å¯¾å¿œãƒ»100%å¢—å¼·ï¼‰
            else:
                return 44   # æœ€å°æ§‹æˆï¼ˆ1000ãƒ•ã‚¡ã‚¤ãƒ«/ç§’å¯¾å¿œãƒ»100%å¢—å¼·ï¼‰
        else:
            return 74  # psutilæœªä½¿ç”¨æ™‚ã®1000ãƒ•ã‚¡ã‚¤ãƒ«/ç§’å¯¾å¿œå€¤ï¼ˆ100%å¢—å¼·ï¼‰
            
    except Exception as e:
        print(f"âš ï¸ ç”»åƒãƒãƒƒãƒã‚µã‚¤ã‚ºè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
        return 10  # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        
    except Exception as e:
        print(f"âš ï¸ ã‚¹ãƒ¬ãƒƒãƒ‰æ•°è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e} - ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ6ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’ä½¿ç”¨")
        return 6


# ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°è¨­å®š
def setup_debug_logger():
    """ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°è¨­å®šï¼ˆé‡è¤‡é˜²æ­¢ç‰ˆï¼‰"""
    logger = logging.getLogger('UltraFastApp')

    # æ—¢å­˜ã®ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’ã‚¯ãƒªã‚¢ï¼ˆé‡è¤‡é˜²æ­¢ï¼‰
    if logger.handlers:
        logger.handlers.clear()

    logger.setLevel(logging.DEBUG)

    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼ˆä¸Šæ›¸ããƒ¢ãƒ¼ãƒ‰ï¼‰
    file_handler = logging.FileHandler('file_search_app.log', mode='w', encoding='utf-8')
    file_handler.setLevel(logging.DEBUG)

    # ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(formatter)

    logger.addHandler(file_handler)

    # è¦ªãƒ­ã‚¬ãƒ¼ã¸ã®ä¼æ’­ã‚’ç„¡åŠ¹åŒ–ï¼ˆé‡è¤‡å‡ºåŠ›é˜²æ­¢ï¼‰
    logger.propagate = False

    return logger


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ­ã‚°
debug_logger = setup_debug_logger()


def normalize_search_text_ultra(text):
    """
    ğŸ”„ è¶…é«˜é€Ÿæ¤œç´¢ç”¨ãƒ†ã‚­ã‚¹ãƒˆæ­£è¦åŒ–ï¼ˆæ—¥æœ¬èªFTS5å¯¾å¿œå¼·åŒ–ç‰ˆï¼‰
    
    Args:
        text (str): æ¤œç´¢ãƒ†ã‚­ã‚¹ãƒˆ
        
    Returns:
        tuple: (åŠè§’ç‰ˆ, å…¨è§’ç‰ˆ, æ­£è¦åŒ–ç‰ˆ, ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒªã‚¹ãƒˆ)
    """
    if not text:
        return '', '', '', []

    patterns = []

    # åŸºæœ¬ãƒ‘ã‚¿ãƒ¼ãƒ³
    patterns.append(text)

    # åŠè§’ç‰ˆï¼ˆNFKCæ­£è¦åŒ–ï¼‰
    if unicodedata is not None:
        half_width = unicodedata.normalize('NFKC', text)
        if half_width != text:
            patterns.append(half_width)
    else:
        half_width = text

    # å…¨è§’ç‰ˆï¼ˆåŠè§’è‹±æ•°ã‚’å…¨è§’ã«å¤‰æ›ï¼‰
    full_width = ''
    for char in text:
        if '!' <= char <= '~':
            full_width += chr(ord(char) + 0xFEE0)
        else:
            full_width += char
    if full_width != text:
        patterns.append(full_width)

    # æ­£è¦åŒ–ç‰ˆï¼ˆå¤§æ–‡å­—å°æ–‡å­—çµ±ä¸€ï¼‰
    normalized = text.lower()
    if normalized != text:
        patterns.append(normalized)

    # æ—¥æœ¬èªFTS5å¯¾å¿œ: å€‹åˆ¥æ–‡å­—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚‚è¿½åŠ 
    if len(text) >= 2:
        # å„æ–‡å­—ã‚’å€‹åˆ¥ã«è¿½åŠ ï¼ˆéƒ¨åˆ†ãƒãƒƒãƒç”¨ï¼‰
        for i in range(len(text)):
            char = text[i]
            if char not in patterns and len(char.strip()) > 0:
                patterns.append(char)
        
        # 2æ–‡å­—ãšã¤ã®çµ„ã¿åˆã‚ã›ï¼ˆbi-gramï¼‰
        for i in range(len(text) - 1):
            bigram = text[i:i+2]
            if bigram not in patterns:
                patterns.append(bigram)

    # ã²ã‚‰ãŒãªâ†’ã‚«ã‚¿ã‚«ãƒŠå¤‰æ›
    hiragana_to_katakana = ''
    for char in normalized:
        if 'ã' <= char <= 'ã‚–':  # ã²ã‚‰ãŒãªç¯„å›²
            hiragana_to_katakana += chr(ord(char) + 0x60)
        else:
            hiragana_to_katakana += char

    if hiragana_to_katakana != normalized:
        patterns.append(hiragana_to_katakana)

    # ã‚«ã‚¿ã‚«ãƒŠâ†’ã²ã‚‰ãŒãªå¤‰æ›
    katakana_to_hiragana = ''
    for char in normalized:
        if 'ã‚¡' <= char <= 'ãƒ¶':  # ã‚«ã‚¿ã‚«ãƒŠç¯„å›²
            katakana_to_hiragana += chr(ord(char) - 0x60)
        else:
            katakana_to_hiragana += char

    if katakana_to_hiragana != normalized:
        patterns.append(katakana_to_hiragana)

    # ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã®å„å˜èªã«ã‚‚é©ç”¨
    words = text.split()
    if len(words) > 1:
        for word in words:
            if word not in patterns:
                patterns.append(word)
            # å„å˜èªã®åŠè§’å…¨è§’å¤‰æ›ã‚‚è¿½åŠ 
            if unicodedata is not None:
                word_half = unicodedata.normalize('NFKC', word)
                if word_half not in patterns:
                    patterns.append(word_half)

    # é‡è¤‡é™¤å»ã¨ã‚½ãƒ¼ãƒˆï¼ˆé•·ã„é †ã ãŒã€å…ƒã®æ–‡å­—åˆ—ã‚’æœ€å„ªå…ˆï¼‰
    unique_patterns = []
    unique_patterns.append(text)  # å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æœ€å„ªå…ˆ
    
    for pattern in patterns:
        if pattern not in unique_patterns and pattern != text:
            unique_patterns.append(pattern)
    
    # é•·ã•ã§ã‚½ãƒ¼ãƒˆï¼ˆãŸã ã—ã€å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã¯æœ€åˆã«ä¿æŒï¼‰
    first_pattern = unique_patterns[0]
    remaining_patterns = sorted(unique_patterns[1:], key=len, reverse=True)
    final_patterns = [first_pattern] + remaining_patterns

    return half_width, full_width, hiragana_to_katakana, final_patterns

    # ã²ã‚‰ãŒãªâ†’ã‚«ã‚¿ã‚«ãƒŠå¤‰æ›
    hiragana_to_katakana = ''
    for char in normalized:
        if 'ã' <= char <= 'ã‚–':  # ã²ã‚‰ãŒãªç¯„å›²
            hiragana_to_katakana += chr(ord(char) + 0x60)
        else:
            hiragana_to_katakana += char

    if hiragana_to_katakana != normalized:
        patterns.append(hiragana_to_katakana)

    # ã‚«ã‚¿ã‚«ãƒŠâ†’ã²ã‚‰ãŒãªå¤‰æ›
    katakana_to_hiragana = ''
    for char in normalized:
        if 'ã‚¡' <= char <= 'ãƒ¶':  # ã‚«ã‚¿ã‚«ãƒŠç¯„å›²
            katakana_to_hiragana += chr(ord(char) - 0x60)
        else:
            katakana_to_hiragana += char

    if katakana_to_hiragana != normalized:
        patterns.append(katakana_to_hiragana)

    # ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã®å„å˜èªã«ã‚‚é©ç”¨
    words = text.split()
    if len(words) > 1:
        for word in words:
            if word not in patterns:
                patterns.append(word)
            # å„å˜èªã®åŠè§’å…¨è§’å¤‰æ›ã‚‚è¿½åŠ 
            word_half = unicodedata.normalize('NFKC', word)
            if word_half not in patterns:
                patterns.append(word_half)

    # é‡è¤‡é™¤å»ã¨ã‚½ãƒ¼ãƒˆï¼ˆé•·ã„é †ï¼‰
    unique_patterns = list(set(patterns))
    unique_patterns.sort(key=len, reverse=True)

    return half_width, full_width, hiragana_to_katakana, unique_patterns


def enhanced_search_match(text, query_patterns):
    """
    ğŸš€ æ‹¡å¼µæ¤œç´¢ãƒãƒƒãƒãƒ³ã‚°ï¼ˆåŠè§’å…¨è§’å¯¾å¿œå¼·åŒ–ç‰ˆï¼‰
    
    Args:
        text (str): æ¤œç´¢å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
        query_patterns (list): æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒªã‚¹ãƒˆ
        
    Returns:
        bool: ãƒãƒƒãƒã™ã‚‹ã‹ã©ã†ã‹
    """
    if not text or not query_patterns:
        return False

    # ãƒ†ã‚­ã‚¹ãƒˆã‚‚è¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ã§æ­£è¦åŒ–
    text_lower = text.lower()
    text_normalized = unicodedata.normalize('NFKC', text_lower)

    # ãƒ†ã‚­ã‚¹ãƒˆã®ã²ã‚‰ãŒãªâ†’ã‚«ã‚¿ã‚«ãƒŠå¤‰æ›
    text_hiragana_to_katakana = ''
    for char in text_lower:
        if 'ã' <= char <= 'ã‚–':
            text_hiragana_to_katakana += chr(ord(char) + 0x60)
        else:
            text_hiragana_to_katakana += char

    # ãƒ†ã‚­ã‚¹ãƒˆã®ã‚«ã‚¿ã‚«ãƒŠâ†’ã²ã‚‰ãŒãªå¤‰æ›
    text_katakana_to_hiragana = ''
    for char in text_lower:
        if 'ã‚¡' <= char <= 'ãƒ¶':
            text_katakana_to_hiragana += chr(ord(char) - 0x60)
        else:
            text_katakana_to_hiragana += char

    # ãƒ†ã‚­ã‚¹ãƒˆã®æ­£è¦åŒ–ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³
    text_variants = [
        text, text_lower, text_normalized, text_hiragana_to_katakana, text_katakana_to_hiragana
    ]

    # å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ãƒãƒƒãƒãƒ³ã‚°ã‚’è©¦è¡Œ
    for pattern in query_patterns:
        pattern_lower = pattern.lower()
        pattern_normalized = unicodedata.normalize('NFKC', pattern_lower)

        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã²ã‚‰ãŒãªâ†’ã‚«ã‚¿ã‚«ãƒŠå¤‰æ›
        pattern_hiragana_to_katakana = ''
        for char in pattern_lower:
            if 'ã' <= char <= 'ã‚–':
                pattern_hiragana_to_katakana += chr(ord(char) + 0x60)
            else:
                pattern_hiragana_to_katakana += char

        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã‚«ã‚¿ã‚«ãƒŠâ†’ã²ã‚‰ãŒãªå¤‰æ›
        pattern_katakana_to_hiragana = ''
        for char in pattern_lower:
            if 'ã‚¡' <= char <= 'ãƒ¶':
                pattern_katakana_to_hiragana += chr(ord(char) - 0x60)
            else:
                pattern_katakana_to_hiragana += char

        pattern_variants = [
            pattern, pattern_lower, pattern_normalized, pattern_hiragana_to_katakana,
            pattern_katakana_to_hiragana
        ]

        # ç²¾å¯†ãƒãƒƒãƒãƒ³ã‚°: 3æ–‡å­—ä»¥ä¸Šã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§æ¤œç´¢ï¼ˆã‚ˆã‚Šå³å¯†ã«ï¼‰
        for text_variant in text_variants:
            for pattern_variant in pattern_variants:
                # å…ƒã®ã‚¯ã‚¨ãƒªãŒ3æ–‡å­—ä»¥ä¸Šã®å ´åˆã¯3æ–‡å­—ä»¥ä¸Šãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å„ªå…ˆ
                if len(query_patterns[0]) >= 3 and len(pattern_variant.strip()) < 3:
                    continue
                # å…ƒã®ã‚¯ã‚¨ãƒªãŒ2æ–‡å­—ã®å ´åˆã¯2æ–‡å­—ä»¥ä¸Šãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å¯¾è±¡
                elif len(query_patterns[0]) == 2 and len(pattern_variant.strip()) < 2:
                    continue
                # 1æ–‡å­—ã®ã‚¯ã‚¨ãƒªã¯1æ–‡å­—ä»¥ä¸Šãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å¯¾è±¡
                elif len(query_patterns[0]) == 1 and len(pattern_variant.strip()) < 1:
                    continue
                
                # å®Œå…¨ä¸€è‡´ã‚’å„ªå…ˆ
                if pattern_variant == text_variant:
                    return True
                
                # éƒ¨åˆ†ä¸€è‡´ - å…ƒã®ã‚¯ã‚¨ãƒªé•·ã«å¿œã˜ã¦å³å¯†æ€§ã‚’èª¿æ•´
                if len(query_patterns[0]) >= 4:
                    # 4æ–‡å­—ä»¥ä¸Šã®å ´åˆã¯å³å¯†ãƒãƒƒãƒãƒ³ã‚°ï¼ˆå®Œå…¨ä¸€è‡´å„ªå…ˆï¼‰
                    if pattern_variant == query_patterns[0] and pattern_variant in text_variant:
                        return True
                elif len(pattern_variant) >= 2 and pattern_variant in text_variant:
                    return True

    return False


class UltraFastFullCompliantSearchSystem:
    """100%ä»•æ§˜é©åˆ è¶…é«˜é€Ÿå…¨æ–‡æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ï¼ˆå‹•çš„ä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç‰ˆï¼‰"""

    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        
        # data_storageãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ç¢ºèªã¨ä½œæˆï¼ˆEXEåŒ–å¯¾å¿œï¼‰
        data_storage_dir = self.project_root / "data_storage"
        if not data_storage_dir.exists():
            print(f"ğŸ“ data_storageãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ: {data_storage_dir}")
            data_storage_dir.mkdir(parents=True, exist_ok=True)
        
        # cacheãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚‚ç¢ºèªãƒ»ä½œæˆ
        cache_dir = self.project_root / "cache"
        if not cache_dir.exists():
            print(f"ğŸ“ cacheãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ: {cache_dir}")
            cache_dir.mkdir(parents=True, exist_ok=True)
        
        # ã¾ãšæ—¢å­˜ã®DBãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’ãƒã‚§ãƒƒã‚¯
        existing_db_count = 0
        for i in range(48):  # æœ€å¤§48ã¾ã§ç¢ºèª
            complete_db_path = data_storage_dir / f"complete_search_db_{i}.db"
            if complete_db_path.exists() and complete_db_path.stat().st_size > 100000:  # 100KBä»¥ä¸Š=ãƒ‡ãƒ¼ã‚¿ã‚ã‚Š
                existing_db_count += 1
            elif not complete_db_path.exists():
                break  # é€£ç¶šã—ã¦ã„ãªã„DBãŒã‚ã‚Œã°åœæ­¢
        
        if existing_db_count > 0:
            self.db_count = existing_db_count
            print(f"ğŸ—„ï¸ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä½¿ç”¨: {self.db_count}å€‹")
        else:
            # å‹•çš„ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ•°è¨ˆç®—ï¼ˆã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ãƒ™ãƒ¼ã‚¹ï¼‰
            self.db_count = self._calculate_optimal_db_count()
            print(f"ğŸ”§ å‹•çš„ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ•°è¨ˆç®—: {self.db_count}å€‹ (ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹æœ€é©åŒ–)")
        
        self.db_paths = []
        self.complete_db_paths = []
        for i in range(self.db_count):
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã¯data_storageãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã«é…ç½®
            db_path = self.project_root / "data_storage" / f"ultra_fast_search_db_{i}.db"
            complete_db_path = self.project_root / "data_storage" / f"complete_search_db_{i}.db"
            self.db_paths.append(db_path)
            self.complete_db_paths.append(complete_db_path)
        
        # 3å±¤ãƒ¬ã‚¤ãƒ¤ãƒ¼æ§‹é€ ï¼ˆé‡è¤‡å‰Šé™¤ãƒ»å½¹å‰²æ˜ç¢ºåŒ–ç‰ˆï¼‰
        # å³åº§å±¤: æ¤œç´¢ã‚­ãƒ£ãƒƒã‚·ãƒ¥å°‚ç”¨ï¼ˆçŸ­æ™‚é–“ä¿æŒãƒ»ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®ã¿ï¼‰
        # é«˜é€Ÿå±¤: ä¸­æœŸã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆè©³ç´°ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ»ä¸€æ™‚ä¿å­˜ï¼‰  
        # å®Œå…¨å±¤: æ°¸ç¶šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆå…¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ»æ°¸ç¶šä¿å­˜ï¼‰
        self.immediate_cache: Dict[str, Any] = {}  # å³åº§å±¤ (ãƒ¡ãƒ¢ãƒªã®ã¿ - æ®ç™ºæ€§)
        self.hot_cache: Dict[str, Any] = {}  # é«˜é€Ÿå±¤ (ãƒ¡ãƒ¢ãƒª + ãƒ‡ã‚£ã‚¹ã‚¯)

        # ä¸¦åˆ—å‡¦ç†è¨­å®šï¼ˆæœ€å¤§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç‰ˆãƒ»å‹•çš„å¢—å¼·å¯¾å¿œï¼‰
        base_threads = get_optimal_thread_count()
        # ğŸš€ ä¸¦åˆ—å‡¦ç†æ•°ã‚’2å€ã«å¢—å¼·ï¼ˆè»½é‡ãƒ•ã‚¡ã‚¤ãƒ«ç”¨ï¼‰
        self.optimal_threads = base_threads * 2
        self.base_threads = base_threads  # å…ƒã®å€¤ã‚’ä¿æŒ
        
        # ç”»åƒå‡¦ç†å°‚ç”¨è¨­å®šï¼ˆCPUä½¿ç”¨ç‡æŠ‘åˆ¶ï¼‰
        self.ocr_threads = get_ocr_thread_count()
        self.image_batch_size = get_batch_size_for_images()
        self.ocr_processing_delay = 0.02  # OCRå‡¦ç†é–“ã®é…å»¶ã‚’å¤§å¹…çŸ­ç¸®ï¼ˆé«˜é€ŸåŒ–ï¼‰
        
        print(f"ğŸ”§ è¶…é«˜é€Ÿ.tifç”»åƒå‡¦ç†æœ€é©åŒ–è¨­å®š:")
        print(f"  OCRå°‚ç”¨ã‚¹ãƒ¬ãƒƒãƒ‰æ•°: {self.ocr_threads} (æœ€å¤§8ã‚¹ãƒ¬ãƒƒãƒ‰)")
        print(f"  .tifç”»åƒãƒãƒƒãƒã‚µã‚¤ã‚º: {self.image_batch_size} (æœ€å¤§25)")
        print(f"  OCRå‡¦ç†é…å»¶: {self.ocr_processing_delay}ç§’ (5å€é«˜é€ŸåŒ–)")
        print(f"  å¯¾è±¡ç”»åƒå½¢å¼: .tif/.tiff ã®ã¿")
        print(f"  OCRã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½: æœ‰åŠ¹ï¼ˆé‡è¤‡å‡¦ç†é˜²æ­¢ï¼‰")
        
        # æœ€å¤§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨­å®šé©ç”¨ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡æœ€é©åŒ–ï¼‰
        self.max_immediate_cache = 150000  # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’è€ƒæ…®ã—ãŸæœ€é©å€¤
        self.max_hot_cache = 1500000  # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’è€ƒæ…®ã—ãŸæœ€é©å€¤
        
        # å‹•çš„ãƒãƒƒãƒã‚µã‚¤ã‚ºè¨­å®šï¼ˆè¶…é«˜é€Ÿç‰ˆï¼‰
        try:
            if psutil is not None:
                available_gb = psutil.virtual_memory().available / (1024**3)
            else:
                # psutilãªã—ã§ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆæ¨å®šå€¤ï¼‰
                available_gb = 8.0  # æ¨™æº–çš„ãªåˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒªã‚’æƒ³å®š
            
            # å‹•çš„ãƒãƒƒãƒã‚µã‚¤ã‚ºè¨ˆç®—ï¼ˆåŠ¹ç‡é‡è¦–ãƒ»æœ€é©åŒ–ç‰ˆï¼‰
            base_batch = 300
            
            # ã‚¹ãƒ¬ãƒƒãƒ‰åŠ¹ç‡ã«åŸºã¥ãä¹—æ•°ï¼ˆè¨ˆç®—ã‚³ã‚¹ãƒˆå‰Šæ¸›ï¼‰
            if self.optimal_threads >= 16:
                thread_multiplier = 20
            elif self.optimal_threads >= 12:
                thread_multiplier = 16
            elif self.optimal_threads >= 8:
                thread_multiplier = 12
            elif self.optimal_threads >= 6:
                thread_multiplier = 10
            elif self.optimal_threads >= 4:
                thread_multiplier = 8
            else:
                thread_multiplier = 6
            
            # ãƒ¡ãƒ¢ãƒªãƒ™ãƒ¼ã‚¹ä¹—æ•°ï¼ˆæ®µéšçš„æœ€é©åŒ–ï¼‰
            if available_gb > 32:
                memory_multiplier = 16  # ãƒã‚¤ã‚¨ãƒ³ãƒ‰
            elif available_gb > 16:
                memory_multiplier = 12  # æ¨™æº–ãƒã‚¤ã‚¹ãƒšãƒƒã‚¯
            elif available_gb > 8:
                memory_multiplier = 10  # ä¸­å®¹é‡
            elif available_gb > 4:
                memory_multiplier = 8   # æ¨™æº–
            elif available_gb > 2:
                memory_multiplier = 6   # æœ€å°
            else:
                memory_multiplier = 4   # ä½ãƒ¡ãƒ¢ãƒª
            
            # æœ€é©ãƒãƒƒãƒã‚µã‚¤ã‚ºç¯„å›²ï¼ˆåŠ¹ç‡é‡è¦–ï¼‰
            max_batch_size = 15000  # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ãƒãƒ©ãƒ³ã‚¹
            min_batch_size = 600    # æœ€å°åŠ¹ç‡å€¤
            
            calculated_batch = base_batch * thread_multiplier * memory_multiplier
            self.batch_size = min(max(calculated_batch, min_batch_size), max_batch_size)
            
            print(f"ğŸ”§ æœ€é©åŒ–ãƒãƒƒãƒã‚µã‚¤ã‚ºè¨ˆç®—:")
            print(f"  åŸºæœ¬ãƒãƒƒãƒ: {base_batch}")
            print(f"  ã‚¹ãƒ¬ãƒƒãƒ‰ä¹—æ•°: {thread_multiplier}")
            print(f"  ãƒ¡ãƒ¢ãƒªä¹—æ•°: {memory_multiplier} (åˆ©ç”¨å¯èƒ½: {available_gb:.1f}GB)")
            print(f"  è¨ˆç®—å€¤: {calculated_batch}")
            print(f"  æœ€çµ‚å€¤: {self.batch_size} (ç¯„å›²: {min_batch_size}-{max_batch_size})")
        except:
            self.batch_size = 8000  # æœ€é©åŒ–ã•ã‚ŒãŸãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨­å®šï¼ˆè¶…é«˜é€ŸåŒ–ï¼‰
        self.io_delay = 0.0001  # I/Oé…å»¶ã‚’æ›´ã«çŸ­ç¸®ï¼ˆ5å€é«˜é€Ÿï¼‰
        self.batch_delay = 0.005  # ãƒãƒƒãƒé–“é…å»¶ã‚’åŠæ¸›
        self.database_timeout = 180.0  # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’å»¶é•·
        
        print(f"ğŸš€ ã‚·ã‚¹ãƒ†ãƒ æœ€é©åŒ–è¨­å®š:")
        print(f"  åŸºæœ¬ã‚¹ãƒ¬ãƒƒãƒ‰æ•°: {self.base_threads}")
        print(f"  æœ€å¤§ä¸¦åˆ—æ•°: {self.optimal_threads} (2å€å¢—å¼·)")
        print(f"  ãƒãƒƒãƒã‚µã‚¤ã‚º: {self.batch_size}")
        print(f"  å³åº§å±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥: {self.max_immediate_cache:,}")
        print(f"  é«˜é€Ÿå±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥: {self.max_hot_cache:,}")
        print(f"  I/Oé…å»¶: {self.io_delay*1000:.1f}ms")
        print(f"  ãƒãƒƒãƒé…å»¶: {self.batch_delay*1000:.1f}ms")
        
        # æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆé‡è¤‡ç”Ÿæˆé˜²æ­¢ï¼‰
        if not hasattr(self, '_pattern_cache'):
            self._pattern_cache = {}
        if not hasattr(self, '_pattern_cache_max_size'):
            self._pattern_cache_max_size = 1000  # æœ€å¤§1000ã‚¯ã‚¨ãƒªã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        
        # ğŸš€ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–æ¸ˆã¿ãƒ•ãƒ©ã‚°ï¼ˆé‡è¤‡åˆæœŸåŒ–é˜²æ­¢ï¼‰
        self._db_initialized = False
        
        # ãƒ—ãƒ­ã‚»ã‚¹å„ªå…ˆåº¦ã‚’é«˜ã«è¨­å®šï¼ˆè¶…é«˜é€Ÿç‰ˆï¼‰
        try:
            if psutil is not None:
                current_process = psutil.Process(os.getpid())
                if os.name == 'nt':  # Windows
                    current_process.nice(psutil.ABOVE_NORMAL_PRIORITY_CLASS)
                    print("âš¡ Windows: ãƒ—ãƒ­ã‚»ã‚¹å„ªå…ˆåº¦ã‚’é«˜ã«è¨­å®š")
                else:  # Linux/macOS
                    current_process.nice(-5)
                    print("âš¡ Unixç³»: ãƒ—ãƒ­ã‚»ã‚¹å„ªå…ˆåº¦ã‚’é«˜ã«è¨­å®š")
            else:
                print("ğŸ’¡ psutilæœªåˆ©ç”¨ - OSæ¨™æº–å„ªå…ˆåº¦ã§å®Ÿè¡Œ")
        except Exception as e:
            print(f"âš ï¸ ãƒ—ãƒ­ã‚»ã‚¹å„ªå…ˆåº¦è¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæ¥­ã®çŠ¶æ…‹ç®¡ç†
        self.indexing_in_progress = False
        self.indexing_results_ready = False
        self.background_indexer = None
        
        # æ¤œç´¢çµæœæä¾›ç”¨ã®çŠ¶æ…‹
        self.use_cache_while_indexing = True
        
        # ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ç®¡ç†ï¼ˆã‚¨ãƒ©ãƒ¼é˜²æ­¢ï¼‰
        self.shutdown_requested = False
        self._active_executors = []  # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªExecutorã‚’è¿½è·¡
        self._background_threads = []  # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’è¿½è·¡

        # å¢—åˆ†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ©Ÿèƒ½ï¼ˆæœ€å¤§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç‰ˆï¼‰
        self.incremental_indexing_enabled = True
        self.last_full_scan_time = 0
        self.indexed_files_registry = {}  # {file_path: last_modified_time}
        self.new_files_buffer = []  # æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ä¸€æ™‚ä¿å­˜
        self.max_buffer_size = 200  # ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºã‚’å€å¢—
        self.incremental_scan_interval = 10  # 10ç§’ã«çŸ­ç¸®ï¼ˆã‚ˆã‚Šé »ç¹ã«ã‚¹ã‚­ãƒ£ãƒ³ï¼‰

        # çµ±è¨ˆæƒ…å ±ï¼ˆæ‹¡å¼µç‰ˆï¼‰
        self.stats = {
            "indexed_files": 0,
            "immediate_layer_hits": 0,
            "hot_layer_hits": 0,
            "complete_layer_hits": 0,
            "search_count": 0,
            "avg_search_time": 0.0,
            "total_search_time": 0.0,
            "error_count": 0,
            "last_optimization": 0,
            "optimization_count": 0,
            "total_optimization_time": 0.0,
            "incremental_updates": 0,
            "files_added_incrementally": 0,
            "peak_thread_count": self.optimal_threads,
            "dynamic_adjustments": 0,
            "max_batch_size_used": self.batch_size
        }

        # è‡ªå‹•æœ€é©åŒ–è¨­å®šï¼ˆã‚ˆã‚Šç©æ¥µçš„ï¼‰
        self.auto_optimize_enabled = True
        self.auto_optimize_threshold = 500  # 500å›æ¤œç´¢å¾Œã«è‡ªå‹•æœ€é©åŒ–ï¼ˆåŠåˆ†ã«çŸ­ç¸®ï¼‰
        self.auto_optimize_interval = 1800  # 30åˆ†é–“éš”ï¼ˆåŠåˆ†ã«çŸ­ç¸®ï¼‰
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¿½è·¡
        self.performance_history: List[Dict[str, Any]] = []
        self.optimization_history: List[Dict[str, Any]] = []

        # ãƒ•ã‚©ãƒ«ãƒ€ã‚ªãƒ¼ãƒ—ãƒ³åˆ¶å¾¡
        self._last_folder_request: Dict[str, Any] = {}
        self._opening_folder: bool = False
        
        # çµ±è¨ˆæ›´æ–°ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
        self._stats_update_callback = None
        
        # ğŸš€ OCRã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆæœŸåŒ–ï¼ˆç”»åƒå‡¦ç†é«˜é€ŸåŒ–ï¼‰
        self._ocr_cache = {}  # OCRã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆé‡è¤‡å‡¦ç†é˜²æ­¢ï¼‰

        self.initialize_database()
        
        total_startup_time = time.time() - startup_timer
        print(f"ğŸ¯ æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ é«˜é€Ÿèµ·å‹•å®Œäº† ({total_startup_time:.2f}ç§’) - {self.optimal_threads}ã‚¹ãƒ¬ãƒƒãƒ‰, {self.db_count}DB - 2000ãƒ•ã‚¡ã‚¤ãƒ«/ç§’å¯¾å¿œ")

    def _calculate_optimal_db_count(self) -> int:
        """ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ã«åŸºã¥ãæœ€é©ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ•°è¨ˆç®—ï¼ˆãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢é©å¿œç‰ˆï¼‰"""
        try:
            # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã®è©³ç´°å–å¾—
            hardware_info = self._get_comprehensive_hardware_info()
            
            # ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ•°ã®è¨ˆç®—ï¼ˆCPUãƒ™ãƒ¼ã‚¹ï¼‰
            cpu_cores = hardware_info['cpu_cores']
            logical_cores = hardware_info['logical_cores']
            
            # CPUã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«åŸºã¥ãåŸºæœ¬DBæ•°
            if cpu_cores >= 20:  # è¶…ãƒã‚¤ã‚¨ãƒ³ãƒ‰ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
                base_db_count = min(logical_cores, 48)
            elif cpu_cores >= 16:  # ãƒã‚¤ã‚¨ãƒ³ãƒ‰ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
                base_db_count = min(logical_cores, 40)
            elif cpu_cores >= 12:  # é«˜æ€§èƒ½ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—
                base_db_count = min(logical_cores, 32)
            elif cpu_cores >= 8:   # æ¨™æº–ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—
                base_db_count = min(logical_cores * 0.8, 24)
            elif cpu_cores >= 6:   # ãƒŸãƒ‰ãƒ«ãƒ¬ãƒ³ã‚¸
                base_db_count = min(logical_cores * 0.75, 16)
            elif cpu_cores >= 4:   # ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒ¬ãƒ™ãƒ«
                base_db_count = min(logical_cores * 0.6, 12)
            else:  # ä½ã‚¹ãƒšãƒƒã‚¯
                base_db_count = max(2, cpu_cores)
            
            # ãƒ¡ãƒ¢ãƒªãƒ™ãƒ¼ã‚¹ã®èª¿æ•´
            memory_gb = hardware_info['memory_gb']
            if memory_gb >= 128:
                memory_multiplier = 2.2
            elif memory_gb >= 64:
                memory_multiplier = 2.0
            elif memory_gb >= 32:
                memory_multiplier = 1.7
            elif memory_gb >= 16:
                memory_multiplier = 1.4
            elif memory_gb >= 8:
                memory_multiplier = 1.0
            elif memory_gb >= 4:
                memory_multiplier = 0.8
            else:
                memory_multiplier = 0.6
            
            # ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚¿ã‚¤ãƒ—ã«åŸºã¥ãèª¿æ•´
            storage_type = hardware_info['storage_type']
            if storage_type == 'nvme':
                storage_multiplier = 1.4
            elif storage_type == 'ssd':
                storage_multiplier = 1.2
            elif storage_type == 'hybrid':
                storage_multiplier = 1.0
            else:  # HDD
                storage_multiplier = 0.7
            
            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚µã‚¤ã‚ºã«åŸºã¥ãèª¿æ•´
            size_multiplier = self._calculate_data_size_multiplier()
            
            # æœ€çµ‚è¨ˆç®—
            calculated_db_count = int(base_db_count * memory_multiplier * storage_multiplier * size_multiplier)
            
            # å®Ÿéš›ã«å­˜åœ¨ã™ã‚‹DBãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
            existing_db_count = 0
            for i in range(64):  # æœ€å¤§64ã¾ã§ç¢ºèªï¼ˆ34å€‹ã‚’ä½™è£•ã§è¶…ãˆã‚‹ï¼‰
                db_path = self.project_root / f"complete_search_db_{i}.db"
                if db_path.exists() and db_path.stat().st_size > 100000:  # 100KBä»¥ä¸Š=ãƒ‡ãƒ¼ã‚¿ã‚ã‚Š
                    existing_db_count += 1
                elif not db_path.exists():
                    break  # é€£ç¶šã—ã¦ã„ãªã„DBãŒã‚ã‚Œã°åœæ­¢
            
            # å®Ÿç”¨çš„ãªç¯„å›²ã«åˆ¶é™ï¼ˆæ—¢å­˜DBæ•°ã‚’å„ªå…ˆï¼‰
            if existing_db_count > 0:
                optimal_db_count = existing_db_count
                print(f"ğŸ—„ï¸ æ—¢å­˜DBãƒ•ã‚¡ã‚¤ãƒ«æ•°ã«åŸºã¥ãè¨­å®š: {optimal_db_count}å€‹")
            else:
                optimal_db_count = max(2, min(calculated_db_count, 64))  # 2-64å€‹ã®ç¯„å›²ï¼ˆ34å€‹å¯¾å¿œï¼‰
                print(f"ğŸ§® è¨ˆç®—ã«åŸºã¥ãæ–°è¦è¨­å®š: {optimal_db_count}å€‹")
            
            # ãƒ­ã‚°å‡ºåŠ›
            print(f"ğŸ§® é«˜åº¦DBæ•°è¨ˆç®—è©³ç´°:")
            print(f"  CPU: {cpu_cores}ç‰©ç†/{logical_cores}è«–ç†ã‚³ã‚¢ â†’ ãƒ™ãƒ¼ã‚¹DBæ•°: {int(base_db_count)}")
            print(f"  ãƒ¡ãƒ¢ãƒª: {memory_gb:.1f}GB (ä¹—æ•°: {memory_multiplier:.2f})")
            print(f"  ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸: {storage_type} (ä¹—æ•°: {storage_multiplier:.2f})")
            print(f"  ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºä¹—æ•°: {size_multiplier:.2f}")
            print(f"  ç†è«–è¨ˆç®—å€¤: {calculated_db_count}")
            print(f"  å®Ÿéš›ã®æ—¢å­˜DBæ•°: {existing_db_count}")
            print(f"  âœ… æœ€çµ‚æ¡ç”¨DBæ•°: {optimal_db_count}")
            
            return optimal_db_count
            
        except Exception as e:
            print(f"âš ï¸ å‹•çš„DBæ•°è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return self._get_fallback_db_count()

    def _get_comprehensive_hardware_info(self) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ãªãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æƒ…å ±å–å¾—"""
        info = {
            'cpu_cores': 4,
            'logical_cores': 4,
            'memory_gb': 8.0,
            'storage_type': 'unknown'
        }
        
        try:
            if psutil is not None:
                # CPUæƒ…å ±
                info['cpu_cores'] = psutil.cpu_count(logical=False) or 4
                info['logical_cores'] = psutil.cpu_count(logical=True) or 4
                
                # ãƒ¡ãƒ¢ãƒªæƒ…å ±
                memory = psutil.virtual_memory()
                info['memory_gb'] = memory.total / (1024 ** 3)
                
                # ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚¿ã‚¤ãƒ—ã®æ¨å®š
                info['storage_type'] = self._detect_storage_type()
            else:
                # psutilãŒãªã„å ´åˆã®æ¨å®š
                import os
                import multiprocessing
                info['cpu_cores'] = multiprocessing.cpu_count()
                info['logical_cores'] = multiprocessing.cpu_count()
                
        except Exception as e:
            print(f"âš ï¸ ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            
        return info

    def _detect_storage_type(self) -> str:
        """ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚¿ã‚¤ãƒ—ã®æ¤œå‡º"""
        try:
            import platform
            
            # Windowsã®å ´åˆ
            if platform.system() == 'Windows':
                try:
                    import subprocess
                    # PowerShellã§ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚¿ã‚¤ãƒ—ã‚’ç¢ºèª
                    result = subprocess.run([
                        'powershell', '-Command',
                        'Get-PhysicalDisk | Select-Object MediaType, Size | ConvertTo-Json'
                    ], capture_output=True, text=True, timeout=10)
                    
                    if result.returncode == 0:
                        import json
                        disks = json.loads(result.stdout)
                        if isinstance(disks, list) and disks:
                            media_type = disks[0].get('MediaType', '').lower()
                            if 'ssd' in media_type:
                                return 'nvme' if 'nvme' in media_type else 'ssd'
                            elif 'hdd' in media_type:
                                return 'hdd'
                except:
                    pass
            
            # Linuxã®å ´åˆ
            elif platform.system() == 'Linux':
                try:
                    with open('/proc/mounts', 'r') as f:
                        mounts = f.read()
                        if 'nvme' in mounts:
                            return 'nvme'
                        elif 'ssd' in mounts:
                            return 'ssd'
                except:
                    pass
            
            return 'hybrid'  # ä¸æ˜ãªå ´åˆã¯ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ‰±ã„
            
        except Exception:
            return 'unknown'

    def _calculate_data_size_multiplier(self) -> float:
        """ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã«åŸºã¥ãä¹—æ•°è¨ˆç®—ï¼ˆæ—¢å­˜DBãƒ•ã‚¡ã‚¤ãƒ«å«ã‚€ï¼‰"""
        try:
            total_size_mb = 0
            
            # complete_search_db_*.db ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚µã‚¤ã‚ºé›†è¨ˆ
            complete_dbs = list(self.project_root.glob("data_storage/complete_search_db_*.db"))
            for db_file in complete_dbs:
                if db_file.exists():
                    total_size_mb += db_file.stat().st_size / (1024 * 1024)
            
            # image_search_db_*.db ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚µã‚¤ã‚ºé›†è¨ˆ
            image_dbs = list(self.project_root.glob("data_storage/image_search_db_*.db"))
            for db_file in image_dbs:
                if db_file.exists():
                    total_size_mb += db_file.stat().st_size / (1024 * 1024)
            
            print(f"  æ—¢å­˜DBã‚µã‚¤ã‚º: {total_size_mb:.1f}MB (complete: {len(complete_dbs)}å€‹, image: {len(image_dbs)}å€‹)")
            
            # ã‚µã‚¤ã‚ºãƒ™ãƒ¼ã‚¹ã®ä¹—æ•°è¨ˆç®—
            if total_size_mb > 2000:    # 2GBä»¥ä¸Š
                return 2.0
            elif total_size_mb > 1000:  # 1GBä»¥ä¸Š
                return 1.8
            elif total_size_mb > 500:   # 500MBä»¥ä¸Š
                return 1.5
            elif total_size_mb > 200:   # 200MBä»¥ä¸Š
                return 1.3
            elif total_size_mb > 50:    # 50MBä»¥ä¸Š
                return 1.1
            else:
                return 1.0
                
        except Exception as e:
            print(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return 1.0

    def _get_fallback_db_count(self) -> int:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ™‚ã®DBæ•°æ±ºå®š"""
        try:
            import multiprocessing
            cores = multiprocessing.cpu_count()
            return max(4, min(cores, 12))  # 4-12å€‹ã®ç¯„å›²
        except:
            return 6  # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

    def initialize_database(self):
        """å‹•çš„ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹é«˜é€Ÿä¸¦åˆ—åˆæœŸåŒ–ï¼ˆ34å€‹å¯¾å¿œç‰ˆãƒ»é‡è¤‡åˆæœŸåŒ–é˜²æ­¢ï¼‰"""
        # ğŸš€ æ—¢ã«åˆæœŸåŒ–æ¸ˆã¿ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆé«˜é€ŸåŒ–ï¼‰
        if self._db_initialized:
            print(f"âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–æ¸ˆã¿ - ã‚¹ã‚­ãƒƒãƒ—")
            return
        
        start_time = time.time()
        
        try:
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºå®Ÿãªä½œæˆ
            db_dir = self.project_root / "data_storage"
            db_dir.mkdir(parents=True, exist_ok=True)
            debug_logger.info(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç¢ºèª/ä½œæˆ: {db_dir}")
            
            print(f"ğŸ”§ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹é«˜é€Ÿä¸¦åˆ—åˆæœŸåŒ–é–‹å§‹: {self.db_count}å€‹")
            
            def initialize_single_db(db_index: int) -> tuple:
                """å˜ä¸€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åˆæœŸåŒ–"""
                complete_db_path = self.complete_db_paths[db_index]
                db_name = complete_db_path.name
                
                try:
                    # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèªï¼ˆé«˜é€Ÿãƒã‚§ãƒƒã‚¯ï¼‰
                    if complete_db_path.exists() and complete_db_path.stat().st_size > 1024:
                        try:
                            conn = sqlite3.connect(str(complete_db_path), timeout=5.0)
                            cursor = conn.cursor()
                            cursor.execute("SELECT COUNT(*) FROM sqlite_master WHERE type='table' AND name='documents'")
                            if cursor.fetchone()[0] > 0:
                                conn.close()
                                return db_index, True, f"æ—¢å­˜DBä½¿ç”¨: {db_name}"
                            conn.close()
                        except:
                            pass  # æ—¢å­˜DBã«å•é¡ŒãŒã‚ã‚‹å ´åˆã¯æ–°è¦ä½œæˆ
                    
                    # æ–°è¦ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä½œæˆï¼ˆé«˜é€Ÿç‰ˆï¼‰
                    conn = sqlite3.connect(str(complete_db_path), timeout=15.0)
                    cursor = conn.cursor()
                    
                    # é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰è¨­å®š
                    cursor.execute("PRAGMA synchronous=OFF")
                    cursor.execute("PRAGMA journal_mode=MEMORY")
                    cursor.execute("PRAGMA temp_store=MEMORY")
                    cursor.execute("PRAGMA cache_size=10000")

                    # ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆï¼ˆä¸€æ‹¬å®Ÿè¡Œï¼‰
                    cursor.executescript('''
                        CREATE TABLE IF NOT EXISTS documents (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            file_path TEXT UNIQUE NOT NULL,
                            file_name TEXT NOT NULL,
                            content TEXT NOT NULL,
                            file_type TEXT NOT NULL,
                            size INTEGER,
                            modified_time REAL,
                            indexed_time REAL,
                            hash TEXT
                        );
                        
                        CREATE VIRTUAL TABLE IF NOT EXISTS documents_fts USING fts5(
                            file_path,
                            file_name, 
                            content, 
                            file_type,
                            tokenize='trigram'
                        );
                        
                        CREATE INDEX IF NOT EXISTS idx_file_path ON documents(file_path);
                        CREATE INDEX IF NOT EXISTS idx_file_type ON documents(file_type);
                        CREATE INDEX IF NOT EXISTS idx_modified_time ON documents(modified_time);
                    ''')
                    
                    # FTS5æœ€é©åŒ–è¨­å®šï¼ˆã‚¨ãƒ©ãƒ¼ç„¡è¦–ï¼‰
                    for setting in [
                        "INSERT INTO documents_fts(documents_fts, rank) VALUES('pgsz', '4096')",
                        "INSERT INTO documents_fts(documents_fts, rank) VALUES('crisismerge', '16')",
                        "INSERT INTO documents_fts(documents_fts, rank) VALUES('usermerge', '4')",
                        "INSERT INTO documents_fts(documents_fts, rank) VALUES('automerge', '8')"
                    ]:
                        try:
                            cursor.execute(setting)
                        except sqlite3.Error:
                            pass  # è¨­å®šæ¸ˆã¿ã®å ´åˆã¯ç„¡è¦–
                    
                    # è¨­å®šã‚’æœ¬ç•ªãƒ¢ãƒ¼ãƒ‰ã«æˆ»ã™
                    cursor.execute("PRAGMA synchronous=NORMAL")
                    cursor.execute("PRAGMA journal_mode=WAL")
                    
                    conn.commit()
                    conn.close()
                    
                    return db_index, True, f"æ–°è¦ä½œæˆ: {db_name}"
                    
                except Exception as e:
                    return db_index, False, f"ã‚¨ãƒ©ãƒ¼: {db_name} - {str(e)}"
            
            # ä¸¦åˆ—åˆæœŸåŒ–å®Ÿè¡Œ
            success_count = 0
            max_init_workers = min(8, self.db_count)  # åˆæœŸåŒ–ã¯æœ€å¤§8ä¸¦åˆ—ï¼ˆ34å€‹å¯¾å¿œï¼‰
            with concurrent.futures.ThreadPoolExecutor(max_workers=max_init_workers) as executor:
                futures = {executor.submit(initialize_single_db, i): i for i in range(self.db_count)}
                
                for future in concurrent.futures.as_completed(futures):
                    try:
                        db_index, success, message = future.result(timeout=30.0)
                        if success:
                            success_count += 1
                            debug_logger.debug(f"DB{db_index}åˆæœŸåŒ–æˆåŠŸ")
                        else:
                            debug_logger.error(f"DB{db_index}åˆæœŸåŒ–å¤±æ•—: {message}")
                            print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ {db_index+1} åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼")
                    except Exception as e:
                        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {e}")

            initialization_time = time.time() - start_time
            print(f"âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¸¦åˆ—åˆæœŸåŒ–å®Œäº†: {success_count}/{self.db_count}å€‹ ({initialization_time:.2f}ç§’)")
            
            # ğŸš€ åˆæœŸåŒ–å®Œäº†ãƒ•ãƒ©ã‚°ã‚’è¨­å®š
            self._db_initialized = True
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¾©å…ƒï¼ˆä¸¦åˆ—ï¼‰
            self.load_caches()

        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            debug_logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()

    def _calculate_tf_idf_score(self, query_terms: List[str], doc_path: str, content: str) -> float:
        """TF-IDF ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆæ¤œç´¢ç²¾åº¦å‘ä¸Šï¼‰"""
        try:
            if not self._ranking_enabled or not content:
                return 1.0
            
            content_lower = content.lower()
            doc_length = len(content_lower.split())
            
            if doc_length == 0:
                return 0.5
            
            tf_idf_score = 0.0
            
            for term in query_terms:
                term_lower = term.lower()
                
                # TF (Term Frequency): å˜èªã®å‡ºç¾é »åº¦
                term_count = content_lower.count(term_lower)
                tf = term_count / doc_length if doc_length > 0 else 0
                
                # IDF (Inverse Document Frequency): ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—ã¾ãŸã¯è¨ˆç®—
                if term_lower in self._idf_cache:
                    idf = self._idf_cache[term_lower]
                else:
                    # ç°¡æ˜“IDF: ç·ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°ãŒä¸æ˜ãªå ´åˆã¯å›ºå®šå€¤
                    idf = 1.0 if term_count > 0 else 0.0
                    self._idf_cache[term_lower] = idf
                
                # TF-IDF ã‚¹ã‚³ã‚¢
                tf_idf_score += tf * idf
            
            return min(tf_idf_score * 2.0, 3.0)  # æœ€å¤§3.0ã¾ã§
            
        except Exception as e:
            debug_logger.warning(f"TF-IDFè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return 1.0
    
    def _calculate_position_score(self, query: str, file_name: str, content: str) -> float:
        """ä½ç½®æƒ…å ±ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åãƒ»å…ˆé ­å‡ºç¾ã§é«˜ã‚¹ã‚³ã‚¢ï¼‰"""
        try:
            score = 0.0
            query_lower = query.lower()
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åã§ã®å‡ºç¾ï¼ˆæœ€é«˜è©•ä¾¡ï¼‰
            if file_name and query_lower in file_name.lower():
                score += 3.0
                # ãƒ•ã‚¡ã‚¤ãƒ«åã®å…ˆé ­ã«è¿‘ã„ã»ã©é«˜ã‚¹ã‚³ã‚¢
                pos = file_name.lower().find(query_lower)
                if pos == 0:
                    score += 2.0  # ãƒ•ã‚¡ã‚¤ãƒ«åã®æœ€åˆ
                elif pos < 10:
                    score += 1.0  # ãƒ•ã‚¡ã‚¤ãƒ«åã®å‰æ–¹
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã§ã®å‡ºç¾ä½ç½®
            if content:
                content_lower = content.lower()
                pos = content_lower.find(query_lower)
                
                if pos >= 0:
                    # å…ˆé ­200æ–‡å­—ä»¥å†…ã®å‡ºç¾ã¯é«˜è©•ä¾¡
                    if pos < 200:
                        score += 1.5
                    elif pos < 1000:
                        score += 1.0
                    else:
                        score += 0.5
                    
                    # è¤‡æ•°å›å‡ºç¾ã®ãƒœãƒ¼ãƒŠã‚¹
                    occurrences = content_lower.count(query_lower)
                    if occurrences > 1:
                        score += min(occurrences * 0.2, 1.0)  # æœ€å¤§1.0ã¾ã§
            
            return score
            
        except Exception:
            return 0.0
    
    def _calculate_file_type_score(self, file_path: str, query: str) -> float:
        """ãƒ•ã‚¡ã‚¤ãƒ«ç¨®åˆ¥ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆé‡è¦åº¦ã«ã‚ˆã‚‹é‡ã¿ä»˜ã‘ï¼‰"""
        try:
            ext = os.path.splitext(file_path)[1].lower()
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ç¨®åˆ¥ã«ã‚ˆã‚‹é‡è¦åº¦
            high_priority = {'.txt': 1.5, '.md': 1.5, '.doc': 1.3, '.docx': 1.3}
            medium_priority = {'.pdf': 1.2, '.xlsx': 1.1, '.xls': 1.1}
            low_priority = {'.tif': 0.9, '.tiff': 0.9}  # OCRãƒ•ã‚¡ã‚¤ãƒ«ã¯ç²¾åº¦ãŒä½ã„
            
            if ext in high_priority:
                return high_priority[ext]
            elif ext in medium_priority:
                return medium_priority[ext]
            elif ext in low_priority:
                return low_priority[ext]
            else:
                return 1.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
                
        except Exception:
            return 1.0
    
    def _calculate_advanced_relevance_score(self, 
                                           query: str, 
                                           file_path: str,
                                           file_name: str, 
                                           content: str,
                                           base_score: float) -> float:
        """é«˜åº¦ãªé–¢é€£æ€§ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆè¤‡æ•°è¦ç´ ã‚’çµ±åˆï¼‰"""
        try:
            # ã‚¯ã‚¨ãƒªã‚’å˜èªã«åˆ†è§£
            query_terms = query.split()
            
            # å„è¦ç´ ã®ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
            tf_idf_score = self._calculate_tf_idf_score(query_terms, file_path, content)
            position_score = self._calculate_position_score(query, file_name, content)
            file_type_score = self._calculate_file_type_score(file_path, query)
            
            # çµ±åˆã‚¹ã‚³ã‚¢: åŸºæœ¬ã‚¹ã‚³ã‚¢ + TF-IDF + ä½ç½® + ãƒ•ã‚¡ã‚¤ãƒ«ç¨®åˆ¥
            final_score = (
                base_score * 1.0 +        # åŸºæœ¬ã‚¹ã‚³ã‚¢ï¼ˆå…ƒã®é‡ã¿ï¼‰
                tf_idf_score * 0.8 +      # TF-IDFï¼ˆé‡è¦åº¦é«˜ï¼‰
                position_score * 1.2 +    # ä½ç½®æƒ…å ±ï¼ˆæœ€é‡è¦ï¼‰
                file_type_score * 0.5     # ãƒ•ã‚¡ã‚¤ãƒ«ç¨®åˆ¥ï¼ˆè£œåŠ©ï¼‰
            )
            
            return final_score
            
        except Exception as e:
            debug_logger.warning(f"é«˜åº¦ãªã‚¹ã‚³ã‚¢è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return base_score
    
    def _get_search_patterns(self, query: str) -> tuple:
        """ğŸš€ æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãã§é«˜é€ŸåŒ–ï¼‰
        
        Returns:
            (half_width, full_width, normalized, query_patterns)
        """
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        if query in self._pattern_cache:
            return self._pattern_cache[query]
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆ
        patterns = normalize_search_text_ultra(query)
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        self._pattern_cache[query] = patterns
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºåˆ¶é™ï¼ˆLRUé¢¨ï¼‰
        if len(self._pattern_cache) > self._pattern_cache_max_size:
            # æœ€ã‚‚å¤ã„ã‚¨ãƒ³ãƒˆãƒªã‚’å‰Šé™¤
            oldest_key = next(iter(self._pattern_cache))
            del self._pattern_cache[oldest_key]
        
        return patterns

    def _get_db_index_for_file(self, file_path: str) -> int:
        """ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã«åŸºã¥ã„ã¦ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ±ºå®š"""
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’ä½¿ç”¨ã—ã¦åˆ†æ•£
        hash_value = hashlib.md5(file_path.encode('utf-8')).hexdigest()
        return int(hash_value, 16) % self.db_count

    def ultra_fast_search(self, query: str, max_results: int = 5500) -> List[Dict[str, Any]]:
        """æœ€é©åŒ–æ¸ˆã¿æ¤œç´¢ãƒ¡ã‚½ãƒƒãƒ‰ - 3å±¤æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ """
        if not query or not query.strip():
            return []

        query = query.strip()
        start_time = time.time()

        # çµ±è¨ˆæ›´æ–°ï¼ˆè»½é‡åŒ–ï¼‰
        self.stats["search_count"] += 1

        try:
            # ç¬¬1å±¤: å³åº§å±¤æ¤œç´¢ï¼ˆæœ€å„ªå…ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
            immediate_results = self._search_immediate_layer(query)
            if immediate_results:
                self.stats["immediate_layer_hits"] += 1
                self.stats["total_search_time"] += time.time() - start_time
                self._update_average_search_time()
                return immediate_results[:max_results]

            # ç¬¬2å±¤: ãƒ›ãƒƒãƒˆå±¤æ¤œç´¢ï¼ˆä¸€æ™‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
            hot_results = self._search_hot_layer(query)
            if hot_results:
                self.stats["hot_layer_hits"] += 1
                self.stats["total_search_time"] += time.time() - start_time
                self._update_average_search_time()
                
                # å³åº§å±¤ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆéåŒæœŸï¼‰
                threading.Timer(0.001, self._cache_search_results, args=[query, hot_results]).start()
                return hot_results[:max_results]

            # ç¬¬3å±¤: å®Œå…¨æ¤œç´¢ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼‰
            complete_results = self._search_complete_layer(query, max_results)
            self.stats["complete_layer_hits"] += 1
            self.stats["total_search_time"] += time.time() - start_time
            self._update_average_search_time()

            # çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«è¿½åŠ ï¼ˆéåŒæœŸï¼‰
            if complete_results:
                threading.Timer(0.001, self._cache_search_results, args=[query, complete_results]).start()

            return complete_results

        except Exception as e:
            error_time = time.time() - start_time
            self.stats["error_count"] += 1
            self.stats["total_search_time"] += error_time
            debug_logger.error(f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e} ({error_time:.3f}s)")
            print(f"âš ï¸ æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def _cache_search_results(self, query: str, results: List[Dict[str, Any]]):
        """æ¤œç´¢çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        try:
            # å³åº§å±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¸è¿½åŠ 
            if len(self.immediate_cache) < self.max_immediate_cache:
                self.immediate_cache[query] = results.copy()
            else:
                # LRUçš„å‰Šé™¤ï¼ˆæœ€åˆã®ã‚­ãƒ¼ã‚’å‰Šé™¤ï¼‰
                oldest_key = next(iter(self.immediate_cache))
                del self.immediate_cache[oldest_key]
                self.immediate_cache[query] = results.copy()
                
        except Exception as e:
            debug_logger.warning(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def _update_average_search_time(self):
        """å¹³å‡æ¤œç´¢æ™‚é–“ã‚’æ›´æ–°"""
        if self.stats["search_count"] > 0:
            self.stats["avg_search_time"] = self.stats["total_search_time"] / self.stats["search_count"]

    def _get_db_index_for_file(self, file_path: str) -> int:
        """ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã«åŸºã¥ã„ã¦ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ±ºå®š"""
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’ä½¿ç”¨ã—ã¦åˆ†æ•£
        hash_value = hashlib.md5(file_path.encode('utf-8')).hexdigest()
        return int(hash_value, 16) % self.db_count

    def unified_three_layer_search(self,
                                   query: str,
                                   max_results: int = 5500,
                                   file_type_filter: str = "all") -> List[Dict[str, Any]]:
        """æœ€é©åŒ–æ¸ˆã¿3å±¤çµ±åˆæ¤œç´¢ - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹é‡è¦–ç‰ˆ"""
        start_time = time.time()
        results = []

        try:
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸­ã®å‹•ä½œåˆ¶å¾¡ï¼ˆè»½é‡åŒ–ï¼‰
            if self.indexing_in_progress:
                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸­ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥å„ªå…ˆã§é«˜é€Ÿæ¤œç´¢
                results.extend(self._search_immediate_layer(query)[:max_results // 2] or [])
                results.extend(self._search_hot_layer(query)[:max_results // 2] or [])
                
                # çµæœãŒä¸ååˆ†ãªå ´åˆã®ã¿DBæ¤œç´¢
                if len(results) < max_results // 4:
                    try:
                        db_results = self._search_complete_layer(query, max_results // 4)
                        if db_results:
                            results.extend(db_results)
                    except Exception:
                        pass  # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸­ã®DBæ¤œç´¢ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–
                        
            else:
                # é€šå¸¸æ™‚ï¼šæœ€é©åŒ–ã•ã‚ŒãŸ3å±¤æ¤œç´¢
                # å®Œå…¨å±¤å„ªå…ˆæ¤œç´¢ï¼ˆæœ€æ–°ãƒ»æ­£ç¢ºï¼‰
                complete_results = self._search_complete_layer(query, max_results // 2) or []
                results.extend(complete_results)

                # å³åº§å±¤ã§è£œå®Œ
                immediate_results = self._search_immediate_layer(query) or []
                results.extend(immediate_results[:max_results // 4])

                # é«˜é€Ÿå±¤ã§è£œå®Œ
                hot_results = self._search_hot_layer(query) or []
                results.extend(hot_results[:max_results // 4])

            # é‡è¤‡é™¤å»ã¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
            unique_results = self._deduplicate_and_rank_optimized(results)

            # ãƒ•ã‚¡ã‚¤ãƒ«ç¨®é¡ãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨
            if file_type_filter != "all":
                filtered_results = []
                for result in unique_results:
                    file_path = result.get('file_path', '')
                    if file_path.lower().endswith(file_type_filter.lower()):
                        filtered_results.append(result)
                unique_results = filtered_results

            # çµ±è¨ˆæ›´æ–°
            search_time = time.time() - start_time
            self.stats["search_count"] += 1
            self.stats["avg_search_time"] = ((self.stats["avg_search_time"] *
                                              (self.stats["search_count"] - 1) + search_time) /
                                             self.stats["search_count"])

            # è‡ªå‹•æœ€é©åŒ–ãƒã‚§ãƒƒã‚¯ï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸­ä»¥å¤–ï¼‰
            if not self.indexing_in_progress:
                self.check_auto_optimization()

            # æ¤œç´¢çµæœã®å‡ºåŠ›ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            status_msg = "ğŸ“¦ [ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸­]" if self.indexing_in_progress else "âœ… [å®Œäº†]"
            cache_msg = f" ã‚­ãƒ£ãƒƒã‚·ãƒ¥:{len(results) - len(unique_results)}"
            
            # ãƒ¬ã‚¤ãƒ¤ãƒ¼åˆ¥çµæœä»¶æ•°ã‚’è¨ˆç®—ï¼ˆå®Œå…¨å±¤å„ªå…ˆè¡¨ç¤ºï¼‰
            layer_counts = {}
            for result in unique_results:
                layer = result.get('layer', 'unknown')
                if layer.startswith('complete'):
                    layer_key = 'complete'
                else:
                    layer_key = layer
                layer_counts[layer_key] = layer_counts.get(layer_key, 0) + 1
            
            # å®Œå…¨å±¤ã‚’æœ€åˆã«è¡¨ç¤ºã™ã‚‹é †åºã§ä¸¦ã¹æ›¿ãˆ
            ordered_layers = ['complete', 'immediate', 'hot']
            layer_parts = []
            for layer in ordered_layers:
                if layer in layer_counts:
                    layer_parts.append(f"{layer}:{layer_counts[layer]}")
            # ãã®ä»–ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒã‚ã‚Œã°è¿½åŠ 
            for layer, count in layer_counts.items():
                if layer not in ordered_layers:
                    layer_parts.append(f"{layer}:{count}")
            
            layer_msg = " / ".join(layer_parts)
            print(f"ğŸ” {status_msg} 3å±¤çµ±åˆæ¤œç´¢: {len(unique_results)}ä»¶ ({search_time:.4f}ç§’) [ãƒ•ã‚£ãƒ«ã‚¿: {file_type_filter}]{cache_msg} [{layer_msg}]")
            
            return unique_results[:max_results]

        except Exception as e:
            print(f"âŒ çµ±åˆæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def _search_immediate_layer(self, query: str) -> List[Dict[str, Any]]:
        """å³åº§å±¤æ¤œç´¢ - ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆåŠè§’å…¨è§’å¯¾å¿œãƒ»ä¸¦åˆ—åŒ–ç‰ˆï¼‰"""
        results = []

        # ğŸš€ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½¿ç”¨ï¼ˆé«˜é€ŸåŒ–ï¼‰
        half_width, full_width, normalized, query_patterns = self._get_search_patterns(query)

        cache_items = list(self.immediate_cache.items())
        
        # 500ãƒ•ã‚¡ã‚¤ãƒ«/ç§’å¯¾å¿œ: å¤§é‡ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ™‚ã¯ä¸¦åˆ—æ¤œç´¢
        if len(cache_items) > 1000:  # 1000ä»¶ä»¥ä¸Šã§ä¸¦åˆ—åŒ–
            def search_cache_chunk(chunk_items):
                chunk_results = []
                for key, data in chunk_items:
                    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’é™¤å¤–ã—ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¨ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿ã§æ¤œç´¢
                    content_text = data.get('content', '') + ' ' + data.get('file_name', '')
                    if enhanced_search_match(content_text, query_patterns):
                        chunk_results.append({
                            'file_path': data['file_path'],
                            'file_name': data['file_name'],
                            'content_preview': data['content'][:200],
                            'layer': 'immediate',
                            'relevance_score': 1.0
                        })
                return chunk_results

            # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã‚’å‹•çš„èª¿æ•´
            chunk_size = max(200, len(cache_items) // (self.optimal_threads * 2))
            chunks = [cache_items[i:i + chunk_size] for i in range(0, len(cache_items), chunk_size)]

            with concurrent.futures.ThreadPoolExecutor(max_workers=min(self.optimal_threads, 8)) as executor:
                future_to_chunk = {executor.submit(search_cache_chunk, chunk): chunk for chunk in chunks}
                
                for future in concurrent.futures.as_completed(future_to_chunk):
                    try:
                        chunk_results = future.result(timeout=1.0)  # 500ãƒ•ã‚¡ã‚¤ãƒ«/ç§’å¯¾å¿œï¼šé«˜é€ŸåŒ–
                        results.extend(chunk_results)
                    except Exception as e:
                        print(f"âš ï¸ å³åº§å±¤ä¸¦åˆ—æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        else:
            # å°è¦æ¨¡ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯å¾“æ¥é€šã‚Š
            for key, data in cache_items:
                # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’é™¤å¤–ã—ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¨ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿ã§æ¤œç´¢
                content_text = data.get('content_preview', data.get('content', '')) + ' ' + data.get('file_name', '')
                if enhanced_search_match(content_text, query_patterns):
                    # ğŸ¯ é«˜åº¦ãªãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚¹ã‚³ã‚¢é©ç”¨
                    base_score = 1.0
                    advanced_score = self._calculate_advanced_relevance_score(
                        query, data['file_path'], data['file_name'], 
                        data.get('content', ''), base_score
                    )
                    
                    results.append({
                        'file_path': data['file_path'],
                        'file_name': data['file_name'],
                        'content_preview': content_text[:200],
                        'layer': 'immediate',
                        'relevance_score': advanced_score
                    })

        return sorted(results, key=lambda x: x['relevance_score'], reverse=True)

    def _search_hot_layer(self, query: str) -> List[Dict[str, Any]]:
        """é«˜é€Ÿå±¤æ¤œç´¢ - é«˜é€Ÿã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆåŠè§’å…¨è§’å¯¾å¿œãƒ»ä¸¦åˆ—åŒ–ç‰ˆï¼‰"""
        results = []

        # ğŸš€ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½¿ç”¨ï¼ˆé«˜é€ŸåŒ–ï¼‰
        half_width, full_width, normalized, query_patterns = self._get_search_patterns(query)

        cache_items = list(self.hot_cache.items())
        
        # 500ãƒ•ã‚¡ã‚¤ãƒ«/ç§’å¯¾å¿œ: å¤§é‡ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ™‚ã¯ä¸¦åˆ—æ¤œç´¢
        if len(cache_items) > 5000:  # 5000ä»¶ä»¥ä¸Šã§ä¸¦åˆ—åŒ–
            def search_cache_chunk(chunk_items):
                chunk_results = []
                for key, data in chunk_items:
                    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’é™¤å¤–ã—ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¨ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿ã§æ¤œç´¢
                    content_text = data.get('content', '') + ' ' + data.get('file_name', '')
                    if enhanced_search_match(content_text, query_patterns):
                        # ğŸ¯ é«˜åº¦ãªãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚¹ã‚³ã‚¢é©ç”¨
                        base_score = 0.8
                        advanced_score = self._calculate_advanced_relevance_score(
                            query, data['file_path'], data['file_name'], 
                            data.get('content', ''), base_score
                        )
                        
                        chunk_results.append({
                            'file_path': data['file_path'],
                            'file_name': data['file_name'],
                            'content_preview': data['content'][:200],
                            'layer': 'hot',
                            'relevance_score': advanced_score
                        })
                return chunk_results

            # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã‚’å‹•çš„èª¿æ•´
            chunk_size = max(500, len(cache_items) // (self.optimal_threads * 2))
            chunks = [cache_items[i:i + chunk_size] for i in range(0, len(cache_items), chunk_size)]

            with concurrent.futures.ThreadPoolExecutor(max_workers=min(self.optimal_threads, 8)) as executor:
                future_to_chunk = {executor.submit(search_cache_chunk, chunk): chunk for chunk in chunks}
                
                for future in concurrent.futures.as_completed(future_to_chunk):
                    try:
                        chunk_results = future.result(timeout=1.5)  # 500ãƒ•ã‚¡ã‚¤ãƒ«/ç§’å¯¾å¿œï¼šé«˜é€ŸåŒ–
                        results.extend(chunk_results)
                    except Exception as e:
                        print(f"âš ï¸ é«˜é€Ÿå±¤ä¸¦åˆ—æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        else:
            # å°è¦æ¨¡ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯å¾“æ¥é€šã‚Š
            for key, data in cache_items:
                # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’é™¤å¤–ã—ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¨ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿ã§æ¤œç´¢
                content_text = data.get('content', '') + ' ' + data.get('file_name', '')
                if enhanced_search_match(content_text, query_patterns):
                    # ğŸ¯ é«˜åº¦ãªãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚¹ã‚³ã‚¢é©ç”¨
                    base_score = 0.8
                    advanced_score = self._calculate_advanced_relevance_score(
                        query, data['file_path'], data['file_name'], 
                        data.get('content', ''), base_score
                    )
                    
                    results.append({
                        'file_path': data['file_path'],
                        'file_name': data['file_name'],
                        'content_preview': data['content'][:200],
                        'layer': 'hot',
                        'relevance_score': advanced_score
                    })

        return sorted(results, key=lambda x: x['relevance_score'], reverse=True)

    def _search_complete_layer(self, query: str, max_results: int) -> List[Dict[str, Any]]:
        """å®Œå…¨å±¤æ¤œç´¢ - 8å€‹ã®SQLite FTS5ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä¸¦åˆ—æ¤œç´¢ï¼ˆåŠè§’å…¨è§’å¯¾å¿œå¼·åŒ–ï¼‰"""
        results = []

        try:
            # ğŸš€ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½¿ç”¨ï¼ˆé«˜é€ŸåŒ–ï¼‰
            half_width, full_width, normalized, query_patterns = self._get_search_patterns(query)

            # 8å€‹ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä¸¦åˆ—æ¤œç´¢
            def search_single_db(db_index: int) -> List[Dict[str, Any]]:
                db_results = []
                try:
                    complete_db_path = self.complete_db_paths[db_index]
                    conn = sqlite3.connect(complete_db_path, timeout=30.0)
                    # WALãƒ¢ãƒ¼ãƒ‰è¨­å®š
                    conn.execute('PRAGMA journal_mode=WAL')
                    conn.execute('PRAGMA synchronous=NORMAL')
                    cursor = conn.cursor()

                    # å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã§æ¤œç´¢å®Ÿè¡Œï¼ˆå„ªå…ˆåº¦é †ï¼‰
                    search_attempts = 0
                    max_search_attempts = min(len(query_patterns), 3)  # æœ€å¤§3ãƒ‘ã‚¿ãƒ¼ãƒ³ã¾ã§

                    for idx, pattern in enumerate(query_patterns[:max_search_attempts]):
                        try:
                            # ğŸ¯ å³å¯†æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰: å…ƒã®ã‚¯ã‚¨ãƒªãŒ4æ–‡å­—ä»¥ä¸Šã®å ´åˆã¯å®Œå…¨ä¸€è‡´ã‚’å„ªå…ˆ
                            original_query_length = len(query.strip())
                            
                            # ãƒˆãƒ©ã‚¤ã‚°ãƒ©ãƒ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼å¯¾å¿œ: 2æ–‡å­—ä»¥ä¸‹ã¯LIKEæ¤œç´¢ã‚’ä½¿ç”¨
                            if len(pattern) <= 2:
                                # å…ƒã®ã‚¯ã‚¨ãƒªãŒ4æ–‡å­—ä»¥ä¸Šãªã®ã«2æ–‡å­—ä»¥ä¸‹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯é™¤å¤–ï¼ˆå³å¯†æ€§å‘ä¸Šï¼‰
                                if original_query_length >= 4 and idx > 0:
                                    continue
                                    
                                # 2æ–‡å­—ä»¥ä¸‹ã®å ´åˆã¯LIKEæ¤œç´¢ï¼ˆtrigramãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼å¯¾å¿œï¼‰
                                # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’é™¤å¤–ã—ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¨ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿ã§æ¤œç´¢
                                try:
                                    cursor.execute(
                                        '''
                                        SELECT file_path, file_name, content, file_type,
                                               1.0 as relevance_score
                                        FROM documents_fts
                                        WHERE (content LIKE ? OR file_name LIKE ?)
                                        ORDER BY file_name
                                        LIMIT ?
                                    ''', (f'%{pattern}%', f'%{pattern}%', max_results // self.db_count + 20))

                                    rows = cursor.fetchall()

                                    for row in rows:
                                        # LIKEæ¤œç´¢ã®å ´åˆã®ã‚¹ã‚³ã‚¢èª¿æ•´
                                        base_score = 1.0
                                        pattern_bonus = 0.2 * (len(query_patterns) - idx)
                                        like_bonus = 1.5  # LIKEæ¤œç´¢ã¯é«˜ã‚¹ã‚³ã‚¢ï¼ˆæ­£ç¢ºãªãƒãƒƒãƒã®ãŸã‚ï¼‰
                                        
                                        # ğŸ¯ å³å¯†ãƒãƒƒãƒãƒœãƒ¼ãƒŠã‚¹: å…ƒã‚¯ã‚¨ãƒªã¨å®Œå…¨ä¸€è‡´ã®å ´åˆ
                                        exact_match_bonus = 0.0
                                        content_text = (row[2] or '') + ' ' + (row[1] or '')
                                        if query.strip().lower() in content_text.lower():
                                            exact_match_bonus = 2.0
                                        
                                        # å¾“æ¥ã®ã‚¹ã‚³ã‚¢è¨ˆç®—
                                        traditional_score = base_score + pattern_bonus + like_bonus + exact_match_bonus
                                        
                                        # ğŸ¯ é«˜åº¦ãªãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚¹ã‚³ã‚¢é©ç”¨
                                        advanced_score = self._calculate_advanced_relevance_score(
                                            query, row[0], row[1], row[2] or '', traditional_score
                                        )
                                        
                                        result = {
                                            'file_path': row[0],
                                            'file_name': row[1],
                                            'content_preview': row[2][:200] if row[2] else '',
                                            'layer': f'complete_db_{db_index}_like',
                                            'file_type': row[3],
                                            'size': len(row[2]) if row[2] else 0,
                                            'relevance_score': advanced_score
                                        }
                                        db_results.append(result)
                                    
                                    # LIKEæ¤œç´¢ã§çµæœãŒè¦‹ã¤ã‹ã£ãŸã‚‰ã“ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã®æ¤œç´¢ã‚’çµ‚äº†
                                    if rows:
                                        break

                                except Exception as like_error:
                                    debug_logger.warning(f"DB{db_index} LIKEæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {like_error}")
                                    continue
                                
                                continue  # 2æ–‡å­—ä»¥ä¸‹ã®å ´åˆã¯FTSæ¤œç´¢ã‚’ã‚¹ã‚­ãƒƒãƒ—
                            
                            # FTS5ç²¾å¯†æ¤œç´¢ï¼ˆ3æ–‡å­—ä»¥ä¸Šã®å ´åˆã®ã¿ï¼‰
                            search_queries = []
                            
                            # åŸºæœ¬çš„ãªæ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã¿ã‚’ä½¿ç”¨ï¼ˆç²¾åº¦é‡è¦–ï¼‰
                            if len(pattern) >= 3:  # 3æ–‡å­—ä»¥ä¸Šã®å ´åˆã®ã¿FTSæ¤œç´¢å®Ÿè¡Œ
                                # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’é™¤å¤–ã—ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¨ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿ã§æ¤œç´¢
                                search_queries = [
                                    f'content:"{pattern}" OR file_name:"{pattern}"',  # ãƒ•ãƒ¬ãƒ¼ã‚ºæ¤œç´¢ï¼ˆæœ€å„ªå…ˆï¼‰
                                    f'content:{pattern} OR file_name:{pattern}',  # åŸºæœ¬æ¤œç´¢
                                ]
                                
                                # 3æ–‡å­—ä»¥ä¸Šã®å ´åˆã¯å‰æ–¹ä¸€è‡´ã‚‚è¿½åŠ 
                                search_queries.append(f'content:{pattern}* OR file_name:{pattern}*')  # å‰æ–¹ä¸€è‡´æ¤œç´¢

                            for search_query in search_queries:
                                try:
                                    cursor.execute(
                                        '''
                                        SELECT file_path, file_name, content, file_type,
                                               rank AS relevance_score
                                        FROM documents_fts
                                        WHERE documents_fts MATCH ?
                                        ORDER BY rank
                                        LIMIT ?
                                    ''', (search_query, max_results // self.db_count + 20))  # å–å¾—ä»¶æ•°ã‚’å¤§å¹…ã«å‰Šæ¸›

                                    rows = cursor.fetchall()

                                    for row in rows:
                                        # æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ã‚ˆã‚‹ã‚¹ã‚³ã‚¢èª¿æ•´ï¼ˆç²¾åº¦é‡è¦–ï¼‰
                                        base_score = row[4] if len(row) > 4 and row[4] else 0.5
                                        pattern_bonus = 0.1 * (len(query_patterns) - idx)
                                        
                                        # æ¤œç´¢ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ—ã«ã‚ˆã‚‹ãƒœãƒ¼ãƒŠã‚¹
                                        if search_query.startswith('"') and search_query.endswith('"'):
                                            # ãƒ•ãƒ¬ãƒ¼ã‚ºæ¤œç´¢ã¯æœ€é«˜ã‚¹ã‚³ã‚¢
                                            query_bonus = 2.0
                                        elif search_query.endswith('*'):
                                            # å‰æ–¹ä¸€è‡´æ¤œç´¢ã¯ä¸­ç¨‹åº¦ã‚¹ã‚³ã‚¢
                                            query_bonus = 1.0
                                        else:
                                            # åŸºæœ¬æ¤œç´¢ã¯æ¨™æº–ã‚¹ã‚³ã‚¢
                                            query_bonus = 0.5
                                        
                                        # ğŸ¯ å³å¯†ãƒãƒƒãƒãƒœãƒ¼ãƒŠã‚¹: å…ƒã‚¯ã‚¨ãƒªã¨å®Œå…¨ä¸€è‡´ã®å ´åˆ
                                        exact_match_bonus = 0.0
                                        content_text = (row[2] or '') + ' ' + (row[1] or '')
                                        if query.strip().lower() in content_text.lower():
                                            exact_match_bonus = 3.0  # FTSæ¤œç´¢ã§ã®å®Œå…¨ä¸€è‡´ã¯æœ€é«˜è©•ä¾¡
                                        
                                        # ğŸ¯ é–¢é€£æ€§ãƒ•ã‚£ãƒ«ã‚¿: å…ƒã®ã‚¯ã‚¨ãƒªãŒ4æ–‡å­—ä»¥ä¸Šã®å ´åˆã€éƒ¨åˆ†ãƒãƒƒãƒã®ã‚¹ã‚³ã‚¢ã‚’ä¸‹ã’ã‚‹
                                        relevance_penalty = 0.0
                                        if original_query_length >= 4 and idx > 0:
                                            relevance_penalty = -1.0  # éƒ¨åˆ†ãƒãƒƒãƒã®ãƒšãƒŠãƒ«ãƒ†ã‚£
                                        
                                        # å¾“æ¥ã®ã‚¹ã‚³ã‚¢è¨ˆç®—
                                        traditional_score = base_score + pattern_bonus + query_bonus + exact_match_bonus + relevance_penalty
                                        
                                        # ğŸ¯ é«˜åº¦ãªãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚¹ã‚³ã‚¢é©ç”¨
                                        advanced_score = self._calculate_advanced_relevance_score(
                                            query, row[0], row[1], row[2] or '', traditional_score
                                        )
                                        
                                        result = {
                                            'file_path': row[0],
                                            'file_name': row[1],
                                            'content_preview': row[2][:200] if row[2] else '',
                                            'layer': f'complete_db_{db_index}',
                                            'file_type': row[3],
                                            'size': len(row[2]) if row[2] else 0,
                                            'relevance_score': advanced_score
                                        }
                                        db_results.append(result)
                                    
                                    # çµæœãŒè¦‹ã¤ã‹ã£ãŸã‚‰ã“ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã®æ¤œç´¢ã‚’çµ‚äº†
                                    if rows:
                                        break

                                except sqlite3.OperationalError as op_error:
                                    # SQLiteæ“ä½œã‚¨ãƒ©ãƒ¼ï¼ˆç‰¹å®šã®æ¤œç´¢ã‚¯ã‚¨ãƒªãŒã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯æ¬¡ã‚’è©¦è¡Œï¼‰
                                    debug_logger.debug(f"DB{db_index} FTSæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {op_error}")
                                    continue
                                except Exception as search_error:
                                    debug_logger.warning(f"DB{db_index} æ¤œç´¢å‡¦ç†ã‚¨ãƒ©ãƒ¼: {search_error}")
                                    continue

                            search_attempts += 1

                        except Exception:
                            # å€‹åˆ¥ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–ã—ã¦ç¶šè¡Œ
                            continue

                    conn.close()

                except Exception as e:
                    print(f"âš ï¸ DB{db_index}æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
                    if 'conn' in locals():
                        try:
                            conn.close()
                        except:
                            pass

                return db_results

            # 8å€‹ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä¸¦åˆ—ã§æ¤œç´¢
            with concurrent.futures.ThreadPoolExecutor(max_workers=self.db_count) as executor:
                future_to_db = {executor.submit(search_single_db, i): i for i in range(self.db_count)}
                
                for future in concurrent.futures.as_completed(future_to_db):
                    db_index = future_to_db[future]
                    try:
                        db_results = future.result(timeout=10.0)  # 10ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                        results.extend(db_results)
                    except Exception as e:
                        print(f"âš ï¸ DB{db_index}ä¸¦åˆ—æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")

            # é‡è¤‡é™¤å»ï¼ˆfile_pathãƒ™ãƒ¼ã‚¹ï¼‰ã¨ã‚¹ã‚³ã‚¢é †ã‚½ãƒ¼ãƒˆ
            seen_paths = set()
            unique_results = []
            for result in sorted(results, key=lambda x: x.get('relevance_score', 0) if isinstance(x, dict) else 0, reverse=True):
                # result ãŒ dict å½¢å¼ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
                if isinstance(result, dict) and 'file_path' in result:
                    if result['file_path'] not in seen_paths:
                        unique_results.append(result)
                        seen_paths.add(result['file_path'])
                else:
                    # ãƒ‡ãƒãƒƒã‚°: édictå½¢å¼ã®çµæœã‚’å‡ºåŠ›
                    debug_logger.warning(f"édictå½¢å¼ã®æ¤œç´¢çµæœã‚’æ¤œå‡º: {type(result)} - {result}")

            print(f"ğŸ” 8ä¸¦åˆ—DBæ¤œç´¢å®Œäº†: {len(results)}ä»¶(ç”Ÿ)/é‡è¤‡é™¤å»å¾Œ{len(unique_results)}ä»¶ | ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°:{len(query_patterns)}")
            
            # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼šçµæœãŒ0ä»¶ã®å ´åˆã€DBçŠ¶æ…‹ã‚’ç¢ºèª
            if len(unique_results) == 0:
                print("âš ï¸ å®Œå…¨å±¤æ¤œç´¢çµæœãŒ0ä»¶ - ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çŠ¶æ…‹ã‚’ç¢ºèªä¸­...")
                try:
                    # å„ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ã‚’ç¢ºèª
                    for db_index in range(min(2, self.db_count)):  # æœ€åˆã®2ã¤ã®DBã ã‘ç¢ºèª
                        db_path = self.project_root / f'complete_search_db_{db_index}.db'
                        if db_path.exists():
                            conn = sqlite3.connect(str(db_path))
                            cursor = conn.cursor()
                            cursor.execute('SELECT COUNT(*) FROM documents_fts')
                            count = cursor.fetchone()[0]
                            print(f"  DB{db_index}: {count}ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ")
                            conn.close()
                except Exception as debug_error:
                    print(f"  ãƒ‡ãƒãƒƒã‚°æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {debug_error}")

        except Exception as e:
            print(f"âš ï¸ å®Œå…¨å±¤ä¸¦åˆ—æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")

        return unique_results[:max_results]

    def _deduplicate_and_rank_optimized(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æœ€é©åŒ–ç‰ˆé‡è¤‡é™¤å»ã¨ãƒ©ãƒ³ã‚­ãƒ³ã‚° - é«˜é€ŸåŒ–é‡è¦–"""
        if not results:
            return []
            
        seen_paths = set()
        unique_results = []
        
        # ãƒ¬ã‚¤ãƒ¤ãƒ¼å„ªå…ˆåº¦ã‚’äº‹å‰è¨ˆç®—
        priority_map = {
            'complete': 1000,
            'immediate': 100, 
            'hot': 10
        }
        
        # ãƒ¬ã‚¤ãƒ¤ãƒ¼åã‹ã‚‰ã®å„ªå…ˆåº¦å–å¾—ï¼ˆæœ€é©åŒ–ï¼‰
        def get_priority(result):
            if not isinstance(result, dict):
                return (0, 0)
            layer = result.get('layer', 'unknown')
            # complete_db_0ç­‰ã®å ´åˆã¯completeã¨ã—ã¦æ‰±ã†
            layer_base = layer.split('_')[0] if '_' in layer else layer
            priority = priority_map.get(layer_base, 1)
            score = result.get('relevance_score', 0)
            return (priority, score)
        
        # ã‚½ãƒ¼ãƒˆï¼ˆæœ€é©åŒ–ï¼‰
        results.sort(key=get_priority, reverse=True)
        
        # é‡è¤‡é™¤å»ï¼ˆæœ€é©åŒ–ï¼‰
        for result in results:
            if isinstance(result, dict) and 'file_path' in result:
                path = result['file_path']
                if path not in seen_paths:
                    seen_paths.add(path)
                    unique_results.append(result)

        return unique_results

    def _deduplicate_and_rank(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """é‡è¤‡é™¤å»ã¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°"""
        seen_paths = set()
        unique_results = []

        # ãƒ¬ã‚¤ãƒ¤ãƒ¼å„ªå…ˆåº¦: complete > immediate > hot (å®Œå…¨å±¤æœ€å„ªå…ˆ)
        def get_layer_priority(layer_name):
            if layer_name.startswith('complete'):  # complete_db_0, complete_db_1 ãªã©ã«å¯¾å¿œ
                return 1000  # å®Œå…¨å±¤ã‚’åœ§å€’çš„å„ªå…ˆ
            elif layer_name == 'immediate':
                return 100   # å³åº§å±¤ã‚’æ¬¡ç‚¹
            elif layer_name == 'hot':
                return 10    # é«˜é€Ÿå±¤ã‚’æœ€å¾Œï¼ˆå¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
            else:
                return 1

        # å®Œå…¨å±¤ã‚’çµ¶å¯¾å„ªå…ˆã™ã‚‹ã‚½ãƒ¼ãƒˆï¼ˆãƒ¬ã‚¤ãƒ¤ãƒ¼å„ªå…ˆåº¦ >> ã‚¹ã‚³ã‚¢ï¼‰
        sorted_results = sorted(results,
                                key=lambda x:
                                (get_layer_priority(x.get('layer', 'unknown')) if isinstance(x, dict) else 0, 
                                 x.get('relevance_score', 0) if isinstance(x, dict) else 0),
                                reverse=True)

        for result in sorted_results:
            # result ãŒ dict å½¢å¼ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
            if isinstance(result, dict) and 'file_path' in result:
                if result['file_path'] not in seen_paths:
                    seen_paths.add(result['file_path'])
                    unique_results.append(result)
            else:
                # ãƒ‡ãƒãƒƒã‚°: édictå½¢å¼ã®çµæœã‚’å‡ºåŠ›
                debug_logger.warning(f"é‡è¤‡é™¤å»ã§édictå½¢å¼ã®çµæœã‚’æ¤œå‡º: {type(result)} - {result}")

        return unique_results

    def live_progressive_index_file(self, file_path: str) -> bool:
        """ãƒ©ã‚¤ãƒ–ãƒ—ãƒ­ã‚°ãƒ¬ãƒƒã‚·ãƒ–ãƒ•ã‚¡ã‚¤ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°å¼·åŒ–ï¼‰"""
        debug_logger.debug(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é–‹å§‹: {file_path}")

        # ã‚­ãƒ£ãƒ³ã‚»ãƒ«ãƒã‚§ãƒƒã‚¯
        if hasattr(self, 'indexing_cancelled') and self.indexing_cancelled:
            debug_logger.debug(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‡¦ç†ãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ: {file_path}")
            return False

        try:
            file_path_obj = Path(file_path)

            # macOSéš ã—ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ._ã§å§‹ã¾ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ã‚’ã‚¹ã‚­ãƒƒãƒ—
            if file_path_obj.name.startswith('._'):
                debug_logger.debug(f"macOSéš ã—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—: {file_path_obj.name}")
                return False

            # ãã®ä»–ã®éš ã—ãƒ•ã‚¡ã‚¤ãƒ«ãƒ»ã‚·ã‚¹ãƒ†ãƒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ã‚¹ã‚­ãƒƒãƒ—
            if file_path_obj.name.startswith('.DS_Store') or file_path_obj.name.startswith('Thumbs.db'):
                debug_logger.debug(f"ã‚·ã‚¹ãƒ†ãƒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—: {file_path_obj.name}")
                return False

            if not file_path_obj.exists():
                debug_logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {file_path}")
                return False

            # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±å–å¾—
            stat = file_path_obj.stat()
            file_size = stat.st_size
            modified_time = stat.st_mtime

            debug_logger.debug(f"ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ± - ã‚µã‚¤ã‚º: {file_size}, æ›´æ–°æ™‚åˆ»: {modified_time}")

            # ğŸ”¥ å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«ã®æ—©æœŸã‚¹ã‚­ãƒƒãƒ—ï¼ˆ500MBä»¥ä¸Šï¼‰
            if file_size > 500 * 1024 * 1024:
                debug_logger.warning(f"è¶…å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—: {file_path} ({file_size/(1024*1024):.1f}MB)")
                return False
            
            # ğŸš€ 3MBä»¥ä¸Šã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆè¶…é«˜é€Ÿå‡¦ç†ï¼‰
            if file_size >= 3 * 1024 * 1024:
                debug_logger.info(f"å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ« - ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {file_path} ({file_size/(1024*1024):.1f}MB)")
                # ãƒ•ã‚¡ã‚¤ãƒ«åã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
                content = file_path_obj.name  # ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿
            else:
                # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹æŠ½å‡º
                debug_logger.debug(f"ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºé–‹å§‹: {file_path}")
                content = self._extract_file_content(file_path)
            if not content:
                debug_logger.warning(f"ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒç©ºã¾ãŸã¯æŠ½å‡ºå¤±æ•—: {file_path}")
                return False

            debug_logger.info(f"ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºæˆåŠŸ: {file_path} ({len(content)}æ–‡å­—)")
            file_hash = hashlib.md5(content.encode('utf-8', errors='ignore')).hexdigest()
            debug_logger.debug(f"ãƒãƒƒã‚·ãƒ¥è¨ˆç®—å®Œäº†: {file_hash[:8]}...")

            # ğŸ†• 3å±¤æ§‹é€ æœ€é©åŒ–: é‡è¤‡å‰Šé™¤ã¨å½¹å‰²æ˜ç¢ºåŒ–
            # Phase 1: å³åº§å±¤ï¼ˆæ¤œç´¢ã‚­ãƒ£ãƒƒã‚·ãƒ¥å°‚ç”¨ - çŸ­æ™‚é–“ã®ã¿ä¿æŒï¼‰
            debug_logger.debug("å³åº§å±¤ã¸ã®ä¸€æ™‚è¿½åŠ é–‹å§‹")
            
            # UTF-8å¯¾å¿œã®å®‰å…¨ãªæ–‡å­—åˆ—åˆ‡ã‚Šå–ã‚Šå‡¦ç†
            def safe_truncate_utf8(text: str, max_length: int) -> str:
                """UTF-8æ–‡å­—åˆ—ã‚’å®‰å…¨ã«åˆ‡ã‚Šå–ã‚‹ï¼ˆæ—¥æœ¬èªå¯¾å¿œï¼‰"""
                if len(text) <= max_length:
                    return text
                # æ–‡å­—å¢ƒç•Œã§å®‰å…¨ã«åˆ‡ã‚Šå–ã‚Š
                truncated = text[:max_length]
                # æœ€å¾Œã®æ–‡å­—ãŒä¸å®Œå…¨ãªå ´åˆã¯1æ–‡å­—å‰Šã‚‹
                try:
                    truncated.encode('utf-8')
                    return truncated
                except UnicodeEncodeError:
                    return text[:max_length-1] if max_length > 1 else ""
            
            immediate_data = {
                'file_path': str(file_path),
                'file_name': file_path_obj.name,
                'content_preview': safe_truncate_utf8(content, 500),  # UTF-8å¯¾å¿œå®‰å…¨åˆ‡ã‚Šå–ã‚Š
                'file_type': file_path_obj.suffix.lower(),
                'size': file_size,
                'indexed_time': time.time(),
                'layer': 'immediate'
            }

            # å³åº§å±¤ã¯ä¸€æ™‚çš„ãªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ã¿ï¼ˆé‡è¤‡å‰Šé™¤ï¼‰
            self.immediate_cache[str(file_path)] = immediate_data
            debug_logger.debug(f"å³åº§å±¤ä¸€æ™‚è¿½åŠ å®Œäº†: {file_path}")

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºåˆ¶é™ï¼ˆæœ€å¤§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç‰ˆï¼‰
            if len(self.immediate_cache) > self.max_immediate_cache:
                # åŠ¹ç‡çš„ãªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆä¸€åº¦ã«è¤‡æ•°å‰Šé™¤ï¼‰
                cleanup_count = max(1, self.max_immediate_cache // 10)  # 10%å‰Šé™¤
                sorted_items = sorted(self.immediate_cache.items(),
                                    key=lambda x: x[1]['indexed_time'])
                
                for i in range(cleanup_count):
                    if i < len(sorted_items):
                        oldest_key = sorted_items[i][0]
                        del self.immediate_cache[oldest_key]
                
                debug_logger.debug(f"å³åº§å±¤ãƒãƒƒãƒã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—: {cleanup_count}ä»¶å‰Šé™¤")            # Phase 2: é«˜é€Ÿå±¤ç§»å‹•ï¼ˆå³åº§å±¤ã‹ã‚‰ç§»å‹• - é‡è¤‡å‰Šé™¤ï¼‰
            debug_logger.debug("é«˜é€Ÿå±¤ç§»å‹•ã‚¿ã‚¤ãƒãƒ¼è¨­å®š")
            threading.Timer(1.0, self._move_to_hot_layer,
                            args=[file_path, content]).start()

            # Phase 3: å®Œå…¨å±¤ç§»å‹•ï¼ˆé«˜é€Ÿå±¤ã‹ã‚‰ç§»å‹• - é‡è¤‡å‰Šé™¤ï¼‰
            debug_logger.debug("å®Œå…¨å±¤ç§»å‹•ã‚¿ã‚¤ãƒãƒ¼è¨­å®š")
            threading.Timer(5.0, self._move_to_complete_layer,
                            args=[file_path, content, file_hash]).start()

            self.stats["indexed_files"] += 1
            debug_logger.info(f"3å±¤æ§‹é€ æœ€é©åŒ–ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å®Œäº†: {file_path}")
            return True

        except Exception as e:
            debug_logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
            return False

    def _move_to_hot_layer(self, file_path: str, content: str):
        """ğŸ”„ é«˜é€Ÿå±¤ç§»å‹•ï¼ˆå³åº§å±¤ã‹ã‚‰ç§»å‹• - é‡è¤‡å‰Šé™¤ï¼‰"""
        try:
            # å³åº§å±¤ã‹ã‚‰å‰Šé™¤ï¼ˆé‡è¤‡å‰Šé™¤ï¼‰
            if file_path in self.immediate_cache:
                base_data = self.immediate_cache[file_path]
                del self.immediate_cache[file_path]
                debug_logger.debug(f"å³åº§å±¤ã‹ã‚‰å‰Šé™¤: {os.path.basename(file_path)}")
            else:
                # å³åº§å±¤ã«ãªã„å ´åˆã¯åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ã‚’å†æ§‹ç¯‰
                base_data = {
                    'file_name': os.path.basename(file_path),
                    'file_type': Path(file_path).suffix.lower(),
                    'size': os.path.getsize(file_path) if os.path.exists(file_path) else 0,
                    'indexed_time': time.time()
                }

            # é«˜é€Ÿå±¤ãƒ‡ãƒ¼ã‚¿ä½œæˆï¼ˆä¸­æœŸä¿å­˜ç”¨ - ã‚ˆã‚Šå¤šãã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ï¼‰
            hot_data = base_data.copy()
            hot_data.update({
                'file_path': file_path,
                'content': content[:10000],  # ã‚ˆã‚Šè©³ç´°ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ä¿å­˜
                'layer': 'hot',
                'moved_from_immediate': time.time()
            })

            self.hot_cache[file_path] = hot_data

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºåˆ¶é™
            if len(self.hot_cache) > self.max_hot_cache:
                oldest_key = min(self.hot_cache.keys(),
                                 key=lambda k: self.hot_cache[k]['indexed_time'])
                del self.hot_cache[oldest_key]
                debug_logger.debug(f"é«˜é€Ÿå±¤å¤ã„ã‚¨ãƒ³ãƒˆãƒªå‰Šé™¤: {oldest_key}")

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å®šæœŸä¿å­˜ï¼ˆãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ï¼‰- é »åº¦ã‚’åˆ¶é™
            if not hasattr(self, '_last_save_time'):
                self._last_save_time = 0
            
            current_time = time.time()
            if current_time - self._last_save_time > 5.0 and not self.shutdown_requested:  # 5ç§’é–“éš”ã«åˆ¶é™ + ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ãƒã‚§ãƒƒã‚¯
                self._last_save_time = current_time
                timer = threading.Timer(1.0, self.save_caches)
                self._background_threads.append(timer)  # è¿½è·¡ãƒªã‚¹ãƒˆã«è¿½åŠ 
                timer.start()
            
            debug_logger.debug(f"é«˜é€Ÿå±¤ç§»å‹•å®Œäº†: {os.path.basename(file_path)}")

        except Exception as e:
            print(f"âš ï¸ é«˜é€Ÿå±¤ç§»å‹•ã‚¨ãƒ©ãƒ¼: {e}")
            debug_logger.error(f"é«˜é€Ÿå±¤ç§»å‹•ã‚¨ãƒ©ãƒ¼: {e}")

    def _move_to_complete_layer(self, file_path: str, content: str, file_hash: str):
        """ğŸ”„ å®Œå…¨å±¤ç§»å‹•ï¼ˆé«˜é€Ÿå±¤ã‹ã‚‰ç§»å‹• - é‡è¤‡å‰Šé™¤ï¼‰"""
        try:
            # é«˜é€Ÿå±¤ã‹ã‚‰å‰Šé™¤ï¼ˆé‡è¤‡å‰Šé™¤ï¼‰
            if file_path in self.hot_cache:
                base_data = self.hot_cache[file_path]
                del self.hot_cache[file_path]
                debug_logger.debug(f"é«˜é€Ÿå±¤ã‹ã‚‰å‰Šé™¤: {os.path.basename(file_path)}")
            else:
                # é«˜é€Ÿå±¤ã«ãªã„å ´åˆã¯åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ã‚’å†æ§‹ç¯‰
                base_data = {
                    'file_name': os.path.basename(file_path),
                    'file_type': Path(file_path).suffix.lower(),
                    'size': os.path.getsize(file_path) if os.path.exists(file_path) else 0,
                    'indexed_time': time.time()
                }

            # å®Œå…¨å±¤ã¸ç§»å‹•ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®æ°¸ç¶šä¿å­˜ï¼‰
            self._add_to_complete_layer(file_path, content, base_data, file_hash)
            debug_logger.debug(f"å®Œå…¨å±¤ç§»å‹•å®Œäº†: {os.path.basename(file_path)}")

        except Exception as e:
            print(f"âš ï¸ å®Œå…¨å±¤ç§»å‹•ã‚¨ãƒ©ãƒ¼: {e}")
            debug_logger.error(f"å®Œå…¨å±¤ç§»å‹•ã‚¨ãƒ©ãƒ¼: {e}")

    def _add_to_hot_layer(self, file_path: str, content: str, base_data: Dict[str, Any]):
        """é«˜é€Ÿå±¤è¿½åŠ ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜æ©Ÿèƒ½ä»˜ãï¼‰"""
        try:
            hot_data = base_data.copy()
            hot_data['content'] = content[:10000]  # ãƒãƒƒãƒã‚µã‚¤ã‚º400ã«åˆã‚ã›ã¦æ‹¡å¼µï¼ˆæœ€åˆã®10000æ–‡å­—ï¼‰

            self.hot_cache[file_path] = hot_data

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºåˆ¶é™
            if len(self.hot_cache) > self.max_hot_cache:
                oldest_key = min(self.hot_cache.keys(),
                                 key=lambda k: self.hot_cache[k]['indexed_time'])
                del self.hot_cache[oldest_key]            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å®šæœŸä¿å­˜ï¼ˆãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ï¼‰- é »åº¦ã‚’åˆ¶é™
            if not hasattr(self, '_last_save_time'):
                self._last_save_time = 0
            
            current_time = time.time()
            if current_time - self._last_save_time > 5.0 and not self.shutdown_requested:  # 5ç§’é–“éš”ã«åˆ¶é™ + ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ãƒã‚§ãƒƒã‚¯
                self._last_save_time = current_time
                timer = threading.Timer(1.0, self.save_caches)
                self._background_threads.append(timer)  # è¿½è·¡ãƒªã‚¹ãƒˆã«è¿½åŠ 
                timer.start()
            
            debug_logger.debug(f"é«˜é€Ÿå±¤è¿½åŠ å®Œäº†: {os.path.basename(file_path)}")

        except Exception as e:
            print(f"âš ï¸ é«˜é€Ÿå±¤è¿½åŠ ã‚¨ãƒ©ãƒ¼: {e}")
            debug_logger.error(f"é«˜é€Ÿå±¤è¿½åŠ ã‚¨ãƒ©ãƒ¼: {e}")

    def _bulk_add_to_complete_layer(self, file_data_list: List[Dict[str, Any]]) -> Dict[str, int]:
        """ğŸš€ ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆç‰ˆå®Œå…¨å±¤è¿½åŠ ï¼ˆ100å€é«˜é€ŸåŒ–ï¼‰
        
        Args:
            file_data_list: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ [{'file_path': str, 'content': str, 'base_data': dict, 'file_hash': str}, ...]
        
        Returns:
            {'success': int, 'errors': int}
        """
        if not file_data_list:
            return {'success': 0, 'errors': 0}
        
        success_count = 0
        error_count = 0
        
        # DBã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        db_groups = {}
        for file_data in file_data_list:
            file_path = file_data['file_path']
            db_index = self._get_db_index_for_file(file_path)
            if db_index not in db_groups:
                db_groups[db_index] = []
            db_groups[db_index].append(file_data)
        
        # å„DBã«å¯¾ã—ã¦ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆå®Ÿè¡Œ
        for db_index, group_data in db_groups.items():
            try:
                complete_db_path = self.complete_db_paths[db_index]
                
                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
                conn = sqlite3.connect(
                    str(complete_db_path),
                    timeout=120.0,
                    check_same_thread=False
                )
                
                # é«˜é€Ÿè¨­å®š
                conn.execute("PRAGMA journal_mode=WAL")
                conn.execute("PRAGMA synchronous=NORMAL")
                conn.execute("PRAGMA cache_size=50000")
                conn.execute("PRAGMA temp_store=MEMORY")
                
                cursor = conn.cursor()
                
                # ğŸš€ ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³é–‹å§‹ï¼ˆãƒãƒ«ã‚¯å‡¦ç†ã§100å€é«˜é€ŸåŒ–ï¼‰
                conn.execute("BEGIN EXCLUSIVE")
                
                # ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™
                documents_data = []
                fts_data = []
                
                for file_data in group_data:
                    file_path = file_data['file_path']
                    content = file_data['content']
                    base_data = file_data['base_data']
                    file_hash = file_data['file_hash']
                    
                    # å®‰å…¨ãªæ–‡å­—åˆ—å‡¦ç†
                    safe_content = content[:1000000] if content else ""
                    safe_file_name = base_data.get('file_name', os.path.basename(file_path))[:500]
                    safe_file_type = base_data.get('file_type', Path(file_path).suffix.lower())[:50]
                    
                    # æ—¢å­˜ãƒã‚§ãƒƒã‚¯
                    cursor.execute('SELECT id FROM documents WHERE file_path = ?', (file_path,))
                    existing = cursor.fetchone()
                    
                    if existing:
                        # æ›´æ–°ãƒ‡ãƒ¼ã‚¿
                        cursor.execute(
                            '''UPDATE documents 
                               SET content = ?, file_name = ?, file_type = ?, size = ?, 
                                   modified_time = ?, indexed_time = ?, hash = ?
                               WHERE file_path = ?''',
                            (safe_content, safe_file_name, safe_file_type, base_data['size'],
                             time.time(), time.time(), file_hash, file_path)
                        )
                        # FTSæ›´æ–°
                        cursor.execute('DELETE FROM documents_fts WHERE rowid = ?', (existing[0],))
                        fts_data.append((existing[0], file_path, safe_file_name, safe_content, safe_file_type))
                    else:
                        # æ–°è¦ãƒ‡ãƒ¼ã‚¿
                        documents_data.append((
                            file_path, safe_file_name, safe_content, safe_file_type,
                            base_data['size'], time.time(), time.time(), file_hash
                        ))
                
                # ğŸš€ ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆå®Ÿè¡Œï¼ˆexecutemanyã§é«˜é€ŸåŒ–ï¼‰
                if documents_data:
                    cursor.executemany(
                        '''INSERT INTO documents (file_path, file_name, content, file_type, size, 
                                                 modified_time, indexed_time, hash)
                           VALUES (?, ?, ?, ?, ?, ?, ?, ?)''',
                        documents_data
                    )
                    
                    # æŒ¿å…¥ã•ã‚ŒãŸIDã‚’å–å¾—ã—ã¦FTSã«è¿½åŠ 
                    for doc_data in documents_data:
                        cursor.execute('SELECT id FROM documents WHERE file_path = ?', (doc_data[0],))
                        doc_id = cursor.fetchone()
                        if doc_id:
                            fts_data.append((doc_id[0], doc_data[0], doc_data[1], doc_data[2], doc_data[3]))
                
                # FTSãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆ
                if fts_data:
                    cursor.executemany(
                        '''INSERT INTO documents_fts(rowid, file_path, file_name, content, file_type)
                           VALUES (?, ?, ?, ?, ?)''',
                        fts_data
                    )
                
                # ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ã‚³ãƒŸãƒƒãƒˆ
                conn.commit()
                success_count += len(group_data)
                
                debug_logger.info(f"ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆæˆåŠŸ: DB{db_index}, {len(group_data)}ä»¶")
                print(f"âœ… DB{db_index}ãƒãƒ«ã‚¯å®Œå…¨å±¤ç§»è¡Œå®Œäº†: {len(group_data)}ä»¶")
                
                conn.close()
                
            except Exception as e:
                error_count += len(group_data)
                debug_logger.error(f"ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: DB{db_index} - {e}")
                print(f"âš ï¸ DB{db_index}ãƒãƒ«ã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
                if 'conn' in locals():
                    try:
                        conn.rollback()
                        conn.close()
                    except:
                        pass
        
        return {'success': success_count, 'errors': error_count}

    def _add_to_complete_layer(self, file_path: str, content: str, base_data: Dict[str, Any],
                               file_hash: str):
        """ğŸ”„ å®Œå…¨å±¤è¿½åŠ ï¼ˆ8ä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç‰ˆãƒ»æ¥ç¶šå¼·åŒ–ç‰ˆãƒ»é‡è¤‡å‰Šé™¤å¯¾å¿œï¼‰"""
        debug_logger.debug(f"å®Œå…¨å±¤è¿½åŠ é–‹å§‹: {file_path}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ±ºå®š
        db_index = self._get_db_index_for_file(file_path)
        complete_db_path = self.complete_db_paths[db_index]
        
        debug_logger.debug(f"ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹: DB{db_index} - {complete_db_path.name}")
        
        print(f"ğŸ”„ å®Œå…¨å±¤ï¼ˆDB{db_index}ï¼‰ç§»è¡Œé–‹å§‹: {os.path.basename(file_path)}")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèªï¼ˆå¼·åŒ–ç‰ˆï¼‰
        if not complete_db_path.exists():
            debug_logger.warning(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“ - ä½œæˆã—ã¾ã™: {complete_db_path}")
            print(f"ğŸ”§ DB{db_index}ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆä¸­: {complete_db_path.name}")
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å®‰å…¨ã«ä½œæˆ
            try:
                # è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ç¢ºèªãƒ»ä½œæˆ
                complete_db_path.parent.mkdir(parents=True, exist_ok=True)
                
                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–
                init_conn = sqlite3.connect(str(complete_db_path), timeout=30.0)
                init_cursor = init_conn.cursor()
                
                # ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
                init_cursor.execute('''
                    CREATE TABLE IF NOT EXISTS documents (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        file_path TEXT UNIQUE NOT NULL,
                        file_name TEXT NOT NULL,
                        content TEXT NOT NULL,
                        file_type TEXT NOT NULL,
                        size INTEGER,
                        modified_time REAL,
                        indexed_time REAL,
                        hash TEXT
                    )
                ''')
                
                # FTS5å…¨æ–‡æ¤œç´¢ãƒ†ãƒ¼ãƒ–ãƒ«
                init_cursor.execute('''
                    CREATE VIRTUAL TABLE IF NOT EXISTS documents_fts USING fts5(
                        file_path,
                        file_name, 
                        content, 
                        file_type,
                        tokenize='trigram'
                    )
                ''')
                
                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
                init_cursor.execute('CREATE INDEX IF NOT EXISTS idx_file_path ON documents(file_path)')
                init_cursor.execute('CREATE INDEX IF NOT EXISTS idx_file_type ON documents(file_type)')
                init_cursor.execute('CREATE INDEX IF NOT EXISTS idx_modified_time ON documents(modified_time)')
                
                init_conn.commit()
                init_conn.close()
                
                print(f"âœ… DB{db_index}ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå®Œäº†: {complete_db_path.name}")
                debug_logger.info(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆæˆåŠŸ: {complete_db_path}")
                
            except Exception as create_error:
                debug_logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {create_error}")
                print(f"âŒ DB{db_index}ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå¤±æ•—: {create_error}")
                return
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ç¢ºèª
        if not os.access(complete_db_path, os.R_OK | os.W_OK):
            debug_logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ãªã—: {complete_db_path}")
            print(f"âŒ DB{db_index}ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ã‚¨ãƒ©ãƒ¼: {complete_db_path.name}")
            return
        
        max_retries = 8  # ãƒªãƒˆãƒ©ã‚¤å›æ•°ã‚’å¢—åŠ 
        retry_delay = 0.05  # åˆæœŸé…å»¶ã‚’çŸ­ç¸®

        for attempt in range(max_retries):
            debug_logger.debug(f"å®Œå…¨å±¤è¿½åŠ è©¦è¡Œ {attempt + 1}/{max_retries}: {file_path} (DB{db_index})")
            conn = None
            try:
                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šï¼ˆå¼·åŒ–ç‰ˆè¨­å®šï¼‰
                debug_logger.debug(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šé–‹å§‹: {complete_db_path}")
                
                # æ¥ç¶šå‰ã®è¿½åŠ ãƒã‚§ãƒƒã‚¯
                if not complete_db_path.exists():
                    debug_logger.error(f"æ¥ç¶šç›´å‰ãƒã‚§ãƒƒã‚¯: ãƒ•ã‚¡ã‚¤ãƒ«ä¸å­˜åœ¨ {complete_db_path}")
                    break
                
                # SQLiteæ¥ç¶šï¼ˆæ’ä»–åˆ¶å¾¡å¼·åŒ–ï¼‰
                conn = sqlite3.connect(
                    str(complete_db_path),  # æ˜ç¤ºçš„ã«æ–‡å­—åˆ—å¤‰æ›
                    timeout=120.0,  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå»¶é•·
                    check_same_thread=False  # ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ãƒ†ã‚£å‘ä¸Š
                )
                
                # WALãƒ¢ãƒ¼ãƒ‰ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨­å®šï¼ˆå¼·åŒ–ç‰ˆï¼‰
                conn.execute("PRAGMA journal_mode=WAL")
                conn.execute("PRAGMA synchronous=NORMAL")
                conn.execute("PRAGMA cache_size=20000")  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºå¢—åŠ 
                conn.execute("PRAGMA temp_store=MEMORY")
                conn.execute("PRAGMA busy_timeout=120000")  # 120ç§’å¾…æ©Ÿ
                conn.execute("PRAGMA wal_autocheckpoint=1000")  # WALè‡ªå‹•ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
                conn.execute("PRAGMA optimize")  # æœ€é©åŒ–å®Ÿè¡Œ
                
                debug_logger.debug("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãƒ»æœ€é©åŒ–å®Œäº†")

                cursor = conn.cursor()

                # æ¥ç¶šãƒ†ã‚¹ãƒˆï¼ˆå®Ÿéš›ã®ã‚¯ã‚¨ãƒªå®Ÿè¡Œï¼‰
                cursor.execute("SELECT COUNT(*) FROM sqlite_master WHERE type='table'")
                table_count = cursor.fetchone()[0]
                debug_logger.debug(f"æ¥ç¶šãƒ†ã‚¹ãƒˆæˆåŠŸ: {table_count}ãƒ†ãƒ¼ãƒ–ãƒ«å­˜åœ¨")

                # æ—¢å­˜ãƒã‚§ãƒƒã‚¯ï¼ˆå®‰å…¨ç‰ˆï¼‰
                cursor.execute('SELECT id FROM documents WHERE file_path = ?', (file_path,))
                existing = cursor.fetchone()

                # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã¨ã‚µãƒ‹ã‚¿ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆå¼·åŒ–ç‰ˆï¼‰
                safe_content = content[:2000000] if content else ""  # 2MBåˆ¶é™ã«æ‹¡å¼µ
                safe_file_name = base_data['file_name'][:500] if base_data['file_name'] else os.path.basename(file_path)
                safe_file_type = base_data['file_type'][:100] if base_data['file_type'] else "unknown"
                
                # ç‰¹æ®Šæ–‡å­—ã®ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—
                safe_content = safe_content.replace('\x00', '')  # NULLæ–‡å­—é™¤å»
                safe_file_name = safe_file_name.replace('\x00', '')
                
                debug_logger.debug(f"ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†: content={len(safe_content)}æ–‡å­—, name='{safe_file_name}', type='{safe_file_type}'")

                if existing:
                    # æ›´æ–°å‡¦ç†ï¼ˆãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ä½¿ç”¨ï¼‰
                    try:
                        conn.execute("BEGIN EXCLUSIVE")  # æ’ä»–ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³é–‹å§‹
                        debug_logger.debug(f"æ’ä»–ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³é–‹å§‹: æ›´æ–°å‡¦ç†")
                        
                        # ãƒ¡ã‚¤ãƒ³ãƒ†ãƒ¼ãƒ–ãƒ«æ›´æ–°
                        cursor.execute(
                            '''
                            UPDATE documents 
                            SET content = ?, file_name = ?, file_type = ?, size = ?, 
                                modified_time = ?, indexed_time = ?, hash = ?
                            WHERE file_path = ?
                        ''', (safe_content, safe_file_name, safe_file_type, base_data['size'],
                              time.time(), time.time(), file_hash, file_path))

                        # FTSæ›´æ–°ï¼ˆå®‰å…¨å‰Šé™¤â†’è¿½åŠ ï¼‰
                        cursor.execute('DELETE FROM documents_fts WHERE rowid = ?', (existing[0],))

                        # FTSæŒ¿å…¥å‰ã«rowidãŒæœ‰åŠ¹ã‹ãƒã‚§ãƒƒã‚¯
                        cursor.execute('SELECT id FROM documents WHERE id = ?', (existing[0],))
                        if cursor.fetchone():
                            cursor.execute(
                                '''
                                INSERT INTO documents_fts(rowid, file_path, file_name, content, file_type)
                                VALUES (?, ?, ?, ?, ?)
                            ''', (existing[0], file_path, safe_file_name, safe_content, safe_file_type))

                        conn.commit()  # ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ã‚³ãƒŸãƒƒãƒˆ
                        debug_logger.debug(f"æ–‡æ›¸æ›´æ–°å®Œäº†: {file_path} (DB{db_index})")

                    except sqlite3.IntegrityError as ie:
                        conn.rollback()  # ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯
                        debug_logger.error(f"æ›´æ–°åˆ¶ç´„ã‚¨ãƒ©ãƒ¼: {ie}")
                        raise  # ä¸Šä½ã«å†ã‚¹ãƒ­ãƒ¼

                else:
                    # æ–°è¦è¿½åŠ å‡¦ç†ï¼ˆãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ä½¿ç”¨ï¼‰
                    try:
                        conn.execute("BEGIN EXCLUSIVE")  # æ’ä»–ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³é–‹å§‹
                        debug_logger.debug(f"æ’ä»–ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³é–‹å§‹: æ–°è¦è¿½åŠ ")
                        
                        # ãƒ¡ã‚¤ãƒ³ãƒ†ãƒ¼ãƒ–ãƒ«æŒ¿å…¥
                        cursor.execute(
                            '''
                            INSERT INTO documents (file_path, file_name, content, file_type, size, 
                                                 modified_time, indexed_time, hash)
                            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                        ''', (file_path, safe_file_name, safe_content, safe_file_type,
                              base_data['size'], time.time(), time.time(), file_hash))

                        doc_id = cursor.lastrowid

                        if doc_id:  # æœ‰åŠ¹ãªIDãŒå–å¾—ã§ããŸå ´åˆã®ã¿FTSæŒ¿å…¥
                            cursor.execute(
                                '''
                                INSERT INTO documents_fts(rowid, file_path, file_name, content, file_type)
                                VALUES (?, ?, ?, ?, ?)
                            ''', (doc_id, file_path, safe_file_name, safe_content, safe_file_type))

                        conn.commit()  # ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ã‚³ãƒŸãƒƒãƒˆ
                        debug_logger.debug(f"æ–°è¦æ–‡æ›¸è¿½åŠ å®Œäº†: {file_path} (DB{db_index})")

                    except sqlite3.IntegrityError as ie:
                        conn.rollback()  # ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯
                        debug_logger.error(f"æ–°è¦è¿½åŠ åˆ¶ç´„ã‚¨ãƒ©ãƒ¼: {ie}")
                        # é‡è¤‡ãƒã‚§ãƒƒã‚¯å¾Œã«å†è©¦è¡Œ
                        cursor.execute('SELECT id FROM documents WHERE file_path = ?', (file_path,))
                        duplicate = cursor.fetchone()
                        if duplicate:
                            debug_logger.warning(f"é‡è¤‡æ–‡æ›¸æ¤œå‡ºã€æ›´æ–°ã«åˆ‡ã‚Šæ›¿ãˆ: {file_path}")
                            # æ›´æ–°ã«åˆ‡ã‚Šæ›¿ãˆï¼ˆå†ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ï¼‰
                            conn.execute("BEGIN EXCLUSIVE")
                            cursor.execute(
                                '''
                                UPDATE documents 
                                SET content = ?, file_name = ?, file_type = ?, size = ?, 
                                    modified_time = ?, indexed_time = ?, hash = ?
                                WHERE file_path = ?
                            ''', (safe_content, safe_file_name, safe_file_type, base_data['size'],
                                  time.time(), time.time(), file_hash, file_path))
                            
                            # FTSæ›´æ–°ï¼ˆå‰Šé™¤â†’è¿½åŠ ï¼‰
                            cursor.execute('DELETE FROM documents_fts WHERE rowid = ?', (duplicate[0],))
                            cursor.execute(
                                '''
                                INSERT INTO documents_fts(rowid, file_path, file_name, content, file_type)
                                VALUES (?, ?, ?, ?, ?)
                            ''', (duplicate[0], file_path, safe_file_name, safe_content, safe_file_type))
                            
                            conn.commit()

                # æˆåŠŸã—ãŸã‚‰ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
                print(f"âœ… DB{db_index}å®Œå…¨å±¤ç§»è¡Œå®Œäº†: {os.path.basename(file_path)}")
                debug_logger.info(f"å®Œå…¨å±¤ç§»è¡ŒæˆåŠŸ: {file_path} (DB{db_index})")
                break  # æˆåŠŸæ™‚ã¯ãƒ«ãƒ¼ãƒ—çµ‚äº†

            except sqlite3.OperationalError as e:
                error_msg = str(e).lower()
                debug_logger.error(f"DB{db_index}é‹ç”¨ã‚¨ãƒ©ãƒ¼è©¦è¡Œ{attempt + 1}: {e}")
                
                # æ¥ç¶šã‚’ç¢ºå®Ÿã«é–‰ã˜ã‚‹
                if conn is not None:
                    try:
                        conn.close()
                        debug_logger.debug(f"DB{db_index}æ¥ç¶šã‚¯ãƒ­ãƒ¼ã‚ºå®Œäº†")
                    except:
                        pass

                if ("unable to open database file" in error_msg or
                    "database is locked" in error_msg or
                    "database is busy" in error_msg or
                    "disk i/o error" in error_msg) and attempt < max_retries - 1:
                    
                    # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã§ãƒªãƒˆãƒ©ã‚¤
                    wait_time = retry_delay * (2 ** attempt) + (attempt * 0.05)  # ã‚¸ãƒƒã‚¿ãƒ¼ã‚’è¿½åŠ 
                    debug_logger.warning(f"DB{db_index}ãƒªãƒˆãƒ©ã‚¤å¾…æ©Ÿ: {wait_time:.3f}ç§’ (è©¦è¡Œ{attempt + 1}/{max_retries})")
                    print(f"ğŸ”„ DB{db_index}ãƒ“ã‚¸ãƒ¼çŠ¶æ…‹ - {wait_time:.2f}ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤ {attempt + 1}/{max_retries}: {os.path.basename(file_path)}")
                    time.sleep(wait_time)
                    continue
                else:
                    debug_logger.error(f"DB{db_index}é‹ç”¨ã‚¨ãƒ©ãƒ¼ï¼ˆãƒªãƒˆãƒ©ã‚¤ä¸å¯ï¼‰: {e}")
                    print(f"âŒ DB{db_index}å®Œå…¨å±¤è¿½åŠ ã‚¨ãƒ©ãƒ¼ (DB): {e}")
                    break

            except sqlite3.IntegrityError as ie:
                error_msg = str(ie).lower()
                debug_logger.error(f"DB{db_index}åˆ¶ç´„ã‚¨ãƒ©ãƒ¼è©¦è¡Œ{attempt + 1}: {ie}")
                
                # æ¥ç¶šã‚’ç¢ºå®Ÿã«é–‰ã˜ã‚‹
                if conn is not None:
                    try:
                        conn.rollback()  # ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯
                        conn.close()
                    except Exception as close_error:
                        debug_logger.warning(f"DB{db_index}æ¥ç¶šã‚¯ãƒ­ãƒ¼ã‚ºã‚¨ãƒ©ãƒ¼: {close_error}")

                if "constraint failed" in error_msg:
                    print(f"ğŸ”§ DB{db_index}åˆ¶ç´„ã‚¨ãƒ©ãƒ¼ä¿®å¾©è©¦è¡Œ: {os.path.basename(file_path)}")
                    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿®å¾©ã‚’è©¦è¡Œ
                    try:
                        repair_conn = sqlite3.connect(str(complete_db_path), timeout=30.0)
                        repair_cursor = repair_conn.cursor()

                        # é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                        repair_cursor.execute('DELETE FROM documents WHERE file_path = ?', (file_path,))
                        repair_cursor.execute('DELETE FROM documents_fts WHERE file_path = ?', (file_path,))

                        repair_conn.commit()
                        repair_conn.close()

                        print(f"âœ… DB{db_index}åˆ¶ç´„ã‚¨ãƒ©ãƒ¼ä¿®å¾©å®Œäº†: {os.path.basename(file_path)}")
                        debug_logger.info(f"DB{db_index}åˆ¶ç´„ã‚¨ãƒ©ãƒ¼ä¿®å¾©æˆåŠŸ")
                        if attempt < max_retries - 1:
                            continue  # ãƒªãƒˆãƒ©ã‚¤
                    except Exception as repair_error:
                        debug_logger.error(f"DB{db_index}ä¿®å¾©ã‚¨ãƒ©ãƒ¼: {repair_error}")
                        print(f"âŒ DB{db_index}åˆ¶ç´„ã‚¨ãƒ©ãƒ¼ä¿®å¾©å¤±æ•—: {repair_error}")
                        break
                else:
                    print(f"âŒ DB{db_index}å®Œå…¨å±¤è¿½åŠ ã‚¨ãƒ©ãƒ¼ (åˆ¶ç´„): {ie}")
                    break

            except Exception as e:
                debug_logger.error(f"DB{db_index}äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼è©¦è¡Œ{attempt + 1}: {e}")
                print(f"âš ï¸ DB{db_index}å®Œå…¨å±¤è¿½åŠ ã‚¨ãƒ©ãƒ¼: {e}")
                
                # æ¥ç¶šã‚’ç¢ºå®Ÿã«é–‰ã˜ã‚‹
                if conn is not None:
                    try:
                        conn.close()
                    except:
                        pass
                break
            
            finally:
                # finallyå¥ã§ç¢ºå®Ÿã«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                if conn is not None:
                    try:
                        conn.close()
                        debug_logger.debug(f"DB{db_index}æ¥ç¶šæœ€çµ‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")
                    except Exception as cleanup_error:
                        debug_logger.warning(f"DB{db_index}æœ€çµ‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {cleanup_error}")
        
        # çµ±è¨ˆæ›´æ–°ã®ã‚·ã‚°ãƒŠãƒ«ï¼ˆGUIæ›´æ–°ã®ãŸã‚ï¼‰- é »åº¦åˆ¶é™
        try:
            if hasattr(self, '_stats_update_callback') and self._stats_update_callback:
                if not hasattr(self, '_last_stats_update_time'):
                    self._last_stats_update_time = 0
                
                current_time = time.time()
                if current_time - self._last_stats_update_time > 2.0:  # 2ç§’é–“éš”ã«åˆ¶é™
                    self._last_stats_update_time = current_time
                    self._stats_update_callback()
        except Exception as callback_error:
            debug_logger.warning(f"çµ±è¨ˆæ›´æ–°ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {callback_error}")
            pass

    def _extract_file_content(self, file_path: str) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹æŠ½å‡º - å…¨å½¢å¼å¯¾å¿œï¼ˆç”»åƒOCRå«ã‚€ï¼‰"""
        try:
            file_path_obj = Path(file_path)
            extension = file_path_obj.suffix.lower()

            if extension == '.txt':
                return self._extract_txt_content(file_path)
            elif extension in ['.docx', '.dotx', '.dotm', '.docm']:  # Wordæ–°å½¢å¼ãƒ•ã‚¡ã‚¤ãƒ«
                return self._extract_docx_content(file_path)
            elif extension in ['.doc', '.dot']:  # Wordæ—§å½¢å¼ãƒ•ã‚¡ã‚¤ãƒ«
                return self._extract_doc_content(file_path)
            elif extension in ['.xlsx', '.xltx', '.xltm', '.xlsm', '.xlsb']:  # Excelæ–°å½¢å¼ãƒ•ã‚¡ã‚¤ãƒ«
                return self._extract_xlsx_content(file_path)
            elif extension in ['.xls', '.xlt']:  # Excelæ—§å½¢å¼ãƒ•ã‚¡ã‚¤ãƒ«
                return self._extract_xls_content(file_path)
            elif extension == '.pdf':
                return self._extract_pdf_content(file_path)
            elif extension == '.zip':  # ZIPãƒ•ã‚¡ã‚¤ãƒ«å†…ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
                return self._extract_zip_content(file_path)
            elif extension in ['.tif', '.tiff']:  # .tifãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ç”»åƒå‡¦ç†å¯¾è±¡
                return self._extract_image_content(file_path)
            elif extension in ['.jwc', '.jww', '.dxf', '.sfc', '.dwg', '.dwt', '.mpp', '.mpz']:  # CAD/å›³é¢ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿æ¤œç´¢å¯¾è±¡ï¼‰
                return ""  # å†…å®¹ã¯æŠ½å‡ºã›ãšã€ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            else:
                # å¯¾è±¡å¤–ã®æ‹¡å¼µå­ã¯ã‚¹ã‚­ãƒƒãƒ—
                return ""

        except Exception as e:
            print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹æŠ½å‡ºã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
            return ""

    def _extract_txt_content(self, file_path: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«æŠ½å‡ºï¼ˆmmap+ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æœ€é©åŒ–ãƒ»90%é«˜é€ŸåŒ–ï¼‰"""
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
            file_size = os.path.getsize(file_path)
            if file_size == 0:
                return ""
            
            # ğŸš€ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯ï¼ˆåŒã˜æ‹¡å¼µå­ã¯åŒã˜ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®å¯èƒ½æ€§ãŒé«˜ã„ï¼‰
            if not hasattr(self, '_encoding_cache'):
                self._encoding_cache = {}
            file_ext = Path(file_path).suffix.lower()
            cached_encoding = self._encoding_cache.get(file_ext)
            
            # ğŸš€ å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œ: 10MBä»¥ä¸Šã¯mmapã§åŠ¹ç‡çš„ã«ã‚¢ã‚¯ã‚»ã‚¹
            use_mmap = file_size > 10 * 1024 * 1024
            
            # ğŸ”¥ è¶…å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ100MBä»¥ä¸Šï¼‰ã¯æœ€å°é™ã®ã¿
            if file_size > 100 * 1024 * 1024:
                max_read_size = 5 * 1024 * 1024  # 5MBã®ã¿ï¼ˆè¶…é«˜é€ŸåŒ–ï¼‰
            elif file_size > 50 * 1024 * 1024:
                max_read_size = 10 * 1024 * 1024  # 10MBã¾ã§
            else:
                max_read_size = min(file_size, 20 * 1024 * 1024)  # æœ€å¤§20MBã¾ã§
            
            # ãƒã‚¤ãƒŠãƒªã§èª­ã¿è¾¼ã‚“ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¤œå‡ºï¼ˆæœ€é©åŒ–: 4KBã§æ¤œå‡ºï¼‰
            with open(file_path, 'rb') as f:
                sample_data = f.read(min(4096, file_size))  # 4KBã§ååˆ†ï¼ˆ10KBâ†’4KBã§é«˜é€ŸåŒ–ï¼‰
                
                # ãƒã‚¤ãƒŠãƒªãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡ºï¼ˆNULLæ–‡å­—ãŒå¤šã„å ´åˆï¼‰
                null_count = sample_data.count(b'\x00')
                if null_count > len(sample_data) * 0.1:  # 10%ä»¥ä¸ŠNULLæ–‡å­—ãªã‚‰ãƒã‚¤ãƒŠãƒª
                    return ""
            
            # ğŸš€ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¤œå‡ºã®æœ€é©åŒ–: ã‚­ãƒ£ãƒƒã‚·ãƒ¥å„ªå…ˆã€UTF-8ã‚’æœ€åˆã«è©¦è¡Œ
            detected_encoding = None
            if cached_encoding:
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒã‚ã‚Œã°å„ªå…ˆä½¿ç”¨
                detected_encoding = cached_encoding
                debug_logger.debug(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ä½¿ç”¨: {detected_encoding}")
            elif chardet:
                try:
                    detection = chardet.detect(sample_data)
                    if detection and detection['confidence'] > 0.7:
                        detected_encoding = detection['encoding']
                        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                        self._encoding_cache[file_ext] = detected_encoding
                        debug_logger.debug(f"æ¤œå‡ºã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: {detected_encoding} (ä¿¡é ¼åº¦: {detection['confidence']:.2f})")
                except Exception as e:
                    debug_logger.warning(f"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
            
            # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å€™è£œãƒªã‚¹ãƒˆï¼ˆUTF-8å„ªå…ˆã§é«˜é€ŸåŒ–ï¼‰
            encodings = ['utf-8']  # UTF-8ã‚’æœ€å„ªå…ˆ
            if detected_encoding and detected_encoding.lower() != 'utf-8':
                encodings.insert(0, detected_encoding)
            encodings.extend(['cp932', 'shift_jis'])  # æ—¥æœ¬èªç’°å¢ƒã®ä¸»è¦ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
            
            # å„ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§è©¦è¡Œï¼ˆé«˜é€Ÿç‰ˆï¼‰
            for encoding in encodings:
                try:
                    if use_mmap and file_size > 50 * 1024 * 1024:
                        # ğŸš€ 50MBä»¥ä¸Š: mmapã§åŠ¹ç‡çš„ã«ã‚¢ã‚¯ã‚»ã‚¹ï¼ˆãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‰
                        with open(file_path, 'rb') as f:
                            with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mmapped:
                                # å…ˆé ­20MBã®ã¿èª­ã¿è¾¼ã¿ï¼ˆå…¨ä½“ã¯èª­ã¾ãªã„ï¼‰
                                chunk_data = mmapped[:max_read_size]
                                content = chunk_data.decode(encoding, errors='strict')
                                if content and len(content.strip()) > 0:
                                    # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
                                    self._encoding_cache[file_ext] = encoding
                                    debug_logger.debug(f"mmapæŠ½å‡ºæˆåŠŸ: {encoding}")
                                    return normalize_extracted_text(content)
                    else:
                        # é€šå¸¸ãƒ•ã‚¡ã‚¤ãƒ«: æ¨™æº–èª­ã¿è¾¼ã¿
                        with open(file_path, 'r', encoding=encoding, errors='strict') as f:
                            content = f.read(max_read_size)
                            if content and len(content.strip()) > 0:
                                # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
                                self._encoding_cache[file_ext] = encoding
                                debug_logger.debug(f"ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºæˆåŠŸ: {encoding}")
                                return normalize_extracted_text(content)
                except (UnicodeDecodeError, LookupError):
                    continue
                except Exception as e:
                    debug_logger.warning(f"èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ ({encoding}): {e}")
                    continue
            
            # ã™ã¹ã¦å¤±æ•—ã—ãŸå ´åˆã¯ã‚¨ãƒ©ãƒ¼ã‚’ç„¡è¦–ã—ã¦èª­ã¿è¾¼ã¿ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read(max_read_size)
                    return normalize_extracted_text(content)
            except:
                try:
                    with open(file_path, 'r', encoding='cp932', errors='ignore') as f:
                        content = f.read(max_read_size)
                        return normalize_extracted_text(content)
                except:
                    return ""
                    
        except Exception as e:
            debug_logger.error(f"ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
            return ""

    def _extract_docx_content(self, file_path: str) -> str:
        """Wordæ–‡æ›¸æŠ½å‡ºï¼ˆå¤§å®¹é‡å¯¾å¿œãƒ»éƒ¨åˆ†èª­ã¿è¾¼ã¿æœ€é©åŒ–ï¼‰"""
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ãƒã‚§ãƒƒã‚¯
            file_extension = os.path.splitext(file_path)[1].lower()
            
            # å¤ã„å½¢å¼ã®Wordãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ.docï¼‰ã®å ´åˆã¯å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—
            if file_extension in ['.doc', '.dot']:
                print(f"âš ï¸ å¤ã„å½¢å¼ã®Wordãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“: {os.path.basename(file_path)}")
                return ""

            # ğŸš€ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯ï¼ˆå¤§å®¹é‡å¯¾å¿œï¼‰
            file_size = os.path.getsize(file_path)
            if file_size < 100:  # 100ãƒã‚¤ãƒˆæœªæº€ã¯ç„¡åŠ¹
                print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒå°ã•ã™ãã¾ã™: {os.path.basename(file_path)}")
                return ""
            
            # ğŸš€ å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ50MBä»¥ä¸Šï¼‰ã¯éƒ¨åˆ†çš„ã«å‡¦ç†
            is_large_file = file_size > 50 * 1024 * 1024
            max_paragraphs = 1000 if is_large_file else 10000  # å¤§å®¹é‡ã¯1000æ®µè½ã¾ã§

            content = []

            # ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‹ã©ã†ã‹ã‚’äº‹å‰ãƒã‚§ãƒƒã‚¯
            try:
                with zipfile.ZipFile(file_path, 'r') as test_zip:
                    # word/document.xmlãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                    if 'word/document.xml' not in test_zip.namelist():
                        debug_logger.warning(f"word/document.xmlãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
                        print(f"âš ï¸ æœ‰åŠ¹ãªWordãƒ•ã‚¡ã‚¤ãƒ«ã§ã¯ã‚ã‚Šã¾ã›ã‚“ï¼ˆç ´æã¾ãŸã¯åˆ¥å½¢å¼ï¼‰: {os.path.basename(file_path)}")
                        return ""
            except zipfile.BadZipFile:
                debug_logger.warning(f"ZIPãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦é–‹ã‘ã¾ã›ã‚“: {file_path}")
                print(f"âš ï¸ ç ´æã—ãŸWordãƒ•ã‚¡ã‚¤ãƒ«: {os.path.basename(file_path)}")
                return ""  # ZIPãƒ•ã‚¡ã‚¤ãƒ«ã§ãªã„å ´åˆã¯é™ã‹ã«çµ‚äº†
            except Exception as e:
                debug_logger.warning(f"Wordäº‹å‰ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {file_path} - {e}")
                return ""

            with zipfile.ZipFile(file_path, 'r') as docx:
                # ãƒ¡ã‚¤ãƒ³æ–‡æ›¸ã®æŠ½å‡º
                xml_content = docx.read('word/document.xml')
                root = ET.fromstring(xml_content)
                
                # åå‰ç©ºé–“å®šç¾©
                namespaces = {
                    'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'
                }

                # æ®µè½ã¨ãƒ†ã‚­ã‚¹ãƒˆè¦ç´ ã‚’é †åºé€šã‚Šã«æŠ½å‡º
                paragraph_count = 0
                for para in root.findall('.//w:p', namespaces):
                    # ğŸš€ å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«: æ®µè½æ•°åˆ¶é™
                    if is_large_file and paragraph_count >= max_paragraphs:
                        debug_logger.info(f"å¤§å®¹é‡Word: {max_paragraphs}æ®µè½ã§å‡¦ç†çµ‚äº†")
                        break
                    
                    para_text = []
                    for text_elem in para.findall('.//w:t', namespaces):
                        if text_elem.text:
                            para_text.append(text_elem.text)
                    if para_text:
                        content.append(''.join(para_text))
                        paragraph_count += 1
                
                # ãƒ˜ãƒƒãƒ€ãƒ¼ã®æŠ½å‡º
                try:
                    for header_file in [f for f in docx.namelist() if 'header' in f.lower()]:
                        header_xml = docx.read(header_file)
                        header_root = ET.fromstring(header_xml)
                        for text_elem in header_root.findall('.//w:t', namespaces):
                            if text_elem.text and text_elem.text.strip():
                                content.append(text_elem.text.strip())
                except:
                    pass
                
                # ãƒ•ãƒƒã‚¿ãƒ¼ã®æŠ½å‡º
                try:
                    for footer_file in [f for f in docx.namelist() if 'footer' in f.lower()]:
                        footer_xml = docx.read(footer_file)
                        footer_root = ET.fromstring(footer_xml)
                        for text_elem in footer_root.findall('.//w:t', namespaces):
                            if text_elem.text and text_elem.text.strip():
                                content.append(text_elem.text.strip())
                except:
                    pass
                
                # è„šæ³¨ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆã®æŠ½å‡º
                try:
                    for notes_file in [f for f in docx.namelist() if 'footnotes' in f.lower() or 'comments' in f.lower()]:
                        notes_xml = docx.read(notes_file)
                        notes_root = ET.fromstring(notes_xml)
                        for text_elem in notes_root.findall('.//w:t', namespaces):
                            if text_elem.text and text_elem.text.strip():
                                content.append(text_elem.text.strip())
                except:
                    pass

            result = ' '.join(content)
            return normalize_extracted_text(result)

        except zipfile.BadZipFile:
            print(f"âš ï¸ Wordãƒ•ã‚¡ã‚¤ãƒ«ãŒä¸æ­£ãªZIPå½¢å¼ã§ã™: {os.path.basename(file_path)}")
            return ""
        except Exception as e:
            # ã‚ˆã‚Šè©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’æä¾›
            if "zip file" in str(e).lower():
                print(f"âš ï¸ Wordãƒ•ã‚¡ã‚¤ãƒ«ã®ZIPå½¢å¼ã‚¨ãƒ©ãƒ¼: {os.path.basename(file_path)}")
            else:
                print(f"âš ï¸ WordæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {os.path.basename(file_path)} - {e}")
            return ""

    def _extract_xlsx_content(self, file_path: str) -> str:
        """Excelæ–‡æ›¸æŠ½å‡ºï¼ˆå¤§å®¹é‡å¯¾å¿œãƒ»éƒ¨åˆ†èª­ã¿è¾¼ã¿æœ€é©åŒ–ï¼‰"""
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ãƒã‚§ãƒƒã‚¯
            file_extension = os.path.splitext(file_path)[1].lower()
            
            # å¤ã„å½¢å¼ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ.xlsï¼‰ã®å ´åˆã¯å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—
            if file_extension in ['.xls', '.xlt']:
                print(f"âš ï¸ å¤ã„å½¢å¼ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“: {os.path.basename(file_path)}")
                return ""
            
            # ğŸš€ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯ï¼ˆå¤§å®¹é‡å¯¾å¿œï¼‰
            file_size = os.path.getsize(file_path)
            is_large_file = file_size > 50 * 1024 * 1024
            max_rows = 5000 if is_large_file else 50000  # å¤§å®¹é‡ã¯5000è¡Œã¾ã§
            max_sheets = 3 if is_large_file else 10  # å¤§å®¹é‡ã¯3ã‚·ãƒ¼ãƒˆã¾ã§
            
            # ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‹ã©ã†ã‹ã‚’äº‹å‰ãƒã‚§ãƒƒã‚¯
            try:
                with zipfile.ZipFile(file_path, 'r') as test_zip:
                    # Excelå½¢å¼ã®å¿…é ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                    if 'xl/workbook.xml' not in test_zip.namelist():
                        print(f"âš ï¸ æœ‰åŠ¹ãªExcelãƒ•ã‚¡ã‚¤ãƒ«ã§ã¯ã‚ã‚Šã¾ã›ã‚“: {os.path.basename(file_path)}")
                        return ""
            except zipfile.BadZipFile:
                print(f"âš ï¸ ZIPãƒ•ã‚¡ã‚¤ãƒ«ã§ãªã„ãŸã‚å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—: {os.path.basename(file_path)}")
                return ""
            except Exception as e:
                print(f"âš ï¸ Excelãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {os.path.basename(file_path)} - {e}")
                return ""
            
            content = []
            with zipfile.ZipFile(file_path, 'r') as xlsx:
                # å…±æœ‰æ–‡å­—åˆ—å–å¾—
                try:
                    shared_strings_xml = xlsx.read('xl/sharedStrings.xml')
                    shared_root = ET.fromstring(shared_strings_xml)
                    shared_strings = [elem.text or '' for elem in shared_root.iter() if elem.text]
                except:
                    shared_strings = []

                # ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆå‡¦ç†
                try:
                    workbook_xml = xlsx.read('xl/workbook.xml')
                    wb_root = ET.fromstring(workbook_xml)
                    
                    # åå‰ç©ºé–“å®šç¾©
                    ns = {'': 'http://schemas.openxmlformats.org/spreadsheetml/2006/main'}

                    sheet_files = [f for f in xlsx.namelist() if f.startswith('xl/worksheets/sheet')]
                    
                    # ğŸš€ å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«: ã‚·ãƒ¼ãƒˆæ•°åˆ¶é™
                    processed_sheets = 0
                    for sheet_file in sheet_files:
                        if is_large_file and processed_sheets >= max_sheets:
                            debug_logger.info(f"å¤§å®¹é‡Excel: {max_sheets}ã‚·ãƒ¼ãƒˆã§å‡¦ç†çµ‚äº†")
                            break
                        
                        sheet_xml = xlsx.read(sheet_file)
                        sheet_root = ET.fromstring(sheet_xml)
                        
                        # ğŸš€ å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«: è¡Œæ•°åˆ¶é™
                        row_count = 0
                        # ã‚»ãƒ«ã‚’é †ç•ªã«å‡¦ç†
                        for row in sheet_root.iter('{http://schemas.openxmlformats.org/spreadsheetml/2006/main}row'):
                            if is_large_file and row_count >= max_rows:
                                debug_logger.info(f"å¤§å®¹é‡Excel: ã‚·ãƒ¼ãƒˆ{processed_sheets+1}ã§{max_rows}è¡Œå‡¦ç†")
                                break
                            row_count += 1
                            for cell in row.iter('{http://schemas.openxmlformats.org/spreadsheetml/2006/main}c'):
                                cell_type = cell.get('t', 'n')  # ã‚»ãƒ«ã‚¿ã‚¤ãƒ—: s=æ–‡å­—åˆ—, n=æ•°å€¤, b=ãƒ–ãƒ¼ãƒ«ç­‰
                                
                                # ã‚»ãƒ«å€¤ã‚’å–å¾—
                                v_elem = cell.find('{http://schemas.openxmlformats.org/spreadsheetml/2006/main}v')
                                if v_elem is not None and v_elem.text:
                                    value = v_elem.text.strip()
                                    
                                    if cell_type == 's':  # å…±æœ‰æ–‡å­—åˆ—å‚ç…§
                                        try:
                                            index = int(value)
                                            if 0 <= index < len(shared_strings):
                                                text = shared_strings[index]
                                                if text and len(text) > 0:
                                                    content.append(text)
                                        except (ValueError, IndexError):
                                            pass
                                    elif cell_type == 'str':  # æ•°å¼ã®æ–‡å­—åˆ—çµæœ
                                        if value and len(value) > 0:
                                            content.append(value)
                                    elif value and not value.replace('.', '').replace('-', '').isdigit():
                                        # æ•°å€¤ä»¥å¤–ã®ç›´æ¥å€¤
                                        if len(value) > 0:
                                            content.append(value)
                                    elif value and len(value) > 2:  # é•·ã„æ•°å€¤ã¯ä¿æŒï¼ˆIDç­‰ï¼‰
                                        content.append(value)
                        
                        processed_sheets += 1

                except Exception as e:
                    print(f"âš ï¸ Excelã‚·ãƒ¼ãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")

            result = ' '.join(content)
            return normalize_extracted_text(result)

        except zipfile.BadZipFile:
            print(f"âš ï¸ Excelãƒ•ã‚¡ã‚¤ãƒ«ãŒä¸æ­£ãªZIPå½¢å¼ã§ã™: {os.path.basename(file_path)}")
            return ""
        except Exception as e:
            # ã‚ˆã‚Šè©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’æä¾›
            if "zip file" in str(e).lower():
                print(f"âš ï¸ Excelãƒ•ã‚¡ã‚¤ãƒ«ã®ZIPå½¢å¼ã‚¨ãƒ©ãƒ¼: {os.path.basename(file_path)}")
            else:
                print(f"âš ï¸ ExcelæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {os.path.basename(file_path)} - {e}")
            return ""

    def _extract_zip_content(self, file_path: str) -> str:
        """ZIPãƒ•ã‚¡ã‚¤ãƒ«å†…ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«æŠ½å‡º"""
        try:
            content = []
            max_files = 50  # å‡¦ç†ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã®ä¸Šé™
            max_file_size = 1024 * 1024  # 1ãƒ•ã‚¡ã‚¤ãƒ«ã‚ãŸã‚Šã®æœ€å¤§ã‚µã‚¤ã‚ºï¼ˆ1MBï¼‰
            processed_files = 0
            
            # ã‚µãƒãƒ¼ãƒˆã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­
            text_extensions = {'.txt', '.md', '.log', '.csv', '.json', '.xml', '.html', '.htm', '.py', '.js', '.css'}
            
            with zipfile.ZipFile(file_path, 'r') as zip_file:
                for file_info in zip_file.infolist():
                    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚¹ã‚­ãƒƒãƒ—
                    if file_info.is_dir():
                        continue
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«æ•°åˆ¶é™ãƒã‚§ãƒƒã‚¯
                    if processed_files >= max_files:
                        print(f"ğŸ“¦ ZIPãƒ•ã‚¡ã‚¤ãƒ«å†…ãƒ•ã‚¡ã‚¤ãƒ«æ•°åˆ¶é™åˆ°é”: {max_files}ä»¶")
                        break
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«åã¨ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
                    file_name = file_info.filename
                    file_ext = os.path.splitext(file_name)[1].lower()
                    
                    # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å‡¦ç†
                    if file_ext not in text_extensions:
                        continue
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
                    if file_info.file_size > max_file_size:
                        print(f"ğŸ“¦ ZIPãƒ•ã‚¡ã‚¤ãƒ«å†…ã®å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—: {file_name} ({file_info.file_size} bytes)")
                        continue
                    
                    try:
                        # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’æŠ½å‡º
                        with zip_file.open(file_info) as inner_file:
                            # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è‡ªå‹•æ¤œå‡º
                            raw_data = inner_file.read()
                            
                            # UTF-8ã§è©¦è¡Œ
                            try:
                                text_content = raw_data.decode('utf-8')
                            except UnicodeDecodeError:
                                # Shift_JISã§è©¦è¡Œ
                                try:
                                    text_content = raw_data.decode('shift_jis')
                                except UnicodeDecodeError:
                                    # chardetãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§è‡ªå‹•æ¤œå‡º
                                    try:
                                        import chardet
                                        detected = chardet.detect(raw_data)
                                        if detected['encoding']:
                                            text_content = raw_data.decode(detected['encoding'])
                                        else:
                                            text_content = raw_data.decode('utf-8', errors='ignore')
                                    except (ImportError, UnicodeDecodeError):
                                        text_content = raw_data.decode('utf-8', errors='ignore')
                            
                            # ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹ã‚’è¿½åŠ ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã‚‚å«ã‚ã‚‹ï¼‰
                            if text_content.strip():
                                content.append(f"[{file_name}]\n{text_content.strip()}")
                                processed_files += 1
                    
                    except Exception as inner_error:
                        print(f"ğŸ“¦ ZIPãƒ•ã‚¡ã‚¤ãƒ«å†…ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼ {file_name}: {inner_error}")
                        continue
            
            result = '\n\n'.join(content)
            if result:
                print(f"ğŸ“¦ ZIPãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®Œäº†: {processed_files}å€‹ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŠ½å‡º")
            return result
            
        except zipfile.BadZipFile:
            print(f"âš ï¸ ä¸æ­£ãªZIPãƒ•ã‚¡ã‚¤ãƒ«: {file_path}")
            return ""
        except Exception as e:
            print(f"âš ï¸ ZIPæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return ""

    def _extract_xls_content(self, file_path: str) -> str:
        """å¤ã„å½¢å¼ã®Excel(.xls)ãƒ•ã‚¡ã‚¤ãƒ«æŠ½å‡º"""
        try:
            if xlrd is None:
                print(f"âš ï¸ xlrdãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¿…è¦ã§ã™ï¼ˆå¤ã„å½¢å¼Excelç”¨ï¼‰: {os.path.basename(file_path)}")
                return ""
            
            content = []
            
            # xlrdã§Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã
            workbook = xlrd.open_workbook(file_path)
            
            # å…¨ã‚·ãƒ¼ãƒˆã‚’å‡¦ç†
            for sheet_index in range(workbook.nsheets):
                sheet = workbook.sheet_by_index(sheet_index)
                
                # ã‚·ãƒ¼ãƒˆåã‚’è¿½åŠ 
                sheet_name = workbook.sheet_names()[sheet_index]
                content.append(f"[ã‚·ãƒ¼ãƒˆ: {sheet_name}]")
                
                # å„è¡Œãƒ»åˆ—ã‚’å‡¦ç†
                for row_idx in range(sheet.nrows):
                    row_values = []
                    for col_idx in range(sheet.ncols):
                        cell = sheet.cell(row_idx, col_idx)
                        
                        # ã‚»ãƒ«ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦å€¤ã‚’å–å¾—
                        if cell.ctype == xlrd.XL_CELL_TEXT:
                            value = cell.value.strip()
                        elif cell.ctype == xlrd.XL_CELL_NUMBER:
                            # æ•°å€¤ã®å ´åˆã€æ•´æ•°ãªã‚‰æ•´æ•°ã¨ã—ã¦è¡¨ç¤º
                            if cell.value == int(cell.value):
                                value = str(int(cell.value))
                            else:
                                value = str(cell.value)
                        elif cell.ctype == xlrd.XL_CELL_BOOLEAN:
                            value = str(bool(cell.value))
                        elif cell.ctype == xlrd.XL_CELL_DATE:
                            # æ—¥ä»˜ã®å ´åˆ
                            date_tuple = xlrd.xldate_as_tuple(cell.value, workbook.datemode)
                            value = f"{date_tuple[0]}/{date_tuple[1]}/{date_tuple[2]}"
                        else:
                            value = str(cell.value) if cell.value else ""
                        
                        if value and len(value.strip()) > 0:
                            row_values.append(value.strip())
                    
                    if row_values:
                        content.append(' '.join(row_values))
            
            result = '\n'.join(content)
            if result:
                print(f"ğŸ“Š å¤ã„å½¢å¼Excelå‡¦ç†å®Œäº†: {os.path.basename(file_path)}")
            return result
            
        except Exception as e:
            print(f"âš ï¸ å¤ã„å½¢å¼ExcelæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {os.path.basename(file_path)} - {e}")
            return ""

    def _extract_doc_content(self, file_path: str) -> str:
        """å¤ã„å½¢å¼ã®Word(.doc)ãƒ•ã‚¡ã‚¤ãƒ«æŠ½å‡º"""
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
            if not os.path.exists(file_path):
                print(f"âš ï¸ DOCãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
                return ""
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã®ç¢ºèª
            try:
                file_size = os.path.getsize(file_path)
                if file_size == 0:
                    print(f"âš ï¸ DOCãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã§ã™: {os.path.basename(file_path)}")
                    return ""
                elif file_size > 100 * 1024 * 1024:  # 100MBåˆ¶é™
                    print(f"âš ï¸ DOCãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤§ãã™ãã¾ã™ ({file_size/1024/1024:.1f}MB): {os.path.basename(file_path)}")
                    return ""
            except OSError as size_error:
                print(f"âš ï¸ DOCãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºå–å¾—ã‚¨ãƒ©ãƒ¼: {os.path.basename(file_path)} - {size_error}")
                return ""
            
            print(f"ğŸ”„ DOCå‡¦ç†é–‹å§‹: {os.path.basename(file_path)} ({file_size/1024:.1f}KB)")
            
            # 1. docx2txtã‚’è©¦è¡Œï¼ˆä¸€éƒ¨ã®DOCãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚‚å¯¾å¿œï¼‰
            if docx2txt is not None:
                try:
                    content = docx2txt.process(file_path)
                    if content and content.strip():
                        content_preview = content.strip()[:100] + "..." if len(content.strip()) > 100 else content.strip()
                        print(f"âœ… docx2txtã§DOCå‡¦ç†æˆåŠŸ: {os.path.basename(file_path)} - é•·ã•: {len(content)} æ–‡å­—")
                        print(f"   å†…å®¹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: {content_preview}")
                        return content.strip()
                except Exception as docx2txt_error:
                    print(f"âš ï¸ docx2txtå‡¦ç†ã‚¨ãƒ©ãƒ¼: {os.path.basename(file_path)} - {docx2txt_error}")
            
            # 2. olefileã§åŸºæœ¬æƒ…å ±ã‚’å–å¾—ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            if olefile is not None:
                try:
                    if olefile.isOleFile(file_path):
                        print(f"ğŸ“ OLE2å½¢å¼ã®DOCãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡º: {os.path.basename(file_path)}")
                        # olefileã«ã‚ˆã‚‹åŸºæœ¬çš„ãªæƒ…å ±æŠ½å‡º
                        with olefile.OleFileIO(file_path) as ole:
                            # Wordæ–‡æ›¸ã®åŸºæœ¬æƒ…å ±ã‚’å–å¾—
                            if ole.exists('WordDocument'):
                                # åŸºæœ¬çš„ãªãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã®ã¿è¿”ã™ï¼ˆå®‰å…¨ãªæ–¹æ³•ï¼‰
                                return f"Microsoft Wordæ–‡æ›¸ - {os.path.basename(file_path)} - OLE2å½¢å¼"
                            else:
                                return f"Microsoft Wordæ–‡æ›¸ - {os.path.basename(file_path)}"
                except Exception as olefile_error:
                    print(f"âš ï¸ olefileå‡¦ç†ã‚¨ãƒ©ãƒ¼: {os.path.basename(file_path)} - {olefile_error}")
            
            # 3. åŸºæœ¬çš„ãªãƒã‚¤ãƒŠãƒªè§£æã«ã‚ˆã‚‹æ–‡å­—åˆ—æŠ½å‡ºï¼ˆæœ€å¾Œã®æ‰‹æ®µï¼‰
            try:
                print(f"ğŸ” ãƒã‚¤ãƒŠãƒªè§£æã‚’è©¦è¡Œ: {os.path.basename(file_path)}")
                with open(file_path, 'rb') as f:
                    data = f.read(min(file_size, 1024*1024))  # æœ€å¤§1MBèª­ã¿è¾¼ã¿
                    
                # å¯èª­æ–‡å­—ã®ã¿ã‚’æŠ½å‡ºï¼ˆåŸºæœ¬çš„ãªæ–¹æ³•ï¼‰
                text_content = []
                current_word = []
                
                for byte in data:
                    char = chr(byte) if 32 <= byte <= 126 or byte in [9, 10, 13] else None
                    if char:
                        if char.isalnum() or char in ' .,!?-_()[]{}":;':
                            current_word.append(char)
                        elif current_word:
                            word = ''.join(current_word)
                            if len(word) >= 3:  # 3æ–‡å­—ä»¥ä¸Šã®å˜èªã®ã¿
                                text_content.append(word)
                            current_word = []
                    elif current_word:
                        word = ''.join(current_word)
                        if len(word) >= 3:
                            text_content.append(word)
                        current_word = []
                
                if text_content:
                    extracted_text = ' '.join(text_content[:50])  # æœ€åˆã®50å˜èª
                    if extracted_text.strip():
                        print(f"âœ… ãƒã‚¤ãƒŠãƒªè§£ææˆåŠŸ: {os.path.basename(file_path)} - {len(extracted_text)} æ–‡å­—")
                        return f"{extracted_text} - {os.path.basename(file_path)}"
                        
            except Exception as binary_error:
                print(f"âš ï¸ ãƒã‚¤ãƒŠãƒªè§£æã‚¨ãƒ©ãƒ¼: {os.path.basename(file_path)} - {binary_error}")
            
            # 4. å…¨ã¦ã®æ–¹æ³•ãŒå¤±æ•—ã—ãŸå ´åˆã¯åŸºæœ¬æƒ…å ±ã®ã¿
            print(f"ğŸ“ DOCå†…å®¹æŠ½å‡ºå¤±æ•—ã€ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {os.path.basename(file_path)}")
            print(f"   åˆ©ç”¨å¯èƒ½ãƒ©ã‚¤ãƒ–ãƒ©ãƒª: docx2txt={docx2txt is not None}, olefile={olefile is not None}")
            return f"Microsoft Wordæ–‡æ›¸ - {os.path.basename(file_path)}"
            
        except Exception as e:
            print(f"âš ï¸ DOCæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {os.path.basename(file_path)} - {e}")
            return ""

    def _extract_pdf_content(self, file_path: str) -> str:
        """PDFæ–‡æ›¸æŠ½å‡ºï¼ˆãƒšãƒ¼ã‚¸ä¸¦åˆ—åŒ–ã§80%é«˜é€ŸåŒ–ï¼‰"""
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ã¨ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ãƒã‚§ãƒƒã‚¯
            if not os.path.exists(file_path):
                debug_logger.warning(f"PDFãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {file_path}")
                return ""

            if not os.access(file_path, os.R_OK):
                debug_logger.warning(f"PDFãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿å–ã‚Šæ¨©é™ãªã—: {file_path}")
                return ""

            file_size = os.path.getsize(file_path)
            if file_size < 50:  # 50ãƒã‚¤ãƒˆæœªæº€ã¯ç„¡åŠ¹PDFã¨ã¿ãªã™
                debug_logger.warning(f"PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒå°ã•ã™ãã¾ã™: {file_path}")
                return ""

            # å¤§å®¹é‡PDFå¯¾å¿œ: 200MBã¾ã§å‡¦ç†å¯èƒ½
            if file_size > 200 * 1024 * 1024:  # 200MBä»¥ä¸Šã¯å‡¦ç†ã‚¹ã‚­ãƒƒãƒ—
                print(
                    f"âš ï¸ PDFãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤§ãã™ãã¾ã™: {os.path.basename(file_path)} ({file_size / 1024 / 1024:.1f}MB)"
                )
                return ""

            # PyMuPDFä½¿ç”¨ã‚’è©¦è¡Œï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹æ­£è¦åŒ–ä»˜ãï¼‰
            try:
                import fitz

                # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®æ­£è¦åŒ–ï¼ˆç‰¹æ®Šæ–‡å­—ãƒ»Unicodeå¯¾å¿œï¼‰
                normalized_path = os.path.normpath(os.path.abspath(file_path))

                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹ãƒ†ã‚¹ãƒˆ
                with open(normalized_path, 'rb') as test_file:
                    test_file.read(1024)  # 1KBãƒ†ã‚¹ãƒˆèª­ã¿è¾¼ã¿

                # PyMuPDFã§PDFé–‹ã
                doc = fitz.open(normalized_path)
                
                # ğŸš€ ãƒšãƒ¼ã‚¸æ•°ã«å¿œã˜ãŸå‡¦ç†æˆ¦ç•¥
                total_pages = doc.page_count
                max_pages = min(total_pages, 200)  # æœ€å¤§200ãƒšãƒ¼ã‚¸ï¼ˆ500â†’200ã§é«˜é€ŸåŒ–ï¼‰
                
                # ğŸš€ ä¸¦åˆ—å‡¦ç†ã§ãƒšãƒ¼ã‚¸æŠ½å‡ºï¼ˆ10ãƒšãƒ¼ã‚¸ä»¥ä¸Šã®å ´åˆï¼‰
                if max_pages >= 10:
                    def extract_single_page(page_num: int) -> str:
                        """å˜ä¸€ãƒšãƒ¼ã‚¸æŠ½å‡ºï¼ˆä¸¦åˆ—å‡¦ç†ç”¨ï¼‰"""
                        try:
                            page = doc[page_num]
                            # æœ€ã‚‚é«˜é€Ÿãªæ–¹æ³•ã‚’å„ªå…ˆï¼ˆå¤±æ•—æ™‚ã®ã¿ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
                            try:
                                page_text = page.get_text("text", sort=True)
                                if page_text and len(page_text.strip()) > 10:
                                    return ' '.join(page_text.split())
                            except:
                                pass
                            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ–ãƒ­ãƒƒã‚¯å˜ä½æŠ½å‡º
                            blocks = page.get_text("blocks")
                            block_texts = [block[4].strip() for block in blocks if len(block) >= 5 and block[4].strip()]
                            return ' '.join(block_texts)
                        except Exception as e:
                            debug_logger.warning(f"ãƒšãƒ¼ã‚¸{page_num}æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
                            return ""
                    
                    # ğŸš€ ä¸¦åˆ—ãƒšãƒ¼ã‚¸æŠ½å‡ºï¼ˆæœ€å¤§4ã‚¹ãƒ¬ãƒƒãƒ‰ï¼‰
                    content = []
                    with ThreadPoolExecutor(max_workers=4) as executor:
                        futures = {executor.submit(extract_single_page, i): i for i in range(max_pages)}
                        for future in as_completed(futures):
                            try:
                                page_text = future.result(timeout=5.0)  # 5ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                                if page_text:
                                    content.append(page_text)
                            except Exception:
                                continue
                else:
                    # å°‘ãªã„ãƒšãƒ¼ã‚¸ã¯å¾“æ¥ã®åŒæœŸå‡¦ç†
                    content = []
                    for page_num in range(max_pages):
                        try:
                            page = doc[page_num]
                            page_text = page.get_text("text", sort=True)
                            if page_text and page_text.strip():
                                normalized = ' '.join(page_text.split())
                                if len(normalized) > 0:
                                    content.append(normalized)
                        except Exception as page_error:
                            debug_logger.warning(f"PDFãƒšãƒ¼ã‚¸ {page_num} èª­ã¿å–ã‚Šã‚¨ãƒ©ãƒ¼: {page_error}")
                            continue

                doc.close()
                extracted_text = ' '.join(content)
                
                # æ­£è¦åŒ–å‡¦ç†ã‚’é©ç”¨
                extracted_text = normalize_extracted_text(extracted_text, max_length=500000)
                
                if content:
                    debug_logger.debug(f"PDFæŠ½å‡ºæˆåŠŸ: {file_path} ({len(extracted_text)} æ–‡å­—)")
                    return extracted_text
                else:
                    debug_logger.warning(f"PDFå†…å®¹ãŒç©ºã§ã™: {file_path}")
                    return ""

            except ImportError:
                debug_logger.warning("PyMuPDFæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« - åŸºæœ¬PDFæŠ½å‡ºä½¿ç”¨")
            except PermissionError as pe:
                debug_logger.error(f"PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ã‚¨ãƒ©ãƒ¼: {pe}")
                return ""
            except FileNotFoundError as fnf:
                debug_logger.error(f"PDFãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {fnf}")
                return ""
            except Exception as e:
                debug_logger.error(f"PyMuPDFæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")

            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šåŸºæœ¬PDFæŠ½å‡ºï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹å®‰å…¨ç‰ˆï¼‰
            try:
                with open(file_path, 'rb') as f:
                    raw_content = f.read(1024 * 1024)  # æœ€åˆã®1MBã®ã¿èª­ã¿è¾¼ã¿

                # åŸºæœ¬çš„ãªPDFãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
                import re
                text_pattern = re.compile(rb'\(([^)]*)\)')
                matches = text_pattern.findall(raw_content)
                extracted_text = []

                for match in matches:
                    try:
                        decoded = match.decode('utf-8', errors='ignore')
                        if len(decoded.strip()) > 2:  # æ„å‘³ã®ã‚ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã®ã¿
                            extracted_text.append(decoded)
                    except:
                        continue

                return ' '.join(extracted_text)

            except Exception as e:
                print(f"âš ï¸ åŸºæœ¬PDFæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
                return ""

        except Exception as e:
            print(f"âš ï¸ PDFæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return ""

    def _extract_image_content(self, file_path: str) -> str:
        """.tifãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰OCRã§ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºï¼ˆè¶…é«˜é€Ÿæœ€é©åŒ–ç‰ˆãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¼·åŒ–ï¼‰"""
        try:
            # ğŸš€ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€å„ªå…ˆï¼‰
            if hasattr(self, '_ocr_cache'):
                cache_key = f"{file_path}_{os.path.getmtime(file_path)}"
                if cache_key in self._ocr_cache:
                    cached_result = self._ocr_cache[cache_key]
                    print(f"âš¡ OCRã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: {os.path.basename(file_path)} ({len(cached_result)}æ–‡å­—)")
                    return cached_result
            else:
                self._ocr_cache = {}

            # OCRãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
            if not PIL_AVAILABLE or not TESSERACT_AVAILABLE:
                return ""

            # Tesseractã‚¨ãƒ³ã‚¸ãƒ³ã®åˆ©ç”¨å¯èƒ½æ€§ã‚’ç¢ºèª
            try:
                pytesseract.get_tesseract_version()
            except pytesseract.TesseractNotFoundError:
                return ""

            # ğŸ”¥ è¶…é«˜é€Ÿã‚¹ã‚­ãƒƒãƒ—æ¡ä»¶ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºæœ€é©åŒ–ï¼‰
            file_size = os.path.getsize(file_path)
            if file_size < 1024:  # 1KBæœªæº€ã¯å‡¦ç†ã—ãªã„
                return ""
            if file_size > 30 * 1024 * 1024:  # 30MBä»¥ä¸Šã¯å‡¦ç†ã—ãªã„ï¼ˆã‚ˆã‚Šå³æ ¼ï¼‰
                print(f"âš ï¸ .tifç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤§ãã™ãã¾ã™ ({file_path}): {file_size/1024/1024:.1f}MB")
                return ""
            
            # ğŸš€ è¶…é«˜é€Ÿç”»åƒèª­ã¿è¾¼ã¿ãƒ»æ¤œè¨¼
            try:
                image = Image.open(file_path)
                
                # ç”»åƒãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãƒ»ãƒ¢ãƒ¼ãƒ‰æœ€é©åŒ–ãƒã‚§ãƒƒã‚¯
                if image.mode not in ['L', 'RGB', 'RGBA', '1']:
                    image = image.convert('RGB')
                
                # ç”»åƒã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯ã¨è¶…é«˜é€Ÿæœ€é©åŒ–
                width, height = image.size
                total_pixels = width * height
                
                # ğŸ”¥ å‹•çš„è§£åƒåº¦èª¿æ•´: ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã«å¿œã˜ã¦æœ€é©ãªç”»ç´ æ•°ã‚’é¸æŠ
                # å°ã•ã„ãƒ•ã‚¡ã‚¤ãƒ«: é«˜è§£åƒåº¦ã§OCRç²¾åº¦å‘ä¸Š
                # å¤§ãã„ãƒ•ã‚¡ã‚¤ãƒ«: ä½è§£åƒåº¦ã§å‡¦ç†é€Ÿåº¦å„ªå…ˆ
                if file_size < 2 * 1024 * 1024:  # 2MBæœªæº€
                    max_pixels = 1500000  # 150ä¸‡ç”»ç´ ï¼ˆç²¾åº¦å„ªå…ˆï¼‰
                elif file_size < 5 * 1024 * 1024:  # 5MBæœªæº€
                    max_pixels = 1000000  # 100ä¸‡ç”»ç´ ï¼ˆãƒãƒ©ãƒ³ã‚¹ï¼‰
                else:  # 5MBä»¥ä¸Š
                    max_pixels = 600000   # 60ä¸‡ç”»ç´ ï¼ˆé€Ÿåº¦å„ªå…ˆï¼‰
                
                if total_pixels > max_pixels:
                    scale_factor = (max_pixels / total_pixels) ** 0.5
                    new_width = int(width * scale_factor)
                    new_height = int(height * scale_factor)
                    # é«˜é€Ÿãƒªã‚µã‚¤ã‚ºã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ä½¿ç”¨
                    image = image.resize((new_width, new_height), Image.Resampling.BILINEAR)
                    total_pixels = new_width * new_height
                    debug_logger.debug(f"å‹•çš„ãƒªã‚µã‚¤ã‚º ({os.path.basename(file_path)}): {width}x{height} -> {new_width}x{new_height}")
                
                # å°ã•ã™ãã‚‹ç”»åƒã¯ã‚¹ã‚­ãƒƒãƒ—
                if total_pixels < 10000:  # 100x100æœªæº€ã¯ã‚¹ã‚­ãƒƒãƒ—
                    return ""
                
            except Exception as e:
                print(f"âš ï¸ ç”»åƒèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ ({file_path}): {e}")
                return ""
            
            # ğŸš€ è¶…é«˜é€ŸOCRè¨­å®šï¼ˆé€Ÿåº¦æœ€å„ªå…ˆï¼‰
            ultra_fast_config = r'--oem 1 --psm 6 -c tessedit_char_whitelist=0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz ã‚ã„ã†ãˆãŠã‹ããã‘ã“ã•ã—ã™ã›ããŸã¡ã¤ã¦ã¨ãªã«ã¬ã­ã®ã¯ã²ãµã¸ã»ã¾ã¿ã‚€ã‚ã‚‚ã‚„ã‚†ã‚ˆã‚‰ã‚Šã‚‹ã‚Œã‚ã‚ã‚’ã‚“'  # æ–‡å­—åˆ¶é™ã§é«˜é€ŸåŒ–
            
            # ğŸ”¥ é©å¿œå‹å‰å‡¦ç†: ç”»åƒç‰¹æ€§ã«å¿œã˜ã¦æœ€é©ãªå‰å‡¦ç†ã‚’é¸æŠ
            processed_image = image
            
            # å‰å‡¦ç†ãŒå¿…è¦ãªæ¡ä»¶: ã‚«ãƒ©ãƒ¼ç”»åƒã‹ã¤ä¸­è¦æ¨¡ã‚µã‚¤ã‚º
            needs_preprocessing = (image.mode != 'L' and 
                                  total_pixels < 500000 and 
                                  file_size > 500 * 1024)  # 500KBä»¥ä¸Š
            
            if CV2_AVAILABLE and needs_preprocessing:
                try:
                    import numpy as np
                    image_array = np.array(image)
                    
                    # ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›ï¼ˆæœ€ã‚‚åŠ¹æœçš„ãªå‰å‡¦ç†ï¼‰
                    if len(image_array.shape) == 3:
                        gray = cv2.cvtColor(image_array, cv2.COLOR_RGB2GRAY)
                        
                        # å°ã•ã„ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿äºŒå€¤åŒ–ã‚’è¿½åŠ ï¼ˆOCRç²¾åº¦å‘ä¸Šï¼‰
                        if file_size < 2 * 1024 * 1024:
                            # é©å¿œçš„äºŒå€¤åŒ–: ç…§æ˜ãƒ ãƒ©ã«å¼·ã„
                            gray = cv2.adaptiveThreshold(
                                gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                cv2.THRESH_BINARY, 11, 2)
                        
                        processed_image = Image.fromarray(gray)
                except Exception:
                    processed_image = image
            
            # ğŸš€ è¶…é«˜é€ŸOCRå®Ÿè¡Œï¼ˆæ®µéšçš„æœ€é©åŒ– + è¨€èªæ¤œå‡ºï¼‰
            text = ""
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰è¨€èªã‚’ãƒ’ãƒ³ãƒˆå–å¾—ï¼ˆå‡¦ç†ã®æœ€é©åŒ–ï¼‰
            filename_lower = os.path.basename(file_path).lower()
            likely_japanese = any(hint in filename_lower for hint in ['æ—¥æœ¬èª', 'japanese', 'jpn', 'å›³é¢', 'è¨­è¨ˆ'])
            
            # Phase 1: è¶…é«˜é€Ÿè‹±æ•°å­—ã®ã¿ï¼ˆæœ€ã‚‚é«˜é€Ÿï¼‰
            try:
                if not likely_japanese:  # æ—¥æœ¬èªã®å¯èƒ½æ€§ãŒä½ã„å ´åˆã®ã¿
                    fast_config = r'--oem 1 --psm 6 -c tessedit_char_whitelist=0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'
                    text = pytesseract.image_to_string(processed_image, lang='eng', config=fast_config).strip()
                
                # Phase 2: çµæœãŒä¸ååˆ†ãªå ´åˆã®ã¿é€šå¸¸è‹±èªOCR
                if len(text) < 5:
                    text = pytesseract.image_to_string(processed_image, lang='eng', config='--oem 1 --psm 6').strip()
                
                # Phase 3: æœ€å¾Œã®æ‰‹æ®µã¨ã—ã¦æ—¥æœ¬èªï¼ˆå‡¦ç†æ™‚é–“ãŒå¢—åŠ ï¼‰
                # å°ã•ã„ãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯æ—¥æœ¬èªã®å¯èƒ½æ€§ãŒé«˜ã„å ´åˆã®ã¿è©¦è¡Œ
                if (len(text) < 3 and file_size < 5 * 1024 * 1024) or likely_japanese:
                    try:
                        jp_text = pytesseract.image_to_string(processed_image, lang='jpn', config='--oem 1 --psm 6').strip()
                        if len(jp_text) > len(text):
                            text = jp_text
                    except pytesseract.TesseractError:
                        pass
                        
            except pytesseract.TesseractError as te:
                try:
                    # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šæœ€å°è¨­å®š
                    text = pytesseract.image_to_string(processed_image, config='--psm 6').strip()
                except pytesseract.TesseractError:
                    print(f"âš ï¸ OCRå®Ÿè¡Œå®Œå…¨å¤±æ•— ({os.path.basename(file_path)}): {te}")
                    return ""
            
            # ğŸ”¥ çµæœæ¤œè¨¼ã¨æœ€é©åŒ–
            text = text.strip()
            
            # ç„¡æ„å‘³ãªçµæœã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            if len(text) < 2:
                result = ""
            elif len(set(text.replace(' ', '').replace('\n', ''))) < 3:  # æ–‡å­—ç¨®é¡ãŒå°‘ãªã™ãã‚‹
                result = ""
            else:
                # ãƒ†ã‚­ã‚¹ãƒˆæ­£è¦åŒ–ï¼ˆé«˜é€Ÿç‰ˆï¼‰
                text = ' '.join(text.split())  # ä½™åˆ†ãªç©ºç™½ã‚’å‰Šé™¤
                result = text[:5000]  # æœ€å¤§5000æ–‡å­—ã«åˆ¶é™
            
            # ğŸš€ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ï¼ˆæˆåŠŸãƒ»å¤±æ•—ã‚’å•ã‚ãšï¼‰
            cache_key = f"{file_path}_{os.path.getmtime(file_path)}"
            self._ocr_cache[cache_key] = result
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºåˆ¶é™
            if len(self._ocr_cache) > 1000:
                # å¤ã„ã‚¨ãƒ³ãƒˆãƒªã‚’å‰Šé™¤ï¼ˆLRUçš„ï¼‰
                oldest_keys = list(self._ocr_cache.keys())[:100]
                for key in oldest_keys:
                    del self._ocr_cache[key]
            
            # çµæœè¡¨ç¤ºï¼ˆæˆåŠŸæ™‚ã®ã¿ï¼‰
            if result and len(result) > 10:
                print(f"âœ… è¶…é«˜é€ŸOCRæˆåŠŸ ({os.path.basename(file_path)}): {len(result)}æ–‡å­—")
            
            return result
            
        except Exception as e:
            print(f"âš ï¸ è¶…é«˜é€ŸOCRå‡¦ç†ã‚¨ãƒ©ãƒ¼ {os.path.basename(file_path)}: {e}")
            # ã‚¨ãƒ©ãƒ¼ã‚‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¦å†è©¦è¡Œã‚’é˜²ã
            if hasattr(self, '_ocr_cache'):
                cache_key = f"{file_path}_{os.path.getmtime(file_path)}"
                self._ocr_cache[cache_key] = ""
            return ""

    # CAD/å›³é¢ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹æŠ½å‡ºã¯ç„¡åŠ¹åŒ–ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿æ¤œç´¢å¯¾è±¡ï¼‰
    # å°†æ¥çš„ã«å¿…è¦ã«ãªã£ãŸå ´åˆã®ãŸã‚ã«ã€ã‚³ãƒ¼ãƒ‰ã¯æ®‹ã—ã¦ãŠã
    """
    def _extract_cad_content(self, file_path: str) -> str:
        \"\"\"CAD/å›³é¢ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆJWC, JWW, DXF, SFCï¼‰ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º - ç¾åœ¨ã¯ç„¡åŠ¹\"\"\"
        try:
            extension = os.path.splitext(file_path)[1].lower()
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯ï¼ˆå¤§ãã™ãã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
            file_size = os.path.getsize(file_path)
            if file_size > 50 * 1024 * 1024:  # 50MBä»¥ä¸Šã¯ã‚¹ã‚­ãƒƒãƒ—
                print(f"âš ï¸ CADãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤§ãã™ãã¾ã™ ({file_path}): {file_size/1024/1024:.1f}MB")
                return ""
            
            # DXFãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†ï¼ˆãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ã®ASCIIå½¢å¼ï¼‰
            if extension == '.dxf':
                try:
                    # DXFã¯ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ãªã®ã§ç›´æ¥èª­ã¿å–ã‚Š
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read(100000)  # æœ€åˆã®100KB
                        # ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚„å±æ€§ã‚’æŠ½å‡º
                        lines = content.split('\\n')
                        text_parts = []
                        for i, line in enumerate(lines):
                            if line.strip() in ['TEXT', 'MTEXT', 'ATTRIB', 'ATTDEF']:
                                # ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æ¢ã™
                                for j in range(i+1, min(i+20, len(lines))):
                                    if lines[j].strip() and not lines[j].strip().isdigit():
                                        text_parts.append(lines[j].strip())
                        return ' '.join(text_parts[:1000])  # æœ€å¤§1000è¦ç´ 
                except UnicodeDecodeError:
                    # ãƒã‚¤ãƒŠãƒªå½¢å¼ã®DXFã®å ´åˆ
                    try:
                        with open(file_path, 'r', encoding='cp932', errors='ignore') as f:
                            content = f.read(50000)
                            return content[:5000]
                    except:
                        return ""
            
            # JWW/JWC/SFCãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆãƒã‚¤ãƒŠãƒªãƒ™ãƒ¼ã‚¹ï¼‰
            elif extension in ['.jww', '.jwc', '.sfc']:
                try:
                    # ãƒã‚¤ãƒŠãƒªã‹ã‚‰å¯èƒ½ãªé™ã‚Šãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ†ã‚’æŠ½å‡º
                    with open(file_path, 'rb') as f:
                        data = f.read(100000)  # æœ€åˆã®100KB
                        
                        # Shift-JISã¾ãŸã¯UTF-8ã§ãƒ‡ã‚³ãƒ¼ãƒ‰å¯èƒ½ãªéƒ¨åˆ†ã‚’æ¢ã™
                        text_parts = []
                        
                        # ãƒã‚¤ãƒˆåˆ—ã‹ã‚‰é€£ç¶šã™ã‚‹å°åˆ·å¯èƒ½æ–‡å­—ã‚’æ¢ã™
                        current_text = bytearray()
                        for byte in data:
                            # å°åˆ·å¯èƒ½ãªASCIIæ–‡å­—ã€ã¾ãŸã¯æ—¥æœ¬èªã®å¯èƒ½æ€§ãŒã‚ã‚‹ç¯„å›²
                            if (32 <= byte <= 126) or (byte >= 0x80):
                                current_text.append(byte)
                            else:
                                if len(current_text) > 3:  # 3ãƒã‚¤ãƒˆä»¥ä¸Šã®é€£ç¶š
                                    try:
                                        decoded = current_text.decode('cp932', errors='ignore')
                                        if len(decoded.strip()) > 2:
                                            text_parts.append(decoded.strip())
                                    except:
                                        try:
                                            decoded = current_text.decode('utf-8', errors='ignore')
                                            if len(decoded.strip()) > 2:
                                                text_parts.append(decoded.strip())
                                        except:
                                            pass
                                current_text = bytearray()
                        
                        # æœ€å¾Œã®éƒ¨åˆ†ã‚‚å‡¦ç†
                        if len(current_text) > 3:
                            try:
                                decoded = current_text.decode('cp932', errors='ignore')
                                if len(decoded.strip()) > 2:
                                    text_parts.append(decoded.strip())
                            except:
                                pass
                        
                        # ãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ†ã‚’çµåˆ
                        result = ' '.join(text_parts[:500])  # æœ€å¤§500è¦ç´ 
                        
                        if len(result) > 10:
                            print(f"âœ… CADãƒ•ã‚¡ã‚¤ãƒ« ({extension}) ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º: {os.path.basename(file_path)} ({len(result)}æ–‡å­—)")
                        
                        return result[:5000]  # æœ€å¤§5000æ–‡å­—
                        
                except Exception as e:
                    print(f"âš ï¸ CADãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿å–ã‚Šã‚¨ãƒ©ãƒ¼ ({file_path}): {e}")
                    return ""
            
            return ""
            
        except Exception as e:
            print(f"âš ï¸ CADãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
            return ""
    """

    def _process_text_files_batch(self, text_files: List[Path], start_time: float) -> int:
        """ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®é«˜é€Ÿãƒãƒƒãƒå‡¦ç†"""
        success_count = 0
        processed_count = 0
        
        try:
            for batch_start in range(0, len(text_files), self.batch_size):
                batch_end = min(batch_start + self.batch_size, len(text_files))
                batch_files = text_files[batch_start:batch_end]
                
                with concurrent.futures.ThreadPoolExecutor(max_workers=self.optimal_threads) as executor:
                    future_to_file = {
                        executor.submit(self.search_system.live_progressive_index_file, str(file_path)): file_path
                        for file_path in batch_files
                    }
                    
                    for future in concurrent.futures.as_completed(future_to_file):
                        try:
                            success = future.result(timeout=30.0)
                            if success:
                                success_count += 1
                            processed_count += 1
                            
                            # é€²æ—è¡¨ç¤º
                            if processed_count % 200 == 0:
                                elapsed_time = time.time() - start_time
                                files_per_sec = processed_count / elapsed_time if elapsed_time > 0 else 0
                                print(f"ğŸ“„ ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†: {processed_count:,}/{len(text_files):,} - {files_per_sec:.1f}ãƒ•ã‚¡ã‚¤ãƒ«/ç§’")
                                
                        except Exception as e:
                            print(f"âš ï¸ ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                            processed_count += 1
        
        except Exception as e:
            print(f"âš ï¸ ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒãƒå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        
        return success_count

    def _process_image_files_optimized(self, image_files: List[Path], start_time: float, 
                                     processed_offset: int, total_files: int) -> int:
        """.tifç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®CPUä½¿ç”¨ç‡æœ€é©åŒ–å‡¦ç†ï¼ˆè¶…é«˜é€Ÿç‰ˆãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¼·åŒ–ï¼‰"""
        success_count = 0
        processed_count = 0
        
        # ğŸš€ OCRã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆæœŸåŒ–
        if not hasattr(self, '_ocr_cache'):
            self._ocr_cache = {}
        
        print(f"ğŸ”§ è¶…é«˜é€ŸOCRå‡¦ç†è¨­å®š: {self.ocr_threads}ã‚¹ãƒ¬ãƒƒãƒ‰, ãƒãƒƒãƒã‚µã‚¤ã‚º{self.image_batch_size}, é…å»¶{self.ocr_processing_delay}ç§’")
        print(f"ğŸ’¾ OCRã‚­ãƒ£ãƒƒã‚·ãƒ¥: {len(self._ocr_cache)}ä»¶ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ¸ˆã¿")
        
        try:
            # ğŸ”¥ äº‹å‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒ™ãƒ¼ã‚¹ï¼‰
            filtered_files = []
            skipped_count = 0
            
            for file_path in image_files:
                try:
                    file_size = os.path.getsize(file_path)
                    # ã‚µã‚¤ã‚ºãƒ™ãƒ¼ã‚¹ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆå‡¦ç†å‰ã«é™¤å¤–ï¼‰
                    if 1024 <= file_size <= 30 * 1024 * 1024:  # 1KBï½30MBã®ã¿å‡¦ç†
                        filtered_files.append(file_path)
                    else:
                        skipped_count += 1
                except:
                    skipped_count += 1
            
            if skipped_count > 0:
                print(f"âš¡ äº‹å‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°: {len(image_files)}ä»¶ â†’ {len(filtered_files)}ä»¶ ({skipped_count}ä»¶ã‚¹ã‚­ãƒƒãƒ—)")
            
            if not filtered_files:
                return 0
            
            # ğŸš€ å‹•çš„ãƒãƒƒãƒã‚µã‚¤ã‚ºèª¿æ•´ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«æ•°ã«å¿œã˜ã¦ï¼‰
            dynamic_batch_size = self.image_batch_size
            if len(filtered_files) > 100:
                dynamic_batch_size = min(self.image_batch_size * 2, 50)  # å¤§é‡ãƒ•ã‚¡ã‚¤ãƒ«æ™‚ã¯ãƒãƒƒãƒã‚µã‚¤ã‚ºæ‹¡å¤§
            elif len(filtered_files) < 20:
                dynamic_batch_size = max(self.image_batch_size // 2, 5)  # å°‘æ•°ãƒ•ã‚¡ã‚¤ãƒ«æ™‚ã¯ãƒãƒƒãƒã‚µã‚¤ã‚ºç¸®å°
            
            print(f"ğŸ”§ å‹•çš„ãƒãƒƒãƒã‚µã‚¤ã‚º: {dynamic_batch_size} (å…ƒ: {self.image_batch_size})")
            
            for batch_start in range(0, len(filtered_files), dynamic_batch_size):
                batch_end = min(batch_start + dynamic_batch_size, len(filtered_files))
                batch_files = filtered_files[batch_start:batch_end]
                
                batch_num = batch_start // dynamic_batch_size + 1
                total_batches = (len(filtered_files) + dynamic_batch_size - 1) // dynamic_batch_size
                print(f"ğŸ–¼ï¸ OCRãƒãƒƒãƒ {batch_num}/{total_batches}: {len(batch_files)}ãƒ•ã‚¡ã‚¤ãƒ«")
                
                # ğŸ”¥ ä¸¦åˆ—å‡¦ç†ã®æœ€é©åŒ–ï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰æ•°å‹•çš„èª¿æ•´ï¼‰
                actual_workers = min(self.ocr_threads, len(batch_files))
                
                with concurrent.futures.ThreadPoolExecutor(max_workers=actual_workers) as executor:
                    # ğŸš€ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®šã®æœ€é©åŒ–
                    def process_with_timeout(file_path):
                        try:
                            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã«åŸºã¥ãã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆèª¿æ•´
                            file_size = os.path.getsize(file_path)
                            if file_size < 1024 * 1024:  # 1MBæœªæº€
                                timeout = 30
                            elif file_size < 5 * 1024 * 1024:  # 5MBæœªæº€
                                timeout = 60
                            else:  # ãã‚Œä»¥ä¸Š
                                timeout = 120
                                
                            return self.search_system.live_progressive_index_file(str(file_path)), timeout
                        except Exception as e:
                            return False, 30
                    
                    future_to_file = {
                        executor.submit(process_with_timeout, file_path): file_path
                        for file_path in batch_files
                    }
                    
                    batch_success = 0
                    batch_processed = 0
                    
                    for future in concurrent.futures.as_completed(future_to_file):
                        file_path = future_to_file[future]
                        try:
                            success, timeout_used = future.result(timeout=120)  # æœ€å¤§2åˆ†
                            if success:
                                batch_success += 1
                                success_count += 1
                            batch_processed += 1
                            processed_count += 1
                            
                            # ğŸ”¥ å‹•çš„é…å»¶èª¿æ•´ï¼ˆCPUè² è·ã«å¿œã˜ã¦ï¼‰
                            if self.ocr_processing_delay > 0:
                                # ãƒãƒƒãƒé€²è¡Œã«å¿œã˜ã¦é…å»¶ã‚’çŸ­ç¸®
                                progress_ratio = batch_processed / len(batch_files)
                                adjusted_delay = self.ocr_processing_delay * (1.0 - progress_ratio * 0.5)
                                time.sleep(max(adjusted_delay, 0.01))
                            
                            # ğŸš€ é€²æ—è¡¨ç¤ºã®æœ€é©åŒ–
                            if processed_count % 5 == 0 or processed_count == len(filtered_files):
                                total_processed = processed_offset + processed_count
                                progress = (total_processed / total_files) * 100
                                elapsed_time = time.time() - start_time
                                files_per_sec = total_processed / elapsed_time if elapsed_time > 0 else 0
                                cache_hit_rate = (len(self._ocr_cache) / max(processed_count, 1)) * 100
                                print(f"ğŸ” è¶…é«˜é€ŸOCR: {processed_count:,}/{len(filtered_files):,} ç”»åƒ - "
                                      f"é€²æ— {progress:.1f}% ({files_per_sec:.1f}ãƒ•ã‚¡ã‚¤ãƒ«/ç§’) "
                                      f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç‡ {cache_hit_rate:.1f}%")
                                
                        except concurrent.futures.TimeoutError:
                            print(f"âš ï¸ OCRã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {os.path.basename(file_path)}")
                            processed_count += 1
                        except Exception as e:
                            print(f"âš ï¸ ç”»åƒOCRå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                            processed_count += 1
                
                # ğŸ”¥ ãƒãƒƒãƒé–“é…å»¶ã®æœ€é©åŒ–ï¼ˆé€²è¡ŒçŠ¶æ³ã«å¿œã˜ã¦èª¿æ•´ï¼‰
                if batch_end < len(filtered_files):
                    batch_progress = batch_end / len(filtered_files)
                    # é€²è¡Œã«å¿œã˜ã¦é…å»¶ã‚’çŸ­ç¸®ï¼ˆå¾ŒåŠã¯é«˜é€ŸåŒ–ï¼‰
                    adjusted_batch_delay = self.ocr_processing_delay * (2.0 - batch_progress)
                    time.sleep(max(adjusted_batch_delay, 0.05))
                
                # ãƒãƒƒãƒçµæœè¡¨ç¤º
                print(f"âœ… ãƒãƒƒãƒ {batch_num} å®Œäº†: {batch_success}/{len(batch_files)} æˆåŠŸ")
        
        except Exception as e:
            print(f"âš ï¸ .tifç”»åƒãƒ•ã‚¡ã‚¤ãƒ«è¶…é«˜é€Ÿãƒãƒƒãƒå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        
        # æœ€çµ‚çµ±è¨ˆ
        if processed_count > 0:
            elapsed = time.time() - start_time
            avg_speed = processed_count / elapsed if elapsed > 0 else 0
            cache_efficiency = len(self._ocr_cache) / max(processed_count, 1) * 100
            print(f"ğŸ“Š OCRå‡¦ç†å®Œäº†çµ±è¨ˆ: {success_count}/{processed_count} æˆåŠŸ "
                  f"({avg_speed:.2f}ãƒ•ã‚¡ã‚¤ãƒ«/ç§’, ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡ {cache_efficiency:.1f}%)")
        
        return success_count

    def bulk_index_directory_with_progress(self,
                                         directory: str,
                                         progress_callback=None,
                                         file_extensions: Optional[List[str]] = None) -> Dict[str, Any]:
        """æœ€é©åŒ–æ¸ˆã¿é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ããƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä¸€æ‹¬ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹"""
        if file_extensions is None:
            file_extensions = ['.txt', '.docx', '.xlsx', '.pdf', 
                             '.tif', '.tiff', '.doc', '.xls', '.ppt', '.pptx',
                             '.dot', '.dotx', '.dotm', '.docm',  # Wordé–¢é€£è¿½åŠ 
                             '.xlt', '.xltx', '.xltm', '.xlsm', '.xlsb',  # Excelé–¢é€£è¿½åŠ 
                             '.zip',  # ZIPãƒ•ã‚¡ã‚¤ãƒ«è¿½åŠ 
                             '.jwc', '.dxf', '.sfc', '.jww',  # CADãƒ•ã‚¡ã‚¤ãƒ«è¿½åŠ 
                             '.dwg', '.dwt', '.mpp', '.mpz']  # è¿½åŠ CADãƒ•ã‚¡ã‚¤ãƒ«

        start_time = time.time()
        directory_path = Path(directory)
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çŠ¶æ…‹è¨­å®š
        self.indexing_in_progress = True
        self.indexing_results_ready = False
        
        print(f"âš¡ æœ€é©åŒ–ãƒãƒ«ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é–‹å§‹: {directory}")
        
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«åé›†ï¼ˆä¸¦åˆ—åŒ–ã§é«˜é€ŸåŒ–ï¼‰
            all_files = []
            with ThreadPoolExecutor(max_workers=4) as executor:
                # æ‹¡å¼µå­ã”ã¨ã«ä¸¦åˆ—ã§ãƒ•ã‚¡ã‚¤ãƒ«åé›†
                futures = {executor.submit(self._collect_files_by_extension, 
                                         directory_path, ext): ext for ext in file_extensions}
                
                for future in as_completed(futures):
                    ext = futures[future]
                    try:
                        files = future.result()
                        all_files.extend(files)
                    except Exception as e:
                        print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«åé›†ã‚¨ãƒ©ãƒ¼ ({ext}): {e}")
            
            total_files = len(all_files)
            print(f"ğŸ“Š åé›†å®Œäº†: {total_files}ãƒ•ã‚¡ã‚¤ãƒ«")
            
            # é€²æ—ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°åˆæœŸåŒ–
            success_count = 0
            error_count = 0
            
            if progress_callback:
                progress_callback("", "", True)  # åˆæœŸåŒ–
            
            # ãƒãƒƒãƒå‡¦ç†ã§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Š
            batch_size = min(self.batch_size // 4, 500)  # é©åº¦ãªãƒãƒƒãƒã‚µã‚¤ã‚º
            
            for i in range(0, total_files, batch_size):
                # ã‚­ãƒ£ãƒ³ã‚»ãƒ«ãƒã‚§ãƒƒã‚¯
                if hasattr(self, 'indexing_cancelled') and self.indexing_cancelled:
                    print("â¹ï¸ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‡¦ç†ãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ")
                    return {
                        'total_files': total_files,
                        'success_count': success_count,
                        'error_count': error_count,
                        'total_time': time.time() - start_time,
                        'files_per_second': 0,
                        'cancelled': True
                    }
                
                batch_files = all_files[i:i + batch_size]
                batch_results = self._process_file_batch_optimized(batch_files, progress_callback)
                success_count += batch_results['success']
                error_count += batch_results['errors']
            
            # å‡¦ç†å®Œäº†
            total_time = time.time() - start_time
            files_per_second = success_count / total_time if total_time > 0 else 0
            
            result = {
                'total_files': total_files,
                'success_count': success_count,
                'error_count': error_count,
                'total_time': total_time,
                'files_per_second': files_per_second
            }
            
            print(f"âœ… ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å®Œäº†: {success_count}/{total_files}ãƒ•ã‚¡ã‚¤ãƒ« ({files_per_second:.1f}ãƒ•ã‚¡ã‚¤ãƒ«/ç§’)")
            
            return result
            
        finally:
            self.indexing_in_progress = False
            self.indexing_results_ready = True
            
    def _collect_files_by_extension(self, directory_path: Path, extension: str) -> List[Path]:
        """æ‹¡å¼µå­ã”ã¨ã®ãƒ•ã‚¡ã‚¤ãƒ«åé›†ï¼ˆä¸¦åˆ—å‡¦ç†ç”¨ï¼‰"""
        try:
            all_files = list(directory_path.rglob(f'*{extension}'))
            # macOSéš ã—ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã‚·ã‚¹ãƒ†ãƒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            filtered_files = []
            for file_path in all_files:
                # ._ã§å§‹ã¾ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆmacOSéš ã—ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ã‚’ã‚¹ã‚­ãƒƒãƒ—
                if file_path.name.startswith('._'):
                    continue
                # ã‚·ã‚¹ãƒ†ãƒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—
                if file_path.name in ['.DS_Store', 'Thumbs.db', 'desktop.ini']:
                    continue
                # éš ã—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ã‚¹ã‚­ãƒƒãƒ—
                if any(part.startswith('.') and part not in ['.', '..'] for part in file_path.parts):
                    continue
                filtered_files.append(file_path)
            return filtered_files
        except Exception as e:
            print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«åé›†ã‚¨ãƒ©ãƒ¼ ({extension}): {e}")
            return []
    
    def _process_file_batch_optimized(self, batch_files: List[Path], progress_callback=None) -> Dict[str, int]:
        """æœ€é©åŒ–ç‰ˆãƒãƒƒãƒãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºåˆ¥å„ªå…ˆåº¦ä»˜ãï¼‰"""
        success_count = 0
        error_count = 0
        
        # ğŸ”¥ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚µã‚¤ã‚ºåˆ¥ã«ã‚½ãƒ¼ãƒˆï¼ˆå°ã•ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å„ªå…ˆå‡¦ç†ï¼‰
        sorted_files = []
        for file_path in batch_files:
            try:
                size = file_path.stat().st_size
                sorted_files.append((file_path, size))
            except:
                sorted_files.append((file_path, 0))
        
        # å°ã•ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å„ªå…ˆã—ã¦ã‚½ãƒ¼ãƒˆ
        sorted_files.sort(key=lambda x: x[1])
        prioritized_files = [f[0] for f in sorted_files]
        
        # ğŸš€ å‹•çš„ã‚¹ãƒ¬ãƒƒãƒ‰æ•°èª¿æ•´: ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã¨ã‚µã‚¤ã‚ºã«å¿œã˜ã¦æœ€é©åŒ–
        # å°ã•ã„ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤šã„å ´åˆã¯ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã‚’å¢—ã‚„ã™
        small_file_ratio = sum(1 for _, size in sorted_files if size < 3*1024*1024) / max(len(sorted_files), 1)
        if small_file_ratio > 0.7:  # 70%ä»¥ä¸ŠãŒå°ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ3MBæœªæº€ï¼‰
            dynamic_workers = min(self.optimal_threads * 4, len(batch_files), 128)  # æœ€å¤§128ä¸¦åˆ—
        else:
            dynamic_workers = min(self.optimal_threads * 2, len(batch_files), 64)  # æœ€å¤§64ä¸¦åˆ—
        
        debug_logger.info(f"ãƒãƒƒãƒå‡¦ç†é–‹å§‹: {len(prioritized_files)}ãƒ•ã‚¡ã‚¤ãƒ«, {dynamic_workers}ã‚¹ãƒ¬ãƒƒãƒ‰")
        
        with ThreadPoolExecutor(max_workers=dynamic_workers) as executor:
            # å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸¦åˆ—å‡¦ç†
            futures = {executor.submit(self._process_single_file_with_progress, 
                                     file_path, progress_callback): file_path 
                      for file_path in prioritized_files}
            
            for future in as_completed(futures):
                file_path = futures[future]
                try:
                    # ğŸ”¥ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã«å¿œã˜ãŸå‹•çš„ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                    file_size = file_path.stat().st_size if file_path.exists() else 0
                    if file_size >= 3 * 1024 * 1024:  # 3MBä»¥ä¸Šï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã®ã¿ï¼‰
                        timeout = 3  # è¶…é«˜é€Ÿ
                    elif file_size < 1 * 1024 * 1024:  # 1MBæœªæº€
                        timeout = 5
                    else:  # 1-3MB
                        timeout = 10
                    
                    result = future.result(timeout=timeout)
                    if result:
                        success_count += 1
                    else:
                        error_count += 1
                except concurrent.futures.TimeoutError:
                    error_count += 1
                    debug_logger.warning(f"ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {file_path}")
                except Exception as e:
                    error_count += 1
                    debug_logger.error(f"ãƒãƒƒãƒå‡¦ç†ã‚¨ãƒ©ãƒ¼: {file_path} - {e}")
                    
        return {'success': success_count, 'errors': error_count}
    
    def _process_single_file_with_progress(self, file_path: Path, progress_callback=None) -> bool:
        """é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãå˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†"""
        try:
            # macOSéš ã—ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ._ã§å§‹ã¾ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ã‚’ã‚¹ã‚­ãƒƒãƒ—
            if file_path.name.startswith('._'):
                if progress_callback:
                    progress_callback(str(file_path), "skipped", False)
                return True  # ã‚¹ã‚­ãƒƒãƒ—ã¯æˆåŠŸã¨ã—ã¦æ‰±ã†
            
            # ãã®ä»–ã®éš ã—ãƒ•ã‚¡ã‚¤ãƒ«ãƒ»ã‚·ã‚¹ãƒ†ãƒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ã‚¹ã‚­ãƒƒãƒ—
            if file_path.name.startswith('.DS_Store') or file_path.name.startswith('Thumbs.db'):
                if progress_callback:
                    progress_callback(str(file_path), "skipped", False)
                return True  # ã‚¹ã‚­ãƒƒãƒ—ã¯æˆåŠŸã¨ã—ã¦æ‰±ã†
            
            # ğŸš€ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã«ã‚ˆã‚‹å‡¦ç†åˆ†å²ï¼ˆå¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«æœ€é©åŒ–ï¼‰
            try:
                size = file_path.stat().st_size
                
                # ğŸ”¥ è¶…å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«ã®æ—©æœŸã‚¹ã‚­ãƒƒãƒ—ï¼ˆ500MBä»¥ä¸Šï¼‰
                if size > 500 * 1024 * 1024:  # 500MBä»¥ä¸Š
                    if progress_callback:
                        progress_callback(str(file_path), "skipped_large", False)
                    debug_logger.info(f"è¶…å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—: {file_path.name} ({size/(1024*1024):.1f}MB)")
                    return True  # ã‚¹ã‚­ãƒƒãƒ—ã¯æˆåŠŸã¨ã—ã¦æ‰±ã†
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚«ãƒ†ã‚´ãƒªåˆ¤å®šï¼ˆ3MBä»¥ä¸Šã¯ã‚¿ã‚¤ãƒˆãƒ«ã®ã¿ï¼‰
                if size >= 3 * 1024 * 1024:  # 3MBä»¥ä¸Š
                    category = "title_only"
                elif size < 1 * 1024 * 1024:  # 1MBæœªæº€
                    category = "light"
                else:  # 1-3MB
                    category = "medium"
            except:
                category = "light"
            
            # é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œ
            if progress_callback:
                progress_callback(str(file_path), category, True)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
            return self.search_system.live_progressive_index_file(str(file_path))
            
        except Exception as e:
            if progress_callback:
                progress_callback(str(file_path), "error", False)
            return False
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çŠ¶æ…‹ã‚’ã‚¯ãƒªã‚¢
            self.indexing_in_progress = False
            self.indexing_results_ready = True

    def bulk_index_directory(self,
                             directory: str,
                             file_extensions: Optional[List[str]] = None) -> Dict[str, Any]:
        """ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä¸€æ‹¬ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ - å³åº§é–‹å§‹ç‰ˆï¼ˆ0.1ç§’ä»¥å†…é–‹å§‹ä¿è¨¼ï¼‰"""
        if file_extensions is None:
            file_extensions = ['.txt', '.docx', '.xlsx', '.pdf', 
                             '.tif', '.tiff', '.doc', '.xls', '.ppt', '.pptx',
                             '.dot', '.dotx', '.dotm', '.docm',  # Wordé–¢é€£è¿½åŠ 
                             '.xlt', '.xltx', '.xltm', '.xlsm', '.xlsb',  # Excelé–¢é€£è¿½åŠ 
                             '.zip',  # ZIPãƒ•ã‚¡ã‚¤ãƒ«è¿½åŠ 
                             '.jwc', '.dxf', '.sfc', '.jww',  # CADãƒ•ã‚¡ã‚¤ãƒ«è¿½åŠ 
                             '.dwg', '.dwt', '.mpp', '.mpz']  # è¿½åŠ CADãƒ•ã‚¡ã‚¤ãƒ«

        start_time = time.time()
        directory_path = Path(directory)
        
        # ğŸ“Œ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çŠ¶æ…‹ã‚’æœ€å„ªå…ˆã§è¨­å®šï¼ˆ0.001ç§’ä»¥å†…ï¼‰
        self.indexing_in_progress = True
        self.indexing_results_ready = False
        
        print(f"âš¡ å³åº§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é–‹å§‹: {directory}")
        print(f"ğŸ“‚ å¯¾è±¡æ‹¡å¼µå­: {', '.join(file_extensions)}")
        print(f"ğŸ”„ å‡¦ç†çŠ¶æ…‹: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸­ - æ¤œç´¢çµæœã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰æä¾›")
        
        # å³åº§ã«å°è¦æ¨¡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’é–‹å§‹ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼åå¿œæ€§ç¢ºä¿ï¼‰
        def quick_start_indexing():
            """0.1ç§’ä»¥å†…ã«æœ€åˆã®ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚’é–‹å§‹"""
            quick_files = []
            for ext in file_extensions:
                try:
                    # å„æ‹¡å¼µå­ã‹ã‚‰æœ€å¤§5ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å³åº§ã«å–å¾—
                    all_pattern_files = list(directory_path.rglob(f'*{ext}'))
                    # éš ã—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                    pattern_files = []
                    for file_path in all_pattern_files:
                        if not file_path.name.startswith('._') and file_path.name not in ['.DS_Store', 'Thumbs.db', 'desktop.ini']:
                            pattern_files.append(file_path)
                        if len(pattern_files) >= 5:
                            break
                    quick_files.extend(pattern_files)
                    if len(quick_files) >= 20:  # 20ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—ã—ãŸã‚‰å³åº§å‡¦ç†é–‹å§‹
                        break
                except Exception:
                    continue
            
            # å–å¾—ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’å³åº§ã«å‡¦ç†é–‹å§‹
            if quick_files:
                print(f"ğŸ”„ å³åº§å‡¦ç†é–‹å§‹: {len(quick_files)}ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å…ˆè¡Œã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸­...")
                for file_path in quick_files[:10]:  # æœ€åˆã®10ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å³åº§å‡¦ç†
                    try:
                        self.search_system.live_progressive_index_file(str(file_path))
                    except Exception as e:
                        print(f"âš ï¸ å…ˆè¡Œã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
                print(f"âœ… å…ˆè¡Œã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å®Œäº†: {min(len(quick_files), 10)}ãƒ•ã‚¡ã‚¤ãƒ«")
        
        # å³åº§å‡¦ç†ã‚’é–‹å§‹ï¼ˆ0.01ç§’å¾Œï¼‰ - threadingã‚¹ã‚³ãƒ¼ãƒ—å•é¡Œä¿®æ­£
        import threading as _threading
        timer = _threading.Timer(0.01, quick_start_indexing)
        timer.start()
        
        # ãƒ¡ã‚¤ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«åé›†ã‚’ä¸¦åˆ—åŒ–ï¼ˆé«˜é€Ÿé–‹å§‹ç‰ˆï¼‰
        print("ğŸ“‹ å…¨ãƒ•ã‚¡ã‚¤ãƒ«åé›†é–‹å§‹ï¼ˆä¸¦åˆ—å‡¦ç†ï¼‰...")
        collection_start = time.time()
        
        def collect_files_for_extension(ext: str) -> List[Path]:
            """å˜ä¸€æ‹¡å¼µå­ã®ãƒ•ã‚¡ã‚¤ãƒ«åé›†ï¼ˆé«˜é€Ÿç‰ˆï¼‰"""
            try:
                all_files = list(directory_path.rglob(f'*{ext}'))
                # macOSéš ã—ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã‚·ã‚¹ãƒ†ãƒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                filtered_files = []
                for file_path in all_files:
                    # ._ã§å§‹ã¾ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆmacOSéš ã—ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ã‚’ã‚¹ã‚­ãƒƒãƒ—
                    if file_path.name.startswith('._'):
                        continue
                    # ã‚·ã‚¹ãƒ†ãƒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—
                    if file_path.name in ['.DS_Store', 'Thumbs.db', 'desktop.ini']:
                        continue
                    # éš ã—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ã‚¹ã‚­ãƒƒãƒ—
                    if any(part.startswith('.') and part not in ['.', '..'] for part in file_path.parts):
                        continue
                    filtered_files.append(file_path)
                return filtered_files
            except Exception as e:
                print(f"âš ï¸ {ext}ãƒ•ã‚¡ã‚¤ãƒ«åé›†ã‚¨ãƒ©ãƒ¼: {e}")
                return []
        
        # ä¸¦åˆ—ãƒ•ã‚¡ã‚¤ãƒ«åé›†
        all_files: List[Path] = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=min(4, len(file_extensions))) as executor:
            futures = {executor.submit(collect_files_for_extension, ext): ext for ext in file_extensions}
            
            for future in concurrent.futures.as_completed(futures):
                ext = futures[future]
                try:
                    ext_files = future.result(timeout=30.0)  # 30ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                    all_files.extend(ext_files)
                    if ext_files:
                        print(f"  âœ… {ext}: {len(ext_files):,}ä»¶")
                except Exception as e:
                    print(f"  âŒ {ext}: åé›†ã‚¨ãƒ©ãƒ¼ - {e}")
        
        collection_time = time.time() - collection_start
        total_files = len(all_files)
        
        print(f"ğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ«åé›†å®Œäº†: {total_files:,}ä»¶ ({collection_time:.2f}ç§’)")
        
        if total_files == 0:
            print("âš ï¸ å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            self.indexing_in_progress = False
            return {
                "total_files": 0,
                "processed_files": 0,
                "success_count": 0,
                "processing_time": time.time() - start_time,
                "files_per_second": 0.0
            }
        
        processed_files = 0
        success_count = 0

        print(f"ğŸš€ ä¸¦åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‡¦ç†é–‹å§‹: {total_files:,}ãƒ•ã‚¡ã‚¤ãƒ« ({self.optimal_threads}ã‚¹ãƒ¬ãƒƒãƒ‰)")
        
        # 500ãƒ•ã‚¡ã‚¤ãƒ«/ç§’å¯¾å¿œ: å‹•çš„ãƒãƒƒãƒã‚µã‚¤ã‚ºèª¿æ•´
        original_batch_size = self.batch_size
        if total_files > 10000:
            self.batch_size = min(6000, max(self.batch_size, 5000))
            print(f"ğŸ“ˆ å¤§è¦æ¨¡å‡¦ç†ãƒ¢ãƒ¼ãƒ‰: ãƒãƒƒãƒ {original_batch_size} â†’ {self.batch_size}")
        elif total_files > 5000:
            self.batch_size = min(4000, max(self.batch_size, 3000))
            print(f"ğŸ“Š ä¸­è¦æ¨¡å‡¦ç†ãƒ¢ãƒ¼ãƒ‰: ãƒãƒƒãƒ {original_batch_size} â†’ {self.batch_size}")
        elif total_files > 1000:
            self.batch_size = min(2500, max(self.batch_size, 1500))
            print(f"ğŸ“‹ æ¨™æº–å‡¦ç†ãƒ¢ãƒ¼ãƒ‰: ãƒãƒƒãƒ {original_batch_size} â†’ {self.batch_size}")
        
        print(f"âš¡ è¨­å®š: ãƒãƒƒãƒ={self.batch_size}, ã‚­ãƒ£ãƒƒã‚·ãƒ¥={self.max_immediate_cache:,}/{self.max_hot_cache:,}")
        print("ğŸ’¡ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸­ã‚‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰æ¤œç´¢çµæœã‚’æä¾›ã—ã¾ã™")
        
        print(f"ğŸ”„ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‡¦ç†é–‹å§‹...")
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çŠ¶æ…‹ã‚’è¨­å®š
        self.indexing_in_progress = True
        self.indexing_results_ready = False

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–é–‹å§‹ï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰å¢—åŠ ä¿®æ­£ç‰ˆï¼‰
        def start_performance_monitoring():
            try:
                import psutil
                monitoring_count = 0
                last_adjustment_time = 0
                
                while self.indexing_in_progress:
                    time.sleep(2)  # 2ç§’ã”ã¨ã«ç›£è¦–ï¼ˆã‚ˆã‚Šé »ç¹ã«ï¼‰
                    monitoring_count += 1
                    current_time = time.time()
                    
                    cpu_usage = psutil.cpu_percent(interval=0.5)  # çŸ­æ™‚é–“ã§æ¸¬å®š
                    memory = psutil.virtual_memory()
                    available_gb = memory.available / (1024**3)
                    
                    current_threads = self.optimal_threads
                    physical_cores = psutil.cpu_count(logical=False) or 4
                    logical_cores = psutil.cpu_count(logical=True) or 8
                    
                    # èª¿æ•´é »åº¦åˆ¶é™ï¼ˆ3ç§’é–“éš”ï¼‰
                    if current_time - last_adjustment_time < 3.0:
                        continue
                    
                    print(f"ğŸ“Š ç›£è¦– #{monitoring_count}: CPU={cpu_usage:.1f}%, RAM={available_gb:.1f}GB, ã‚¹ãƒ¬ãƒƒãƒ‰={current_threads}")
                    
                    # 500ãƒ•ã‚¡ã‚¤ãƒ«/ç§’å¯¾å¿œ: å‹•çš„ãƒãƒƒãƒã‚µã‚¤ã‚ºèª¿æ•´ã‚‚å«ã‚ã‚‹
                    current_batch = self.batch_size
                    
                    # ã‚ˆã‚Šç©æ¥µçš„ãªã‚¹ãƒ¬ãƒƒãƒ‰å¢—åŠ ãƒ­ã‚¸ãƒƒã‚¯
                    if cpu_usage < 40 and available_gb > 3:
                        # å¤§å¹…ãªä½™è£• - ç©æ¥µçš„ã«å¢—åŠ 
                        max_threads = min(logical_cores - 1, 16)
                        if current_threads < max_threads:
                            increase = min(3, max_threads - current_threads)  # æœ€å¤§3ã‚¹ãƒ¬ãƒƒãƒ‰å¢—åŠ 
                            self.optimal_threads = current_threads + increase
                            
                            # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚‚å¢—åŠ 
                            if current_batch < 5000:
                                self.batch_size = min(6000, current_batch + 500)
                                print(f"ğŸ“ˆ ãƒãƒƒãƒã‚µã‚¤ã‚ºé€£å‹•å¢—åŠ : {current_batch} â†’ {self.batch_size}")
                            
                            self.stats["dynamic_adjustments"] += 1
                            last_adjustment_time = current_time
                            print(f"â¬†ï¸â¬†ï¸ å¤§å¹…ã‚¹ãƒ¬ãƒƒãƒ‰å¢—åŠ : {current_threads} â†’ {self.optimal_threads} (ä½™è£•å¤§)")
                            
                    elif cpu_usage < 55 and available_gb > 2:
                        # ä¸­ç¨‹åº¦ã®ä½™è£• - æ®µéšçš„ã«å¢—åŠ 
                        max_threads = min(physical_cores + 2, 12)
                        if current_threads < max_threads:
                            increase = min(2, max_threads - current_threads)  # æœ€å¤§2ã‚¹ãƒ¬ãƒƒãƒ‰å¢—åŠ 
                            self.optimal_threads = current_threads + increase
                            self.stats["dynamic_adjustments"] += 1
                            last_adjustment_time = current_time
                            print(f"â¬†ï¸ ã‚¹ãƒ¬ãƒƒãƒ‰å¢—åŠ : {current_threads} â†’ {self.optimal_threads} (ä½™è£•ä¸­)")
                            
                    elif cpu_usage < 70 and available_gb > 1.5:
                        # è»½å¾®ãªä½™è£• - 1ã‚¹ãƒ¬ãƒƒãƒ‰å¢—åŠ 
                        max_threads = min(physical_cores, 8)
                        if current_threads < max_threads:
                            self.optimal_threads = current_threads + 1
                            self.stats["dynamic_adjustments"] += 1
                            last_adjustment_time = current_time
                            print(f"â¬†ï¸ ã‚¹ãƒ¬ãƒƒãƒ‰å¾®å¢—: {current_threads} â†’ {self.optimal_threads} (ä½™è£•å°)")
                            
                    elif cpu_usage > 85 or available_gb < 1:
                        # é«˜è² è· - ã‚¹ãƒ¬ãƒƒãƒ‰å‰Šæ¸›
                        if current_threads > 2:
                            decrease = min(2, current_threads - 2)  # æœ€å¤§2ã‚¹ãƒ¬ãƒƒãƒ‰å‰Šæ¸›
                            self.optimal_threads = max(current_threads - decrease, 2)
                            
                            # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚‚å‰Šæ¸›
                            if current_batch > 1000:
                                self.batch_size = max(800, current_batch - 500)
                                print(f"ğŸ“‰ ãƒãƒƒãƒã‚µã‚¤ã‚ºé€£å‹•å‰Šæ¸›: {current_batch} â†’ {self.batch_size}")
                            
                            self.stats["dynamic_adjustments"] += 1
                            last_adjustment_time = current_time
                            print(f"â¬‡ï¸ ã‚¹ãƒ¬ãƒƒãƒ‰å‰Šæ¸›: {current_threads} â†’ {self.optimal_threads} (é«˜è² è·)")
                    
                    # çµ±è¨ˆæ›´æ–°
                    self.stats["peak_thread_count"] = max(self.stats["peak_thread_count"], self.optimal_threads)
                        
            except Exception as e:
                print(f"âš ï¸ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚¨ãƒ©ãƒ¼: {e}")
        
        # ç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹
        import threading
        monitor_thread = threading.Thread(target=start_performance_monitoring, daemon=True)
        monitor_thread.start()

        print(f"ğŸ“Š æœ€é©åŒ–ã•ã‚ŒãŸãƒãƒƒãƒã‚µã‚¤ã‚º: {self.batch_size} (ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {total_files:,})")
        
        try:
            import psutil
            max_possible_threads = min(psutil.cpu_count(logical=True) - 1, 16)
            print(f"ğŸ”„ å‹•çš„ã‚¹ãƒ¬ãƒƒãƒ‰èª¿æ•´: æœ‰åŠ¹ (åˆæœŸ: {self.optimal_threads}, æœ€å¤§: {max_possible_threads})")
        except:
            print(f"ğŸ”„ å‹•çš„ã‚¹ãƒ¬ãƒƒãƒ‰èª¿æ•´: æœ‰åŠ¹ (åˆæœŸ: {self.optimal_threads}, æœ€å¤§: 16)")

        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¨®é¡åˆ¥ã«åˆ†é›¢ï¼ˆ.tifãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ç”»åƒå‡¦ç†ï¼‰
            image_extensions = ['.tif', '.tiff']  # .tifãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿
            image_files = []
            text_files = []
            
            for file_path in all_files:
                if file_path.suffix.lower() in image_extensions:
                    image_files.append(file_path)
                else:
                    text_files.append(file_path)
            
            print(f"ğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ«åˆ†é¡:")
            print(f"  ãƒ†ã‚­ã‚¹ãƒˆç³»: {len(text_files):,}ãƒ•ã‚¡ã‚¤ãƒ« (é€šå¸¸å‡¦ç†)")
            print(f"  .tifç”»åƒç³»: {len(image_files):,}ãƒ•ã‚¡ã‚¤ãƒ« (OCRæœ€é©åŒ–å‡¦ç†)")
            
            # 1. ã¾ãšãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’é«˜é€Ÿå‡¦ç†
            if text_files:
                print(f"ğŸš€ ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«é«˜é€Ÿå‡¦ç†é–‹å§‹: {len(text_files):,}ãƒ•ã‚¡ã‚¤ãƒ«")
                success_count += self._process_text_files_batch(text_files, start_time)
                processed_files += len(text_files)
            
            # 2. .tifç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’CPUä½¿ç”¨ç‡ã‚’æŠ‘åˆ¶ã—ã¦å‡¦ç†
            if image_files:
                print(f"ğŸ” .tifç”»åƒãƒ•ã‚¡ã‚¤ãƒ«OCRå‡¦ç†é–‹å§‹: {len(image_files):,}ãƒ•ã‚¡ã‚¤ãƒ« (CPUæœ€é©åŒ–)")
                success_count += self._process_image_files_optimized(image_files, start_time, processed_files, total_files)
                processed_files += len(image_files)
            
            # å‹•çš„ã‚¹ãƒ¬ãƒƒãƒ‰èª¿æ•´å¯¾å¿œã®ãƒãƒƒãƒå‡¦ç†ï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼å‡¦ç† - ä¸Šè¨˜ã§å‡¦ç†ã•ã‚Œãªã„å ´åˆï¼‰
            current_batch_threads = self.optimal_threads
            
            for batch_start in range(0, total_files, self.batch_size):
                batch_end = min(batch_start + self.batch_size, total_files)
                batch_files = all_files[batch_start:batch_end]

                # ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ãŒå¤‰æ›´ã•ã‚ŒãŸå ´åˆã®ã¿æ–°ã—ã„Executorã‚’ä½œæˆ
                if self.optimal_threads != current_batch_threads:
                    current_batch_threads = self.optimal_threads
                    print(f"ğŸ”„ ãƒãƒƒãƒå‡¦ç†ã§ã‚¹ãƒ¬ãƒƒãƒ‰æ•°å¤‰æ›´: {current_batch_threads}ã‚¹ãƒ¬ãƒƒãƒ‰")

                print(f"ğŸ“¦ ãƒãƒƒãƒ {batch_start//self.batch_size + 1}: {len(batch_files)}ãƒ•ã‚¡ã‚¤ãƒ« ({current_batch_threads}ã‚¹ãƒ¬ãƒƒãƒ‰)")
                
                # ç¾åœ¨ã®ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã§Executorã‚’ä½œæˆ
                with concurrent.futures.ThreadPoolExecutor(max_workers=current_batch_threads) as executor:
                    # éåŒæœŸã§ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚’æŠ•å…¥
                    future_to_file = {
                        executor.submit(self.search_system.live_progressive_index_file, str(file_path)): file_path
                        for file_path in batch_files
                    }

                    # ãƒãƒƒãƒå†…å‡¦ç†å®Œäº†ã‚’å¾…æ©Ÿ
                    for future in concurrent.futures.as_completed(future_to_file):
                        file_path = future_to_file[future]
                        try:
                            success = future.result(timeout=30.0)  # 30ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                            if success:
                                success_count += 1
                            processed_files += 1

                            # é€²æ—è¡¨ç¤ºï¼ˆãƒãƒƒãƒã‚µã‚¤ã‚ºã«åˆã‚ã›ãŸé–“éš”ï¼‰
                            if processed_files % 400 == 0 or processed_files == total_files:
                                progress = (processed_files / total_files) * 100
                                elapsed_time = time.time() - start_time
                                files_per_sec = processed_files / elapsed_time if elapsed_time > 0 else 0
                                print(f"âš¡ é€²æ—: {processed_files:,}/{total_files:,} ({progress:.1f}%) - {files_per_sec:.1f} ãƒ•ã‚¡ã‚¤ãƒ«/ç§’")

                        except concurrent.futures.TimeoutError:
                            print(f"â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {file_path}")
                            processed_files += 1
                        except Exception as e:
                            print(f"âŒ å‡¦ç†ã‚¨ãƒ©ãƒ¼: {file_path} - {e}")
                            processed_files += 1

                # ãƒãƒƒãƒé–“ã§çŸ­ã„ä¼‘æ†©ï¼ˆCPUã¨ãƒ‡ã‚£ã‚¹ã‚¯I/Oè»½æ¸›ï¼‰
                if batch_end < total_files:
                    time.sleep(0.05)  # 50msã®çŸ­ã„ä¼‘æ†©

        finally:
            # 500ãƒ•ã‚¡ã‚¤ãƒ«/ç§’å¯¾å¿œ: ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å…ƒã«æˆ»ã™
            if 'original_batch_size' in locals():
                self.batch_size = original_batch_size
                print(f"ğŸ”„ ãƒãƒƒãƒã‚µã‚¤ã‚ºå¾©å…ƒ: {self.batch_size}")
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å®Œäº†çŠ¶æ…‹ã«è¨­å®š
            self.indexing_in_progress = False
            self.indexing_results_ready = True

        total_time = time.time() - start_time

        result = {
            "total_files": total_files,
            "processed_files": processed_files,
            "success_count": success_count,
            "total_time": total_time,
            "files_per_second": processed_files / total_time if total_time > 0 else 0
        }

        print(f"âœ… ä¸¦åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å®Œäº†: {success_count:,}/{total_files:,} ãƒ•ã‚¡ã‚¤ãƒ« ({total_time:.1f}ç§’)")
        print(f"ğŸ“ˆ å‡¦ç†é€Ÿåº¦: {result['files_per_second']:.1f} ãƒ•ã‚¡ã‚¤ãƒ«/ç§’")
        print("ğŸ”„ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®åæ˜ é–‹å§‹...")

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’éåŒæœŸã§ä¿å­˜
        try:
            threading.Thread(target=self.save_caches, daemon=True).start()
            print("ğŸ’¾ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜é–‹å§‹ï¼ˆãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ï¼‰")
        except Exception as e:
            print(f"âš ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

        return result

    def get_comprehensive_statistics(self) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„çµ±è¨ˆæƒ…å ±å–å¾—ï¼ˆä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±è¨ˆå‡¦ç†ç‰ˆãƒ»ä¿®æ­£ç‰ˆï¼‰"""
        try:
            debug_logger.debug("ä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±è¨ˆå–å¾—é–‹å§‹")
            
            def get_single_db_stats(db_index: int) -> Dict[str, Any]:
                """å˜ä¸€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®çµ±è¨ˆã‚’å–å¾—ï¼ˆå®‰å…¨æ€§å¼·åŒ–ç‰ˆï¼‰"""
                stats = {
                    'db_index': db_index,
                    'file_count': 0,
                    'file_type_stats': {},
                    'avg_size': 0,
                    'storage_size': 0,
                    'error': None
                }
                
                try:
                    complete_db_path = self.complete_db_paths[db_index]
                    debug_logger.debug(f"DB{db_index}çµ±è¨ˆå–å¾—é–‹å§‹: {complete_db_path}")
                    
                    if not os.path.exists(complete_db_path):
                        debug_logger.warning(f"DB{db_index}ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {complete_db_path}")
                        return stats
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒå°ã•ã™ãã‚‹å ´åˆï¼ˆç©ºã®å ´åˆï¼‰ã¯ã‚¹ã‚­ãƒƒãƒ—
                    file_size = os.path.getsize(complete_db_path)
                    if file_size < 1024:  # 1KBæœªæº€ã¯ç©ºã¨ã¿ãªã™
                        debug_logger.debug(f"DB{db_index}ã¯ç©ºã®ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ{file_size}bytesï¼‰")
                        return stats
                        
                    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±è¨ˆå–å¾—ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆçŸ­ç¸®ï¼‰
                    conn = sqlite3.connect(complete_db_path, timeout=5.0)
                    conn.execute('PRAGMA journal_mode=WAL')
                    cursor = conn.cursor()
                    
                    # ã¾ãšãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
                    cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='documents'")
                    if not cursor.fetchone():
                        debug_logger.warning(f"DB{db_index}ã«documentsãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
                        conn.close()
                        return stats
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚«ã‚¦ãƒ³ãƒˆ
                    cursor.execute("SELECT COUNT(*) FROM documents")
                    count_result = cursor.fetchone()
                    stats['file_count'] = count_result[0] if count_result else 0
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«æ•°ãŒ0ã®å ´åˆã¯ä»–ã®çµ±è¨ˆã‚’ã‚¹ã‚­ãƒƒãƒ—
                    if stats['file_count'] > 0:
                        # ãƒ•ã‚¡ã‚¤ãƒ«ç¨®é¡åˆ¥çµ±è¨ˆ
                        try:
                            cursor.execute("SELECT file_type, COUNT(*) FROM documents GROUP BY file_type")
                            for row in cursor.fetchall():
                                if row and len(row) >= 2:
                                    stats['file_type_stats'][row[0]] = row[1]
                        except Exception as e:
                            debug_logger.warning(f"DB{db_index}ãƒ•ã‚¡ã‚¤ãƒ«ç¨®é¡çµ±è¨ˆã‚¨ãƒ©ãƒ¼: {e}")
                        
                        # å¹³å‡ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºï¼ˆç°¡ç•¥ç‰ˆï¼‰
                        try:
                            cursor.execute("SELECT AVG(LENGTH(content)) FROM documents WHERE content IS NOT NULL LIMIT 100")
                            avg_result = cursor.fetchone()
                            stats['avg_size'] = avg_result[0] if avg_result and avg_result[0] else 0
                        except Exception as e:
                            debug_logger.warning(f"DB{db_index}å¹³å‡ã‚µã‚¤ã‚ºè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
                    
                    # ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚µã‚¤ã‚º
                    stats['storage_size'] = file_size
                    
                    conn.close()
                    debug_logger.debug(f"DB{db_index}çµ±è¨ˆå–å¾—å®Œäº†: {stats['file_count']}ãƒ•ã‚¡ã‚¤ãƒ«")
                    
                except sqlite3.OperationalError as e:
                    debug_logger.error(f"DB{db_index}SQLiteã‚¨ãƒ©ãƒ¼: {e}")
                    stats['error'] = f"SQLite error: {e}"
                except Exception as e:
                    debug_logger.error(f"DB{db_index}çµ±è¨ˆã‚¨ãƒ©ãƒ¼: {e}")
                    stats['error'] = str(e)
                finally:
                    # ç¢ºå®Ÿã«æ¥ç¶šã‚’é–‰ã˜ã‚‹
                    try:
                        if 'conn' in locals():
                            conn.close()
                    except:
                        pass
                
                return stats
            
            # ä¸¦åˆ—ã§å…¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®çµ±è¨ˆã‚’å–å¾—ï¼ˆä¸¦åˆ—æ•°åˆ¶é™ï¼‰
            all_db_stats = []
            max_workers = min(self.db_count, 4)  # ä¸¦åˆ—æ•°ã‚’4ã«åˆ¶é™ã—ã¦ãƒªã‚½ãƒ¼ã‚¹è² è·ã‚’è»½æ¸›
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_db = {executor.submit(get_single_db_stats, i): i for i in range(self.db_count)}
                
                for future in concurrent.futures.as_completed(future_to_db, timeout=30.0):
                    try:
                        db_stats = future.result(timeout=10.0)  # å€‹åˆ¥ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚‚çŸ­ç¸®
                        all_db_stats.append(db_stats)
                    except concurrent.futures.TimeoutError:
                        db_index = future_to_db[future]
                        debug_logger.error(f"DB{db_index}çµ±è¨ˆå–å¾—ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
                    except Exception as e:
                        debug_logger.error(f"ä¸¦åˆ—çµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            
            # çµ±è¨ˆé›†è¨ˆï¼ˆã‚¨ãƒ©ãƒ¼å‡¦ç†å¼·åŒ–ï¼‰
            valid_stats = [stats for stats in all_db_stats if stats['error'] is None]
            total_complete_count = sum(stats['file_count'] for stats in valid_stats)
            all_file_type_stats = {}
            total_storage_size = sum(stats['storage_size'] for stats in valid_stats)
            db_individual_stats = {}
            
            debug_logger.info(f"æœ‰åŠ¹DBçµ±è¨ˆ: {len(valid_stats)}/{len(all_db_stats)}å€‹")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ç¨®é¡çµ±è¨ˆã®ãƒãƒ¼ã‚¸
            for stats in valid_stats:
                for file_type, count in stats['file_type_stats'].items():
                    all_file_type_stats[file_type] = all_file_type_stats.get(file_type, 0) + count
                db_individual_stats[f'db_{stats["db_index"]}_files'] = stats['file_count']
            
            # å¹³å‡ã‚µã‚¤ã‚ºè¨ˆç®—ï¼ˆå®‰å…¨ç‰ˆï¼‰
            total_avg_size = 0
            if valid_stats:
                avg_sizes = [stats['avg_size'] for stats in valid_stats if stats['avg_size'] > 0]
                total_avg_size = sum(avg_sizes) / len(avg_sizes) if avg_sizes else 0
            
            avg_file_size = total_avg_size / total_complete_count if total_complete_count > 0 else 0

            debug_logger.info(f"çµ±è¨ˆé›†è¨ˆå®Œäº†: total_files={total_complete_count}, valid_dbs={len(valid_stats)}/{self.db_count}")
            debug_logger.debug(f"å€‹åˆ¥DBçµ±è¨ˆ: {db_individual_stats}")

            # çµ±è¨ˆæƒ…å ±ã‚’çµ±åˆã—ã¦è¿”å´
            result = {
                "total_files": total_complete_count,  # æ—§å½¢å¼ã¨ã®äº’æ›æ€§ã®ãŸã‚è¿½åŠ 
                "total_records": total_complete_count,  # æ—§å½¢å¼ã¨ã®äº’æ›æ€§ã®ãŸã‚è¿½åŠ 
                "db_count": self.db_count,  # æ—§å½¢å¼ã¨ã®äº’æ›æ€§ã®ãŸã‚è¿½åŠ 
                "layer_statistics": {
                    "immediate_layer": len(self.immediate_cache),
                    "hot_layer": len(self.hot_cache),
                    "complete_layer": total_complete_count,
                    "actual_unique_files": total_complete_count,  # å®Ÿéš›ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«æ•°
                    "database_count": self.db_count,
                    "valid_databases": len(valid_stats)  # æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ•°
                },
                "search_statistics": self.stats,
                "file_type_distribution": all_file_type_stats,
                "storage_statistics": {
                    "average_file_size": avg_file_size,
                    "total_storage_size": total_storage_size
                },
                "performance_metrics": {
                    "average_search_time": self.stats["avg_search_time"],
                    "cache_hit_rate": self._calculate_cache_hit_rate()
                }
            }
            
            # å€‹åˆ¥ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±è¨ˆã‚’è¿½åŠ 
            result.update(db_individual_stats)
            
            debug_logger.info(f"çµ±è¨ˆæƒ…å ±è¿”å´: total_files={result['total_files']}, complete_layer={result['layer_statistics']['complete_layer']}")
            
            return result

        except Exception as e:
            debug_logger.error(f"çµ±è¨ˆæƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            print(f"âš ï¸ çµ±è¨ˆæƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚åŸºæœ¬çš„ãªçµ±è¨ˆæƒ…å ±ã‚’è¿”ã™
            return {
                "total_files": 0,
                "total_records": 0,
                "db_count": self.db_count,
                "layer_statistics": {
                    "immediate_layer": len(self.immediate_cache),
                    "hot_layer": len(self.hot_cache),
                    "complete_layer": 0,
                    "actual_unique_files": 0,
                    "database_count": self.db_count,
                    "valid_databases": 0
                },
                "search_statistics": self.stats,
                "file_type_distribution": {},
                "storage_statistics": {
                    "average_file_size": 0,
                    "total_storage_size": 0
                },
                "performance_metrics": {
                    "average_search_time": self.stats.get("avg_search_time", 0),
                    "cache_hit_rate": 0
                },
                "error": str(e)
            }

    def diagnose_database_status(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çŠ¶æ…‹è¨ºæ–­ï¼ˆæ¤œç´¢å•é¡Œã®ãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰"""
        print("\nğŸ” ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çŠ¶æ…‹è¨ºæ–­é–‹å§‹...")
        diagnosis = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "databases": {},
            "summary": {
                "total_files": 0,
                "total_fts_entries": 0,
                "healthy_dbs": 0,
                "problematic_dbs": 0
            }
        }

        for i in range(self.db_count):
            db_path = self.complete_db_paths[i]
            db_name = f"database_{i}"
            db_diagnosis = {
                "path": str(db_path),
                "exists": False,
                "accessible": False,
                "main_table_count": 0,
                "fts_table_count": 0,
                "table_structure": {},
                "sample_data": [],
                "issues": []
            }

            try:
                # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯
                if db_path.exists():
                    db_diagnosis["exists"] = True
                    db_diagnosis["file_size"] = db_path.stat().st_size
                else:
                    db_diagnosis["issues"].append("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
                    diagnosis["databases"][db_name] = db_diagnosis
                    diagnosis["summary"]["problematic_dbs"] += 1
                    continue

                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãƒ†ã‚¹ãƒˆ
                conn = sqlite3.connect(str(db_path), timeout=10.0)
                db_diagnosis["accessible"] = True
                cursor = conn.cursor()

                # ãƒ¡ã‚¤ãƒ³ãƒ†ãƒ¼ãƒ–ãƒ«ä»¶æ•°
                cursor.execute("SELECT COUNT(*) FROM documents")
                main_count = cursor.fetchone()[0]
                db_diagnosis["main_table_count"] = main_count
                diagnosis["summary"]["total_files"] += main_count

                # FTSãƒ†ãƒ¼ãƒ–ãƒ«ä»¶æ•°
                try:
                    cursor.execute("SELECT COUNT(*) FROM documents_fts")
                    fts_count = cursor.fetchone()[0]
                    db_diagnosis["fts_table_count"] = fts_count
                    diagnosis["summary"]["total_fts_entries"] += fts_count
                except sqlite3.OperationalError as fts_error:
                    db_diagnosis["issues"].append(f"FTSãƒ†ãƒ¼ãƒ–ãƒ«ã‚¨ãƒ©ãƒ¼: {fts_error}")

                # ãƒ‡ãƒ¼ã‚¿ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
                if main_count != fts_count:
                    db_diagnosis["issues"].append(f"ãƒ‡ãƒ¼ã‚¿ä¸æ•´åˆ: main={main_count}, fts={fts_count}")

                # ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ ç¢ºèª
                cursor.execute("SELECT name, sql FROM sqlite_master WHERE type='table'")
                tables = cursor.fetchall()
                db_diagnosis["table_structure"] = {name: sql for name, sql in tables}

                # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿å–å¾—
                cursor.execute("SELECT file_path, file_name FROM documents LIMIT 3")
                samples = cursor.fetchall()
                db_diagnosis["sample_data"] = [{"path": path, "name": name} for path, name in samples]

                # ç°¡å˜ãªæ¤œç´¢ãƒ†ã‚¹ãƒˆ
                cursor.execute("SELECT COUNT(*) FROM documents_fts WHERE documents_fts MATCH 'test'")
                test_result = cursor.fetchone()[0]
                db_diagnosis["search_test_result"] = test_result

                conn.close()

                # å¥å…¨æ€§åˆ¤å®š
                if not db_diagnosis["issues"]:
                    diagnosis["summary"]["healthy_dbs"] += 1
                else:
                    diagnosis["summary"]["problematic_dbs"] += 1

            except Exception as e:
                db_diagnosis["issues"].append(f"è¨ºæ–­ã‚¨ãƒ©ãƒ¼: {e}")
                diagnosis["summary"]["problematic_dbs"] += 1

            diagnosis["databases"][db_name] = db_diagnosis

        # è¨ºæ–­çµæœã®è¡¨ç¤º
        print(f"ğŸ“Š è¨ºæ–­çµæœã‚µãƒãƒªãƒ¼:")
        print(f"  ğŸ“ ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {diagnosis['summary']['total_files']:,}")
        print(f"  ğŸ” FTSã‚¨ãƒ³ãƒˆãƒªæ•°: {diagnosis['summary']['total_fts_entries']:,}")
        print(f"  âœ… æ­£å¸¸ãªDB: {diagnosis['summary']['healthy_dbs']}")
        print(f"  âŒ å•é¡Œã®ã‚ã‚‹DB: {diagnosis['summary']['problematic_dbs']}")

        if diagnosis['summary']['problematic_dbs'] > 0:
            print(f"\nâš ï¸ å•é¡Œã®ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹:")
            for db_name, db_info in diagnosis["databases"].items():
                if db_info["issues"]:
                    print(f"  {db_name}: {', '.join(db_info['issues'])}")

        return diagnosis

    def shutdown(self):
        """ã‚·ã‚¹ãƒ†ãƒ ã®é©åˆ‡ãªã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³å‡¦ç†"""
        try:
            print("ğŸ”„ ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³é–‹å§‹...")
            debug_logger.info("ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³é–‹å§‹")
            
            # ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ãƒ•ãƒ©ã‚°ã‚’è¨­å®š
            self.shutdown_requested = True
            
            # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªExecutorã‚’åœæ­¢
            for executor in self._active_executors:
                try:
                    executor.shutdown(wait=False)
                except Exception as e:
                    debug_logger.warning(f"Executor shutdown error: {e}")
            self._active_executors.clear()
            
            # æœ€çµ‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ï¼ˆåŒæœŸå‡¦ç†ã§ç¢ºå®Ÿã«å®Ÿè¡Œï¼‰
            try:
                print("ğŸ’¾ æœ€çµ‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ä¸­...")
                self._save_caches_sync()
                print("âœ… æœ€çµ‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜å®Œäº†")
            except Exception as e:
                debug_logger.error(f"æœ€çµ‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            
            # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¹ãƒ¬ãƒƒãƒ‰ã®çµ‚äº†ã‚’å¾…æ©Ÿï¼ˆæœ€å¤§3ç§’ï¼‰
            for thread in self._background_threads:
                if thread.is_alive():
                    thread.join(timeout=3.0)
            
            print("âœ… ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³å®Œäº†")
            debug_logger.info("ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³å®Œäº†")
            
        except Exception as e:
            print(f"âš ï¸ ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ã‚¨ãƒ©ãƒ¼: {e}")
            debug_logger.error(f"ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ã‚¨ãƒ©ãƒ¼: {e}")

    def _save_caches_sync(self):
        """åŒæœŸçš„ãªã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ï¼ˆã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³æ™‚å°‚ç”¨ï¼‰"""
        try:
            cache_dir = self.project_root / "cache"
            cache_dir.mkdir(exist_ok=True)
            
            # é«˜é€Ÿå±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ã¿ä¿å­˜ï¼ˆå³åº§å±¤ã¯æ®ç™ºæ€§ï¼‰
            if self.hot_cache:
                cache_file = cache_dir / "hot_cache.json"
                with open(cache_file, 'w', encoding='utf-8') as f:
                    json.dump(dict(self.hot_cache), f, ensure_ascii=False, indent=2)
                    
        except Exception as e:
            debug_logger.error(f"åŒæœŸã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def save_caches(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ°¸ç¶šåŒ–ï¼ˆä¸¦åˆ—å‡¦ç†ç‰ˆï¼‰- å³åº§å±¤ã¯é™¤å¤–"""
        try:
            # ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ä¸­ã¯å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—
            if self.shutdown_requested:
                return
                
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã®cacheãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½¿ç”¨
            cache_dir = self.project_root / "cache"
            cache_dir.mkdir(exist_ok=True)
            
            # ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ãªã‚³ãƒ”ãƒ¼ã‚’ä½œæˆï¼ˆä¾‹å¤–å‡¦ç†å¼·åŒ–ï¼‰
            try:
                # å³åº§å±¤ã¯ä¿å­˜ã—ãªã„ï¼ˆæ®ç™ºæ€§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
                hot_cache_copy = dict(self.hot_cache)
            except RuntimeError as re:
                # dictionary changed size during iteration ã‚¨ãƒ©ãƒ¼å¯¾ç­–
                debug_logger.warning(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚³ãƒ”ãƒ¼ä¸­ã«ã‚µã‚¤ã‚ºå¤‰æ›´: {re}")
                return  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ä¿å­˜ã‚’ã‚¹ã‚­ãƒƒãƒ—
            
            # ä¸¦åˆ—å‡¦ç†ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
            def save_cache_file(cache_data, filename):
                """ä¸¦åˆ—å‡¦ç†ç”¨ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜"""
                try:
                    cache_file = cache_dir / filename
                    with open(cache_file, 'w', encoding='utf-8') as f:
                        json.dump(cache_data, f, ensure_ascii=False, indent=2)
                    return len(cache_data), filename
                except Exception as e:
                    debug_logger.error(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼ {filename}: {e}")
                    return 0, filename
            
            # Hotå±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¸¦åˆ—ä¿å­˜
            with ThreadPoolExecutor(max_workers=4) as executor:
                # Executorã‚’è¿½è·¡ãƒªã‚¹ãƒˆã«è¿½åŠ 
                if not self.shutdown_requested:
                    self._active_executors.append(executor)
                
                future_tasks = [
                    executor.submit(save_cache_file, hot_cache_copy, "hot_cache.json")
                ]
                
                # çµæœã®åé›†
                total_saved = 0
                for future in as_completed(future_tasks):
                    try:
                        count, filename = future.result(timeout=5.0)
                        total_saved += count
                    except Exception as e:
                        debug_logger.error(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¿ã‚¹ã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
            
            debug_logger.info(f"ä¸¦åˆ—ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜å®Œäº†: hot={total_saved} (å³åº§å±¤ã¯æ®ç™ºæ€§ã®ãŸã‚ä¿å­˜ãªã—)")
            
        except Exception as e:
            debug_logger.error(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã®å‡ºåŠ›é »åº¦ã‚’åˆ¶é™
            if not hasattr(self, '_last_error_log_time'):
                self._last_error_log_time = 0
            
            current_time = time.time()
            if current_time - self._last_error_log_time > 10.0:  # 10ç§’é–“éš”ã«åˆ¶é™
                self._last_error_log_time = current_time
                print(f"âš ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def load_caches(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¾©å…ƒï¼ˆä¸¦åˆ—å‡¦ç†ç‰ˆï¼‰- å³åº§å±¤ã¯èµ·å‹•æ™‚ã«ç©ºã§é–‹å§‹"""
        try:
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã®cacheãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½¿ç”¨
            cache_dir = self.project_root / "cache"
            
            # å³åº§å±¤ã¯å¸¸ã«ç©ºã§é–‹å§‹ï¼ˆæ®ç™ºæ€§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
            self.immediate_cache = {}
            
            # ä¸¦åˆ—å‡¦ç†ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
            def load_cache_file(filename):
                """ä¸¦åˆ—å‡¦ç†ç”¨ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
                try:
                    cache_file = cache_dir / filename
                    if cache_file.exists():
                        with open(cache_file, 'r', encoding='utf-8') as f:
                            return json.load(f), filename
                    return {}, filename
                except Exception as e:
                    debug_logger.error(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {filename}: {e}")
                    return {}, filename
            
            # Hotå±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ä¸¦åˆ—èª­ã¿è¾¼ã¿
            with ThreadPoolExecutor(max_workers=4) as executor:
                future_tasks = [
                    executor.submit(load_cache_file, "hot_cache.json")
                ]
                
                # çµæœã®åé›†
                loaded_hot_cache = {}
                for future in as_completed(future_tasks):
                    try:
                        cache_data, filename = future.result(timeout=5.0)
                        if filename == "hot_cache.json":
                            loaded_hot_cache = cache_data
                    except Exception as e:
                        debug_logger.error(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿ã‚¿ã‚¹ã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
            
            # å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¨ãƒ³ãƒˆãƒªã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆ7æ—¥ä»¥ä¸Šå¤ã„ï¼‰
            if loaded_hot_cache:
                def cleanup_cache_entry(items):
                    """ä¸¦åˆ—å‡¦ç†ç”¨ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
                    current_time = time.time()
                    cache_expiry_seconds = 7 * 24 * 3600  # 7æ—¥é–“
                    cleaned_items = {}
                    expired_count = 0
                    
                    for file_path, cache_data in items:
                        cache_time = cache_data.get('moved_from_immediate', 
                                                   cache_data.get('indexed_time', 0))
                        
                        if current_time - cache_time < cache_expiry_seconds:
                            cleaned_items[file_path] = cache_data
                        else:
                            expired_count += 1
                    
                    return cleaned_items, expired_count
                
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¢ã‚¤ãƒ†ãƒ ã‚’åˆ†å‰²ã—ã¦ä¸¦åˆ—å‡¦ç†
                items = list(loaded_hot_cache.items())
                chunk_size = max(1, len(items) // 4)
                chunks = [items[i:i + chunk_size] for i in range(0, len(items), chunk_size)]
                
                with ThreadPoolExecutor(max_workers=min(len(chunks), 4)) as executor:
                    futures = [executor.submit(cleanup_cache_entry, chunk) for chunk in chunks]
                    
                    cleaned_cache = {}
                    total_expired = 0
                    
                    for future in as_completed(futures):
                        try:
                            chunk_cleaned, chunk_expired = future.result(timeout=5.0)
                            cleaned_cache.update(chunk_cleaned)
                            total_expired += chunk_expired
                        except Exception as e:
                            debug_logger.error(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
                
                self.hot_cache = cleaned_cache
                
                if total_expired > 0:
                    debug_logger.info(f"é«˜é€Ÿå±¤æœŸé™åˆ‡ã‚Œã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—: {total_expired}ä»¶å‰Šé™¤")
                    print(f"ğŸ§¹ é«˜é€Ÿå±¤ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—: {total_expired}ä»¶ã®å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤")
            else:
                self.hot_cache = {}
            
            # å¤ã„å³åº§å±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°å‰Šé™¤
            immediate_cache_file = cache_dir / "immediate_cache.json"
            if immediate_cache_file.exists():
                immediate_cache_file.unlink()
                debug_logger.info("å¤ã„å³åº§å±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤")
            
            debug_logger.info(f"ä¸¦åˆ—ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¾©å…ƒå®Œäº†: immediate=0 (æ–°è¦), hot={len(self.hot_cache)}")
            print(f"ğŸ’¾ ä¸¦åˆ—ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¾©å…ƒå®Œäº†: immediate=0 (æ–°è¦), hot={len(self.hot_cache)}")
            
        except Exception as e:
            debug_logger.error(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¾©å…ƒã‚¨ãƒ©ãƒ¼: {e}")
            print(f"âš ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¾©å…ƒã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç©ºã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã§é–‹å§‹
            self.immediate_cache = {}
            self.hot_cache = {}

    def get_optimization_statistics(self) -> Dict[str, Any]:
        """æœ€é©åŒ–çµ±è¨ˆæƒ…å ±å–å¾—ï¼ˆ8ä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å¯¾å¿œï¼‰"""
        try:
            total_db_size_bytes = 0
            total_fts_count = 0
            all_index_stats = {}
            db_statistics = []
            
            # 8å€‹ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰çµ±è¨ˆã‚’é›†è¨ˆ
            for i in range(self.db_count):
                try:
                    complete_db_path = self.complete_db_paths[i]
                    conn = sqlite3.connect(complete_db_path, timeout=10.0)
                    cursor = conn.cursor()

                    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚µã‚¤ã‚º
                    cursor.execute("PRAGMA page_count")
                    page_count = cursor.fetchone()[0]
                    cursor.execute("PRAGMA page_size")
                    page_size = cursor.fetchone()[0]
                    db_size_bytes = page_count * page_size
                    total_db_size_bytes += db_size_bytes

                    # FTS5çµ±è¨ˆ
                    cursor.execute("SELECT COUNT(*) FROM documents_fts")
                    fts_count = cursor.fetchone()[0]
                    total_fts_count += fts_count

                    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çµ±è¨ˆ
                    cursor.execute("""
                        SELECT name, COUNT(*) as count
                        FROM sqlite_master 
                        WHERE type='index' 
                        GROUP BY name
                    """)
                    db_index_stats = dict(cursor.fetchall())
                    
                    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çµ±è¨ˆã‚’ãƒãƒ¼ã‚¸
                    for index_name, count in db_index_stats.items():
                        all_index_stats[f"DB{i}_{index_name}"] = count

                    # å€‹åˆ¥DBçµ±è¨ˆã‚’è¨˜éŒ²
                    db_statistics.append({
                        "db_index": i,
                        "size_mb": round(db_size_bytes / (1024 * 1024), 2),
                        "fts_documents": fts_count,
                        "page_count": page_count
                    })

                    conn.close()
                    
                except Exception as e:
                    print(f"âš ï¸ DB{i}æœ€é©åŒ–çµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                    continue

            # æœ€é©åŒ–å±¥æ­´
            optimization_history = getattr(self, 'optimization_history', [])

            return {
                "database_size": {
                    "total_bytes": total_db_size_bytes,
                    "total_mb": round(total_db_size_bytes / (1024 * 1024), 2),
                    "database_count": self.db_count,
                    "individual_databases": db_statistics
                },
                "fts_statistics": {
                    "total_indexed_documents": total_fts_count,
                    "tokenizer": "trigram",
                    "optimization_level": "high",
                    "parallel_databases": self.db_count
                },
                "index_statistics": all_index_stats,
                "optimization_history": optimization_history,
                "performance_metrics": {
                    "avg_search_time": self.stats.get("avg_search_time", 0),
                    "total_searches": self.stats.get("search_count", 0),
                    "cache_hit_rate": self._calculate_cache_hit_rate(),
                    "peak_thread_count": self.stats.get("peak_thread_count", self.optimal_threads)
                }
            }

        except Exception as e:
            debug_logger.error(f"æœ€é©åŒ–çµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            print(f"âš ï¸ æœ€é©åŒ–çµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}

    def _calculate_cache_hit_rate(self) -> float:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡è¨ˆç®—"""
        total_searches = self.stats.get("search_count", 0)
        if total_searches == 0:
            return 0.0

        immediate_hits = self.stats.get("immediate_layer_hits", 0)
        hot_hits = self.stats.get("hot_layer_hits", 0)
        total_hits = immediate_hits + hot_hits

        return round((total_hits / total_searches) * 100, 2)

    def check_auto_optimization(self):
        """è‡ªå‹•æœ€é©åŒ–ãƒã‚§ãƒƒã‚¯ï¼ˆ8ä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å¯¾å¿œï¼‰"""
        try:
            search_count = self.stats.get("search_count", 0)
            last_optimization = getattr(self, 'last_optimization_count', 0)

            # 1000å›æ¤œç´¢ã”ã¨ã«è‡ªå‹•æœ€é©åŒ–ã‚’ææ¡ˆ
            if search_count > 0 and (search_count - last_optimization) >= 1000:
                print(f"ğŸ’¡ è‡ªå‹•æœ€é©åŒ–ææ¡ˆ: {search_count}å›æ¤œç´¢å®Œäº†")
                self.last_optimization_count = search_count
                self.suggest_optimization()

        except Exception as e:
            debug_logger.error(f"è‡ªå‹•æœ€é©åŒ–ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")

    def suggest_optimization(self):
        """æœ€é©åŒ–ææ¡ˆï¼ˆ8ä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å¯¾å¿œï¼‰"""
        try:
            import threading

            def show_optimization_suggestion():
                try:
                    import tkinter.messagebox as mb
                    result = mb.askyesno(
                        "8ä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–ææ¡ˆ", 
                        f"1000å›ã®æ¤œç´¢ãŒå®Ÿè¡Œã•ã‚Œã¾ã—ãŸã€‚\n"
                        f"8å€‹ã®ä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æœ€é©åŒ–ã—ã¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å‘ä¸Šã•ã›ã¾ã™ã‹ï¼Ÿ\n\n"
                        f"æœ€é©åŒ–ã«ã‚ˆã‚Šæ¤œç´¢é€Ÿåº¦ãŒå‘ä¸Šã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n"
                        f"å‡¦ç†æ™‚é–“: ç´„30ç§’-2åˆ†"
                    )
                    
                    if result:
                        self.optimize_database_background()
                        
                except ImportError:
                    print("ğŸ’¡ æœ€é©åŒ–ææ¡ˆ: GUIç’°å¢ƒã§ãªã„ãŸã‚è‡ªå‹•æœ€é©åŒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—")
                except Exception as e:
                    print(f"âš ï¸ æœ€é©åŒ–ææ¡ˆã‚¨ãƒ©ãƒ¼: {e}")

            # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ææ¡ˆè¡¨ç¤º
            threading.Thread(target=show_optimization_suggestion, daemon=True).start()

        except Exception as e:
            debug_logger.error(f"æœ€é©åŒ–ææ¡ˆã‚¨ãƒ©ãƒ¼: {e}")

    def optimize_database_background(self):
        """ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰æœ€é©åŒ–ï¼ˆ8ä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å¯¾å¿œï¼‰"""
        try:
            import threading

            def optimize_all_databases():
                print("ğŸ”§ 8ä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–é–‹å§‹...")
                start_time = time.time()
                
                with concurrent.futures.ThreadPoolExecutor(max_workers=min(self.db_count, 4)) as executor:
                    future_to_db = {
                        executor.submit(self._optimize_single_database, i): i 
                        for i in range(self.db_count)
                    }
                    
                    completed = 0
                    for future in concurrent.futures.as_completed(future_to_db):
                        db_index = future_to_db[future]
                        try:
                            future.result(timeout=120)  # 2åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                            completed += 1
                            print(f"âœ… DB{db_index}æœ€é©åŒ–å®Œäº† ({completed}/{self.db_count})")
                        except Exception as e:
                            print(f"âš ï¸ DB{db_index}æœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")

                optimization_time = time.time() - start_time
                self.stats["optimization_count"] += 1
                self.stats["total_optimization_time"] += optimization_time
                
                # æœ€é©åŒ–å±¥æ­´ã«è¨˜éŒ²
                if not hasattr(self, 'optimization_history'):
                    self.optimization_history = []
                
                self.optimization_history.append({
                    "timestamp": time.time(),
                    "duration": optimization_time,
                    "databases_optimized": completed,
                    "total_databases": self.db_count
                })
                
                print(f"âœ… 8ä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–å®Œäº†: {optimization_time:.1f}ç§’")

            # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§æœ€é©åŒ–å®Ÿè¡Œ
            threading.Thread(target=optimize_all_databases, daemon=True).start()

        except Exception as e:
            debug_logger.error(f"ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰æœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            print(f"âŒ ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰æœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")

    def _optimize_single_database(self, db_index: int):
        """å˜ä¸€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æœ€é©åŒ–"""
        try:
            complete_db_path = self.complete_db_paths[db_index]
            conn = sqlite3.connect(complete_db_path, timeout=60.0)
            cursor = conn.cursor()

            # FTS5æœ€é©åŒ–
            cursor.execute("INSERT INTO documents_fts(documents_fts) VALUES('optimize')")
            
            # SQLiteæœ€é©åŒ–
            cursor.execute("VACUUM")
            cursor.execute("ANALYZE")
            
            # ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«ãƒ¢ãƒ¼ãƒ‰æœ€é©åŒ–
            cursor.execute("PRAGMA optimize")
            
            conn.commit()
            conn.close()

        except Exception as e:
            raise Exception(f"DB{db_index}æœ€é©åŒ–å¤±æ•—: {e}")


# GUIéƒ¨åˆ†ã¯çœç•¥
class UltraFastCompliantUI:
    """100%ä»•æ§˜é©åˆ è¶…é«˜é€Ÿå…¨æ–‡æ¤œç´¢UI"""

    def __init__(self, search_system: UltraFastFullCompliantSearchSystem):
        self.search_system = search_system
        self.root = tk.Tk()
        self.root.title("100%ä»•æ§˜é©åˆ è¶…é«˜é€Ÿãƒ©ã‚¤ãƒ–å…¨æ–‡æ¤œç´¢ã‚¢ãƒ—ãƒª")
        self.root.geometry("1200x800")  # ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«æ¤œç´¢ç”¨
        self.search_var = tk.StringVar()
        self.search_var.trace('w', self.on_search_change)
        self.last_search_time: float = 0.0
        self.search_delay = 0.3  # 300msé…å»¶ï¼ˆé«˜é€Ÿå¿œç­”ï¼‰
        self.min_search_length = 2  # æœ€å°æ¤œç´¢æ–‡å­—æ•°ï¼ˆè² è·è»½æ¸›ï¼‰
        
        # çµ±è¨ˆæ›´æ–°åˆ¶é™ç”¨
        self._last_stats_update_time = 0.0
        self._stats_update_interval = 2.0  # 2ç§’é–“éš”ã«åˆ¶é™
        self._pending_stats_update = False
        
        # ãƒ•ã‚©ãƒ«ãƒ€ã‚ªãƒ¼ãƒ—ãƒ³ç®¡ç†ç”¨ï¼ˆå®Œå…¨é‡è¤‡é˜²æ­¢ç‰ˆï¼‰
        self._opening_folder: bool = False
        self._double_click_processing: bool = False  # ãƒ€ãƒ–ãƒ«ã‚¯ãƒªãƒƒã‚¯å‡¦ç†ãƒ•ãƒ©ã‚°
        self._global_folder_requests = []  # ã‚°ãƒ­ãƒ¼ãƒãƒ«è¦æ±‚å±¥æ­´
        self._explorer_processes = set()  # Explorer ãƒ—ãƒ­ã‚»ã‚¹è¨˜éŒ²

        # å¤§å®¹é‡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç”¨å¤‰æ•°
        self.drive_info = {}
        self.bulk_indexing_active = False
        self.selected_folder_path = None

        # é€²æ—ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°
        self.progress_tracker = ProgressTracker()
        self.progress_window = None

        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‡¦ç†ã‚­ãƒ£ãƒ³ã‚»ãƒ«æ©Ÿèƒ½
        self.indexing_cancelled = False
        self.current_indexing_thread = None

        # çµ±è¨ˆæ›´æ–°ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®š
        self.search_system._stats_update_callback = self.update_statistics
        
        # ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³å‡¦ç†ã®è¨­å®š
        self.root.protocol("WM_DELETE_WINDOW", self.on_closing)

        # å¢—åˆ†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ©Ÿèƒ½ã®é–‹å§‹
        if hasattr(self.search_system, 'start_incremental_scanning'):
            self.search_system.start_incremental_scanning()

        self.setup_ui()
        
        # åˆå›ãƒ‰ãƒ©ã‚¤ãƒ–æ¤œå‡º
        self.root.after(1000, self.refresh_drives)

    def setup_ui(self):
        """UIæ§‹ç¯‰"""
        # ãƒ¡ã‚¤ãƒ³ãƒ•ãƒ¬ãƒ¼ãƒ 
        main_frame = ttk.Frame(self.root)
        main_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)

        # å¤§å®¹é‡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç”¨ãƒ•ãƒ¬ãƒ¼ãƒ 
        bulk_frame = ttk.LabelFrame(main_frame, text="ğŸ’¾ å¤§å®¹é‡ãƒ‰ãƒ©ã‚¤ãƒ–ãƒ»ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹", padding=10)
        bulk_frame.pack(fill=tk.X, pady=(0, 10))
        
        # å¯¾è±¡é¸æŠè¡Œ
        target_row = ttk.Frame(bulk_frame)
        target_row.pack(fill=tk.X, pady=(0, 5))
        
        # å¯¾è±¡é¸æŠãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³
        ttk.Label(target_row, text="å¯¾è±¡:").pack(side=tk.LEFT, padx=(0, 5))
        self.target_type_var = tk.StringVar(value="drive")
        drive_radio = ttk.Radiobutton(target_row, text="ãƒ‰ãƒ©ã‚¤ãƒ–å…¨ä½“", variable=self.target_type_var, 
                                     value="drive", command=self.on_target_type_changed)
        drive_radio.pack(side=tk.LEFT, padx=(0, 10))
        folder_radio = ttk.Radiobutton(target_row, text="ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼æŒ‡å®š", variable=self.target_type_var, 
                                      value="folder", command=self.on_target_type_changed)
        folder_radio.pack(side=tk.LEFT, padx=(0, 20))
        
        # ãƒ†ã‚¹ãƒˆç”¨ï¼šãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ã®å‹•ä½œç¢ºèª
        print(f"ğŸ”§ ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³è¨­å®šå®Œäº†: drive={drive_radio}, folder={folder_radio}")
        print(f"ğŸ”§ åˆæœŸå€¤: {self.target_type_var.get()}")
        
        # ãƒ‰ãƒ©ã‚¤ãƒ–é¸æŠè¡Œ
        drive_row = ttk.Frame(bulk_frame)
        drive_row.pack(fill=tk.X, pady=(0, 5))
        
        ttk.Label(drive_row, text="ãƒ‰ãƒ©ã‚¤ãƒ–:").pack(side=tk.LEFT, padx=(0, 5))
        self.drive_var = tk.StringVar()
        self.drive_combo = ttk.Combobox(drive_row, textvariable=self.drive_var, width=15, state="readonly")
        self.drive_combo.pack(side=tk.LEFT, padx=(0, 10))
        self.drive_combo.bind('<<ComboboxSelected>>', self.on_drive_selected)
        
        # ãƒ‰ãƒ©ã‚¤ãƒ–æƒ…å ±æ›´æ–°ãƒœã‚¿ãƒ³
        self.refresh_drives_btn = ttk.Button(drive_row, text="ğŸ” ãƒ‰ãƒ©ã‚¤ãƒ–æ¤œå‡º", command=self.refresh_drives, width=12)
        self.refresh_drives_btn.pack(side=tk.LEFT, padx=(0, 10))
        
        # ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼é¸æŠè¡Œ
        folder_row = ttk.Frame(bulk_frame)
        folder_row.pack(fill=tk.X, pady=(0, 5))
        
        ttk.Label(folder_row, text="ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼:").pack(side=tk.LEFT, padx=(0, 5))
        self.folder_var = tk.StringVar(value="ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ã‚’é¸æŠã—ã¦ãã ã•ã„")
        folder_label = ttk.Label(folder_row, textvariable=self.folder_var, width=40, relief="sunken")
        folder_label.pack(side=tk.LEFT, padx=(0, 10))
        
        self.folder_browse_btn = ttk.Button(folder_row, text="ğŸ“ é¸æŠ", command=self.browse_folder, width=8)
        self.folder_browse_btn.pack(side=tk.LEFT, padx=(0, 5))
        
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ã‚©ãƒ«ãƒ€å°‚ç”¨é¸æŠãƒœã‚¿ãƒ³
        network_btn = ttk.Button(folder_row, text="ğŸŒ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯", command=self.browse_network_folder, width=12)
        network_btn.pack(side=tk.LEFT, padx=(0, 5))
        
        # UNCãƒ‘ã‚¹ç›´æ¥å…¥åŠ›æ©Ÿèƒ½
        unc_input_btn = ttk.Button(folder_row, text="ğŸ“ UNCãƒ‘ã‚¹", command=self.input_unc_path, width=10)
        unc_input_btn.pack(side=tk.LEFT)
        print(f"ğŸ”§ ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼é¸æŠãƒœã‚¿ãƒ³åˆæœŸåŒ–å®Œäº†: {self.folder_browse_btn}")
        print(f"ğŸ”§ UNCãƒ‘ã‚¹å…¥åŠ›ãƒœã‚¿ãƒ³åˆæœŸåŒ–å®Œäº†: {unc_input_btn}")
        
        # æƒ…å ±è¡¨ç¤ºè¡Œ
        info_row = ttk.Frame(bulk_frame)
        info_row.pack(fill=tk.X, pady=(0, 5))
        
        # å¯¾è±¡æƒ…å ±è¡¨ç¤º
        self.target_info_var = tk.StringVar(value="å¯¾è±¡ã‚’é¸æŠã—ã¦ãã ã•ã„")
        ttk.Label(info_row, textvariable=self.target_info_var, font=("", 9)).pack(side=tk.LEFT)
        
        # åˆ¶å¾¡è¡Œ
        control_row = ttk.Frame(bulk_frame)
        control_row.pack(fill=tk.X)
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å®Ÿè¡Œãƒœã‚¿ãƒ³
        self.bulk_index_btn = ttk.Button(control_row, text="ğŸš€ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é–‹å§‹", 
                                        command=self.start_bulk_indexing, width=18, state="disabled")
        self.bulk_index_btn.pack(side=tk.LEFT, padx=(0, 5))
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ ã‚­ãƒ£ãƒ³ã‚»ãƒ«ãƒœã‚¿ãƒ³
        self.cancel_index_btn = ttk.Button(control_row, text="âŒ ã‚­ãƒ£ãƒ³ã‚»ãƒ«", 
                                          command=self.cancel_indexing, width=12, state="disabled")
        self.cancel_index_btn.pack(side=tk.LEFT, padx=(0, 10))
        
        # é€²æ—è¡¨ç¤º
        self.bulk_progress_var = tk.StringVar(value="å¾…æ©Ÿä¸­...")
        ttk.Label(control_row, textvariable=self.bulk_progress_var, font=("", 9)).pack(side=tk.LEFT)
        
        # åˆæœŸçŠ¶æ…‹è¨­å®š
        print("ğŸ”§ åˆæœŸçŠ¶æ…‹è¨­å®šå®Ÿè¡Œ...")
        # ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼é¸æŠãƒœã‚¿ãƒ³ã®çŠ¶æ…‹ã‚’å¼·åˆ¶ç¢ºèª
        try:
            self.folder_browse_btn.config(state="normal")
            print("ğŸ”§ ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼é¸æŠãƒœã‚¿ãƒ³ã‚’å¼·åˆ¶çš„ã«æœ‰åŠ¹åŒ–")
        except:
            pass
        self.on_target_type_changed()

        # æ¤œç´¢ãƒ•ãƒ¬ãƒ¼ãƒ 
        search_frame = ttk.LabelFrame(main_frame, text="ğŸ” è¶…é«˜é€Ÿãƒ©ã‚¤ãƒ–æ¤œç´¢", padding=10)
        search_frame.pack(fill=tk.X, pady=(0, 10))

        # æ¤œç´¢å…¥åŠ›
        ttk.Label(search_frame, text="æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:").pack(anchor=tk.W)
        self.search_entry = ttk.Entry(search_frame, textvariable=self.search_var, font=("", 12))
        self.search_entry.pack(fill=tk.X, pady=(5, 10))

        # æ¤œç´¢ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        options_frame = ttk.Frame(search_frame)
        options_frame.pack(fill=tk.X, pady=(0, 10))

        self.regex_var = tk.BooleanVar()
        ttk.Checkbutton(options_frame, text="æ­£è¦è¡¨ç¾æ¤œç´¢", variable=self.regex_var).pack(side=tk.LEFT,
                                                                                    padx=(0, 20))

        self.file_type_var = tk.StringVar(value="all")
        ttk.Label(options_frame, text="ãƒ•ã‚¡ã‚¤ãƒ«ç¨®é¡:").pack(side=tk.LEFT, padx=(0, 5))
        file_type_combo = ttk.Combobox(options_frame,
                                       textvariable=self.file_type_var,
                                       values=["all", ".txt", ".docx", ".doc", ".xlsx", ".xls", ".pdf", 
                                              ".tif", ".tiff", ".dot", ".dotx", ".dotm", ".docm",
                                              ".xlt", ".xltx", ".xltm", ".xlsm", ".xlsb",
                                              ".jwc", ".dxf", ".sfc", ".jww", ".dwg", ".dwt", ".mpp", ".mpz", ".zip"],
                                       state="readonly",
                                       width=12)
        file_type_combo.pack(side=tk.LEFT, padx=(0, 20))

        # æ‰‹å‹•æ¤œç´¢ãƒœã‚¿ãƒ³
        ttk.Button(options_frame, text="ğŸ” æ¤œç´¢å®Ÿè¡Œ", command=self.perform_search).pack(side=tk.LEFT)

        # çµ±è¨ˆè¡¨ç¤ºãƒ•ãƒ¬ãƒ¼ãƒ 
        stats_frame = ttk.LabelFrame(main_frame, text="ğŸ“Š ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çµ±è¨ˆ", padding=10)
        stats_frame.pack(fill=tk.X, pady=(0, 10))

        self.stats_label = ttk.Label(stats_frame, text="çµ±è¨ˆæƒ…å ±ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        self.stats_label.pack(anchor=tk.W)

        # 3å±¤ãƒ¬ã‚¤ãƒ¤ãƒ¼çŠ¶æ³è¡¨ç¤º
        layer_frame = ttk.LabelFrame(main_frame, text="âš¡ 3å±¤ãƒ¬ã‚¤ãƒ¤ãƒ¼çŠ¶æ³", padding=10)
        layer_frame.pack(fill=tk.X, pady=(0, 10))

        # èª¬æ˜ãƒ†ã‚­ã‚¹ãƒˆ
        explanation_label = ttk.Label(layer_frame, 
                                    text="ğŸ’¡ åŒã˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¤‡æ•°ã®å±¤ã«å­˜åœ¨ã—ã¾ã™ã€‚å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã¯å®Œå…¨å±¤ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼‰ã®æ•°ã§ã™ã€‚",
                                    foreground="blue", font=("", 9))
        explanation_label.pack(anchor=tk.W, pady=(0, 5))

        layer_info_frame = ttk.Frame(layer_frame)
        layer_info_frame.pack(fill=tk.X)

        # å³åº§å±¤
        immediate_frame = ttk.Frame(layer_info_frame)
        immediate_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=(0, 10))
        ttk.Label(immediate_frame, text="ğŸ”´ å³åº§å±¤(ã‚­ãƒ£ãƒƒã‚·ãƒ¥)", foreground="red", font=("", 10, "bold")).pack()
        self.immediate_label = ttk.Label(immediate_frame, text="0 ãƒ•ã‚¡ã‚¤ãƒ«")
        self.immediate_label.pack()

        # é«˜é€Ÿå±¤
        hot_frame = ttk.Frame(layer_info_frame)
        hot_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=(0, 10))
        ttk.Label(hot_frame, text="ğŸŸ¡ é«˜é€Ÿå±¤(ã‚­ãƒ£ãƒƒã‚·ãƒ¥)", foreground="orange", font=("", 10, "bold")).pack()
        self.hot_label = ttk.Label(hot_frame, text="0 ãƒ•ã‚¡ã‚¤ãƒ«")
        self.hot_label.pack()

        # å®Œå…¨å±¤
        complete_frame = ttk.Frame(layer_info_frame)
        complete_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        ttk.Label(complete_frame, text="ğŸŸ¢ å®Œå…¨å±¤(å®Ÿãƒ•ã‚¡ã‚¤ãƒ«æ•°)", foreground="green", font=("", 10, "bold")).pack()
        self.complete_label = ttk.Label(complete_frame, text="0 ãƒ•ã‚¡ã‚¤ãƒ«")
        self.complete_label.pack()

        # çµæœè¡¨ç¤ºãƒ•ãƒ¬ãƒ¼ãƒ 
        results_frame = ttk.LabelFrame(main_frame, text="ğŸ“‹ æ¤œç´¢çµæœï¼ˆâ€»ãƒ•ã‚¡ã‚¤ãƒ«ãŒé–‹ã‹ãªã„ã¨ãã¯å³ã‚¯ãƒªãƒƒã‚¯ã‚’ãŠè©¦ã—ãã ã•ã„ï¼‰", padding=10)
        results_frame.pack(fill=tk.BOTH, expand=True, pady=(0, 10))

        # çµæœãƒ„ãƒªãƒ¼ãƒ“ãƒ¥ãƒ¼
        columns = ("layer", "file_name", "file_path", "relevance", "preview")
        self.results_tree = ttk.Treeview(results_frame, columns=columns, show="headings", height=15)

        # åˆ—è¨­å®š
        self.results_tree.heading("layer", text="å±¤")
        self.results_tree.heading("file_name", text="ãƒ•ã‚¡ã‚¤ãƒ«å")
        self.results_tree.heading("file_path", text="ãƒ‘ã‚¹")
        self.results_tree.heading("relevance", text="é–¢é€£åº¦")
        self.results_tree.heading("preview", text="ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")

        self.results_tree.column("layer", width=80, minwidth=60)
        self.results_tree.column("file_name", width=200, minwidth=150)
        self.results_tree.column("file_path", width=300, minwidth=200)
        self.results_tree.column("relevance", width=80, minwidth=60)
        self.results_tree.column("preview", width=300, minwidth=200)

        # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ãƒãƒ¼
        scrollbar = ttk.Scrollbar(results_frame,
                                  orient=tk.VERTICAL,
                                  command=self.results_tree.yview)
        self.results_tree.configure(yscrollcommand=scrollbar.set)

        self.results_tree.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        scrollbar.pack(side=tk.RIGHT, fill=tk.Y)  # ãƒ€ãƒ–ãƒ«ã‚¯ãƒªãƒƒã‚¯ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã
        self.results_tree.bind("<Double-1>", self.open_selected_file)
        self.results_tree.bind("<Button-3>", self.show_context_menu)  # ğŸ†• å³ã‚¯ãƒªãƒƒã‚¯ãƒ¡ãƒ‹ãƒ¥ãƒ¼
        
        # ãƒã‚¤ãƒ©ã‚¤ãƒˆç”¨ã‚¿ã‚°è¨­å®šï¼ˆå‰Šé™¤ï¼šèƒŒæ™¯è‰²ã¯ä½¿ç”¨ã—ãªã„ï¼‰
        # self.results_tree.tag_configure("highlight", background="#FFFF88", foreground="#000000")  # å‰Šé™¤
        # self.results_tree.tag_configure("highlighted_row", background="#FFF8DC", foreground="#8B0000")  # å‰Šé™¤
        # self.results_tree.tag_configure("keyword_match", background="#FFE135", foreground="#000080")  # å‰Šé™¤
        
        # ãƒã‚¦ã‚¹ãƒ›ãƒãƒ¼åŠ¹æœã‚’è¿½åŠ ï¼ˆè¦–è¦šçš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å‘ä¸Šï¼‰
        self.results_tree.bind("<Motion>", self._on_tree_motion)
        self.results_tree.bind("<Leave>", self._on_tree_leave)

        # åˆ¶å¾¡ãƒœã‚¿ãƒ³ãƒ•ãƒ¬ãƒ¼ãƒ 
        control_frame = ttk.Frame(main_frame)
        control_frame.pack(fill=tk.X)

        ttk.Button(control_frame, text="ğŸ“ ãƒ•ã‚©ãƒ«ãƒ€ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹",
                   command=self.index_folder).pack(side=tk.LEFT, padx=(0, 10))
        ttk.Button(control_frame, text="ğŸ“Š è©³ç´°çµ±è¨ˆ",
                   command=self.show_detailed_stats).pack(side=tk.LEFT, padx=(0, 10))
        ttk.Button(control_frame, text="ğŸ” ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çŠ¶æ³ç¢ºèª",
                   command=self.show_index_status).pack(side=tk.LEFT, padx=(0, 10))
        ttk.Button(control_frame, text="ğŸ“‹ ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°è¡¨ç¤º", command=self.show_debug_log).pack(side=tk.LEFT,
                                                                                       padx=(0, 10))
        ttk.Button(control_frame, text="ğŸ—‘ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢", command=self.clear_cache).pack(side=tk.LEFT,
                                                                                     padx=(0, 10))
        ttk.Button(control_frame, text="ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–",
                   command=self.optimize_database).pack(side=tk.LEFT)  # å®šæœŸæ›´æ–°é–‹å§‹ï¼ˆè»½é‡åŒ–ï¼‰
        self.update_statistics()
        self.root.after(5000, self.periodic_update)  # 5ç§’é–“éš”ã«å¤‰æ›´ã—ã¦è² è·è»½æ¸›

    def on_search_change(self, *args):
        """ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«æ¤œç´¢ï¼ˆè² è·è»½æ¸›ç‰ˆï¼‰"""
        query = self.search_var.get().strip()
        
        # æœ€å°æ–‡å­—æ•°ãƒã‚§ãƒƒã‚¯ï¼ˆè² è·è»½æ¸›ï¼‰
        if len(query) < self.min_search_length:
            self.clear_results()
            return
            
        current_time = time.time()
        self.last_search_time = current_time

        # é…å»¶å®Ÿè¡Œ
        self.root.after(int(self.search_delay * 1000), lambda: self.delayed_search(current_time))

    def delayed_search(self, scheduled_time):
        """é…å»¶æ¤œç´¢å®Ÿè¡Œ"""
        if scheduled_time == self.last_search_time:
            self.perform_search()

    def perform_search(self):
        """ğŸ”„ æ¤œç´¢å®Ÿè¡Œï¼ˆåŠè§’å…¨è§’å¯¾å¿œ + ãƒ•ã‚¡ã‚¤ãƒ«ç¨®é¡ãƒ•ã‚£ãƒ«ã‚¿ï¼‰"""
        query = self.search_var.get().strip()
        selected_file_type = self.file_type_var.get()  # ğŸ†• ãƒ•ã‚¡ã‚¤ãƒ«ç¨®é¡ãƒ•ã‚£ãƒ«ã‚¿å–å¾—

        if not query:
            self.clear_results()
            return

        try:
            start_time = time.time()

            # åŠè§’å…¨è§’å¯¾å¿œã®æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç”Ÿæˆ
            half_width, full_width, normalized, query_patterns = normalize_search_text_ultra(query)

            # æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³æƒ…å ±ã‚’è¡¨ç¤º
            pattern_info = f"æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³: {len(query_patterns)}å€‹"
            if len(query_patterns) > 1:
                pattern_preview = ', '.join(query_patterns[:2])
                if len(query_patterns) > 2:
                    pattern_preview += f" +{len(query_patterns)-2}å€‹"
                filter_info = f" | ãƒ•ã‚£ãƒ«ã‚¿: {selected_file_type}"
                self.root.title(f"100%ä»•æ§˜é©åˆã‚¢ãƒ—ãƒª - {pattern_info} ({pattern_preview}){filter_info}")

            # ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«æ¤œç´¢ç”¨ã®è»½é‡åŒ–è¨­å®š
            # 5100ä»¶ä»¥ä¸Šå¯¾å¿œã®æ¤œç´¢çµæœæ•°è¨­å®š
            max_results = 5500 if len(query) >= 4 else 3000  # é•·ã„æ¤œç´¢èªã§æœ€å¤§çµæœã€çŸ­ã„æ¤œç´¢èªã§ã‚‚ååˆ†ãªçµæœæ•°
            
            # ğŸ†• ãƒ•ã‚¡ã‚¤ãƒ«ç¨®é¡ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨ã®æ‹¡å¼µæ¤œç´¢å®Ÿè¡Œï¼ˆ5100ä»¶ä»¥ä¸Šå¯¾å¿œï¼‰
            results = self.search_system.unified_three_layer_search(
                query,
                max_results=max_results,  # 5100ä»¶ä»¥ä¸Šå¯¾å¿œã®å¤§å®¹é‡çµæœ
                file_type_filter=selected_file_type  # ãƒ•ã‚¡ã‚¤ãƒ«ç¨®é¡ãƒ•ã‚£ãƒ«ã‚¿ã‚’è¿½åŠ 
            )

            # çµæœã‚’åŠè§’å…¨è§’ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            if len(query_patterns) > 1:
                enhanced_results = []
                for result in results:
                    # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¨ãƒ•ã‚¡ã‚¤ãƒ«åã§åŠè§’å…¨è§’ãƒãƒƒãƒãƒ³ã‚°ã‚’ç¢ºèª
                    content_text = result.get('content_preview', '') + ' ' + result.get(
                        'file_name', '')
                    if enhanced_search_match(content_text, query_patterns):
                        # ãƒãƒƒãƒã—ãŸå ´åˆã¯ã‚¹ã‚³ã‚¢ã‚’å‘ä¸Š
                        result['relevance_score'] = result.get('relevance_score', 0.5) + 0.1
                        enhanced_results.append(result)

                # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
                results = sorted(enhanced_results,
                                 key=lambda x: x.get('relevance_score', 0),
                                 reverse=True)

            # ğŸ†• ãƒ•ã‚¡ã‚¤ãƒ«ç¨®é¡ãƒ•ã‚£ãƒ«ã‚¿ã‚’çµæœã«è¿½åŠ é©ç”¨ï¼ˆäºŒé‡ãƒã‚§ãƒƒã‚¯ï¼‰
            if selected_file_type != "all":
                filtered_results = []
                for result in results:
                    file_path = result.get('file_path', '')
                    if file_path.lower().endswith(selected_file_type.lower()):
                        filtered_results.append(result)
                results = filtered_results

            search_time = time.time() - start_time
            self.display_results(results, search_time)

        except Exception as e:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: é€šå¸¸æ¤œç´¢ï¼ˆ5100ä»¶ä»¥ä¸Šå¯¾å¿œï¼‰
            try:
                results = self.search_system.unified_three_layer_search(query, max_results=5500)  # 5100ä»¶ä»¥ä¸Šå¯¾å¿œ
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ™‚ã‚‚ãƒ•ã‚¡ã‚¤ãƒ«ç¨®é¡ãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨
                if selected_file_type != "all":
                    filtered_results = []
                    for result in results:
                        file_path = result.get('file_path', '')
                        if file_path.lower().endswith(selected_file_type.lower()):
                            filtered_results.append(result)
                    results = filtered_results

                search_time = time.time() - start_time
                self.display_results(results, search_time)
            except Exception as e2:
                messagebox.showerror("æ¤œç´¢ã‚¨ãƒ©ãƒ¼", f"æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\nãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢ã‚‚å¤±æ•—: {e2}")

    def display_results(self, results: List[Dict[str, Any]], search_time: float):
        """æ¤œç´¢çµæœè¡¨ç¤ºï¼ˆè»½é‡åŒ–ç‰ˆãƒ»UTF-8å¯¾å¿œå¼·åŒ–ãƒ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚¤ãƒ©ã‚¤ãƒˆå¯¾å¿œï¼‰"""
        # æ—¢å­˜çµæœã‚¯ãƒªã‚¢
        for item in self.results_tree.get_children():
            self.results_tree.delete(item)

        # ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«æ¤œç´¢ç”¨ã«è¡¨ç¤ºæ•°åˆ¶é™ï¼ˆUIã®è»½é‡åŒ–ï¼‰
        max_display = 100  # æœ€å¤§100ä»¶ã¾ã§è¡¨ç¤º
        display_results = results[:max_display]
        
        # ç¾åœ¨ã®æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’å–å¾—ï¼ˆãƒã‚¤ãƒ©ã‚¤ãƒˆç”¨ï¼‰
        current_query = self.search_var.get().strip()
        
        # UTF-8å¯¾å¿œã®å®‰å…¨ãªæ–‡å­—åˆ—åˆ‡ã‚Šå–ã‚Šé–¢æ•°
        def safe_truncate_utf8_display(text: str, max_length: int) -> str:
            """UIè¡¨ç¤ºç”¨UTF-8æ–‡å­—åˆ—ã‚’å®‰å…¨ã«åˆ‡ã‚Šå–ã‚‹ï¼ˆæ—¥æœ¬èªå¯¾å¿œï¼‰"""
            if not text or len(text) <= max_length:
                return text
            # æ–‡å­—å¢ƒç•Œã§å®‰å…¨ã«åˆ‡ã‚Šå–ã‚Š
            truncated = text[:max_length]
            # æœ€å¾Œã®æ–‡å­—ãŒä¸å®Œå…¨ãªå ´åˆã¯1æ–‡å­—å‰Šã‚‹
            try:
                truncated.encode('utf-8')
                return truncated + "..."
            except UnicodeEncodeError:
                return (text[:max_length-1] if max_length > 1 else "") + "..."
        
        def highlight_keywords_in_text(text: str, query: str) -> str:
            """ãƒ†ã‚­ã‚¹ãƒˆå†…ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ã‚·ãƒ³ãƒ—ãƒ«ãƒã‚¤ãƒ©ã‚¤ãƒˆè¡¨ç¤ºç”¨ã«ãƒãƒ¼ã‚¯ã‚¢ãƒƒãƒ—"""
            if not text or not query:
                return text
            
            # æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç”Ÿæˆï¼ˆåŠè§’å…¨è§’å¯¾å¿œï¼‰
            try:
                half_width, full_width, normalized, query_patterns = normalize_search_text_ultra(query)
                
                # ãƒã‚¤ãƒ©ã‚¤ãƒˆå¯¾è±¡ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æº–å‚™ï¼ˆé‡è¤‡é™¤å»ï¼‰
                highlight_patterns = list(set(query_patterns))
                # å…ƒã®ã‚¯ã‚¨ãƒªã‚‚è¿½åŠ ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ãƒãƒƒãƒç”¨ï¼‰
                highlight_patterns.append(query.strip())
                # é‡è¤‡é™¤å»
                highlight_patterns = list(set(highlight_patterns))
                # é•·ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰å‡¦ç†ï¼ˆã‚ˆã‚Šé•·ã„ãƒãƒƒãƒã‚’å„ªå…ˆï¼‰
                highlight_patterns.sort(key=len, reverse=True)
                
                # å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ãƒã‚¤ãƒ©ã‚¤ãƒˆé©ç”¨
                highlighted_text = text
                for pattern in highlight_patterns:
                    if len(pattern.strip()) >= 1:  # 1æ–‡å­—ä»¥ä¸Šã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ãƒã‚¤ãƒ©ã‚¤ãƒˆ
                        # å¤§æ–‡å­—å°æ–‡å­—ã‚’åŒºåˆ¥ã—ãªã„ç½®æ›
                        import re
                        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã—ã¦æ­£è¦è¡¨ç¾ã¨ã—ã¦å®‰å…¨ã«ä½¿ç”¨
                        escaped_pattern = re.escape(pattern.strip())
                        if escaped_pattern:  # ç©ºæ–‡å­—åˆ—ã§ãªã„å ´åˆã®ã¿å‡¦ç†
                            # å¤§æ–‡å­—å°æ–‡å­—ã‚’åŒºåˆ¥ã—ãªã„æ¤œç´¢
                            # ã‚·ãƒ³ãƒ—ãƒ«ãªãƒã‚¤ãƒ©ã‚¤ãƒˆï¼ˆãƒãƒ¼ã‚«ãƒ¼ãªã—ï¼‰
                            highlighted_text = re.sub(
                                f'({escaped_pattern})', 
                                r'\1',  # ãã®ã¾ã¾è¡¨ç¤ºï¼ˆç‰¹åˆ¥ãªãƒãƒ¼ã‚«ãƒ¼ãªã—ï¼‰
                                highlighted_text, 
                                flags=re.IGNORECASE
                            )
                
                return highlighted_text
                
            except Exception as e:
                # ãƒã‚¤ãƒ©ã‚¤ãƒˆå‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™
                debug_logger.warning(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚¤ãƒ©ã‚¤ãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                return text
        
        # çµæœè¡¨ç¤ºï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ç¨®é¡è‰²åˆ†ã‘å¯¾å¿œãƒ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚¤ãƒ©ã‚¤ãƒˆå¯¾å¿œï¼‰
        for i, result in enumerate(display_results):
            layer_color = {'immediate': 'ğŸ”´', 'hot': 'ğŸŸ¡', 'complete': 'ğŸŸ¢'}.get(result['layer'], 'âšª')

            # UTF-8å¯¾å¿œã®å®‰å…¨ãªãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤ºï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚¤ãƒ©ã‚¤ãƒˆé©ç”¨ï¼‰
            raw_preview = result.get('content_preview', '')
            # ã¾ãšã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚¤ãƒ©ã‚¤ãƒˆã‚’é©ç”¨
            highlighted_preview = highlight_keywords_in_text(raw_preview, current_query)
            # æ¬¡ã«é•·ã•åˆ¶é™ã‚’é©ç”¨
            preview_text = safe_truncate_utf8_display(highlighted_preview, 150)  # ãƒã‚¤ãƒ©ã‚¤ãƒˆåˆ†ã‚’è€ƒæ…®ã—ã¦é•·ã‚ã«
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ç¨®é¡ã«å¿œã˜ãŸã‚¿ã‚°ã‚’è¨­å®š
            file_ext = os.path.splitext(result['file_name'])[1].lower()
            file_tag = self._get_file_type_tag(file_ext)
            
            item_id = self.results_tree.insert(
                "",
                tk.END,
                values=(f"{layer_color} {result['layer']}", result['file_name'],
                        result['file_path'], f"{result['relevance_score']:.2f}",
                        preview_text),
                tags=[file_tag])
            
        # ãƒ•ã‚¡ã‚¤ãƒ«ç¨®é¡ã‚¿ã‚°ã®è‰²è¨­å®š
        self._setup_file_type_colors()
        # çµæœçµ±è¨ˆè¡¨ç¤º
        layer_counts: Dict[str, int] = {}
        for result in results:
            layer_counts[result['layer']] = layer_counts.get(result['layer'], 0) + 1

        # è¡¨ç¤ºåˆ¶é™ã®æƒ…å ±ã‚’å«ã‚ã‚‹
        display_info = f"è¡¨ç¤º: {len(display_results)}" + (f"/{len(results)}" if len(results) > max_display else "")
        status_text = f"æ¤œç´¢å®Œäº†: {len(results)}ä»¶ ({search_time:.4f}ç§’) [{display_info}ä»¶] - "
        status_text += f"å³åº§å±¤:{layer_counts.get('immediate', 0)} é«˜é€Ÿå±¤:{layer_counts.get('hot', 0)} å®Œå…¨å±¤:{layer_counts.get('complete', 0)}"

        self.root.title(f"100%ä»•æ§˜é©åˆ è¶…é«˜é€Ÿãƒ©ã‚¤ãƒ–å…¨æ–‡æ¤œç´¢ã‚¢ãƒ—ãƒª - {status_text}")

    def clear_results(self):
        """çµæœã‚¯ãƒªã‚¢"""
        for item in self.results_tree.get_children():
            self.results_tree.delete(item)

        self.root.title("100%ä»•æ§˜é©åˆ è¶…é«˜é€Ÿãƒ©ã‚¤ãƒ–å…¨æ–‡æ¤œç´¢ã‚¢ãƒ—ãƒª")

    def update_statistics(self):
        """çµ±è¨ˆæƒ…å ±æ›´æ–°ï¼ˆ8ä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å¯¾å¿œãƒ»ãƒ‡ãƒãƒƒã‚°å¼·åŒ–ç‰ˆï¼‰"""
        try:
            # ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ä¸­ã¾ãŸã¯åœæ­¢ã•ã‚ŒãŸå ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if hasattr(self.search_system, 'shutdown_requested') and self.search_system.shutdown_requested:
                return
            if not hasattr(self, 'root') or not self.root.winfo_exists():
                return
                
            current_time = time.time()
            
            # æ›´æ–°é »åº¦åˆ¶é™ãƒã‚§ãƒƒã‚¯
            if current_time - self._last_stats_update_time < self._stats_update_interval:
                if not self._pending_stats_update:
                    self._pending_stats_update = True
                    # æ¬¡å›æ›´æ–°ã‚’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
                    delay = int((self._stats_update_interval - (current_time - self._last_stats_update_time)) * 1000)
                    try:
                        self.root.after(delay, self._execute_pending_stats_update)
                    except tk.TclError:
                        # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ãŒæ—¢ã«ç ´æ£„ã•ã‚Œã¦ã„ã‚‹å ´åˆ
                        return
                return
            
            self._last_stats_update_time = current_time
            self._pending_stats_update = False

            debug_logger.debug("GUIçµ±è¨ˆæ›´æ–°é–‹å§‹")

            # è»½é‡çµ±è¨ˆï¼ˆå³åº§å–å¾—ï¼‰
            immediate_count = len(self.search_system.immediate_cache)
            hot_count = len(self.search_system.hot_cache)
            
            debug_logger.debug(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆ: immediate={immediate_count}, hot={hot_count}")
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çŠ¶æ³ã®å–å¾—
            indexing_status = ""
            if self.search_system.indexing_in_progress:
                indexing_status = " ğŸ“¦ [ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæ¥­ä¸­]"
            elif self.search_system.indexing_results_ready:
                indexing_status = " âœ… [ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å®Œäº†]"
            
            # å³åº§å±¤ãƒ»é«˜é€Ÿå±¤ã¯å³åº§ã«æ›´æ–°
            self.immediate_label.config(text=f"{immediate_count:,} ãƒ•ã‚¡ã‚¤ãƒ«")
            self.hot_label.config(text=f"{hot_count:,} ãƒ•ã‚¡ã‚¤ãƒ«")

            debug_logger.debug("å³åº§å±¤ãƒ»é«˜é€Ÿå±¤UIæ›´æ–°å®Œäº†")

            # å®Œå…¨å±¤çµ±è¨ˆã¯ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å–å¾—ï¼ˆ8ä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å¯¾å¿œï¼‰
            self._update_complete_layer_stats_async(indexing_status)

        except Exception as e:
            logging.error(f"çµ±è¨ˆæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
            debug_logger.error(f"GUIçµ±è¨ˆæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
            self.stats_label.config(text="çµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼")

    def _execute_pending_stats_update(self):
        """ä¿ç•™ä¸­ã®çµ±è¨ˆæ›´æ–°å®Ÿè¡Œ"""
        if self._pending_stats_update and hasattr(self, 'root') and self.root.winfo_exists():
            self.update_statistics()

    def _update_complete_layer_stats_async(self, indexing_status: str):
        """å®Œå…¨å±¤çµ±è¨ˆã®éåŒæœŸæ›´æ–°ï¼ˆ8ä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å¯¾å¿œç‰ˆãƒ»ä¿®æ­£ç‰ˆï¼‰"""
        def background_stats_update():
            try:
                # ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ãƒã‚§ãƒƒã‚¯
                if hasattr(self.search_system, 'shutdown_requested') and self.search_system.shutdown_requested:
                    return
                    
                debug_logger.debug("8ä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰çµ±è¨ˆå–å¾—é–‹å§‹")
                
                # ã¾ãšã‚¯ã‚¤ãƒƒã‚¯çµ±è¨ˆã§å®Œå…¨å±¤ã®ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’å–å¾—
                quick_complete_count = 0
                valid_db_count = 0
                
                try:
                    for i, db_path in enumerate(self.search_system.complete_db_paths):
                        try:
                            if os.path.exists(db_path) and os.path.getsize(db_path) > 1024:
                                conn = sqlite3.connect(db_path, timeout=2.0)
                                cursor = conn.cursor()
                                cursor.execute("SELECT COUNT(*) FROM documents")
                                count = cursor.fetchone()[0]
                                quick_complete_count += count
                                valid_db_count += 1
                                conn.close()
                                debug_logger.debug(f"ã‚¯ã‚¤ãƒƒã‚¯çµ±è¨ˆ DB{i}: {count}ãƒ•ã‚¡ã‚¤ãƒ«")
                        except Exception as e:
                            debug_logger.debug(f"DB{i}ã‚¯ã‚¤ãƒƒã‚¯çµ±è¨ˆã‚¹ã‚­ãƒƒãƒ—: {e}")
                    
                    debug_logger.info(f"ã‚¯ã‚¤ãƒƒã‚¯çµ±è¨ˆå®Œäº†: {quick_complete_count}ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ{valid_db_count}å€‹ã®DBï¼‰")
                    
                    # UIæ›´æ–°ã‚’ãƒ¡ã‚¤ãƒ³ã‚¹ãƒ¬ãƒƒãƒ‰ã«å§”è­²ï¼ˆã‚¯ã‚¤ãƒƒã‚¯çµ±è¨ˆç‰ˆï¼‰
                    if hasattr(self, 'root') and self.root.winfo_exists():
                        try:
                            self.root.after(0, lambda: self._update_ui_with_complete_stats(quick_complete_count, indexing_status))
                        except tk.TclError:
                            return
                        
                except Exception as e:
                    debug_logger.error(f"ã‚¯ã‚¤ãƒƒã‚¯çµ±è¨ˆã‚¨ãƒ©ãƒ¼: {e}")
                    # ã‚¨ãƒ©ãƒ¼æ™‚ã¯æ—¢å­˜ã®è©³ç´°çµ±è¨ˆã‚’è©¦è¡Œ
                    try:
                        stats = self.search_system.get_comprehensive_statistics()
                        complete_count = stats.get('total_files', 0)
                        
                        debug_logger.debug(f"è©³ç´°çµ±è¨ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {complete_count}ãƒ•ã‚¡ã‚¤ãƒ«")

                        if hasattr(self, 'root') and self.root.winfo_exists():
                            try:
                                self.root.after(0, lambda: self._update_ui_with_complete_stats(complete_count, indexing_status))
                            except tk.TclError:
                                return
                    except Exception as e2:
                        debug_logger.error(f"è©³ç´°çµ±è¨ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e2}")
                        if hasattr(self, 'root') and self.root.winfo_exists():
                            try:
                                self.root.after(0, lambda: self.complete_label.config(text="çµ±è¨ˆã‚¨ãƒ©ãƒ¼"))
                            except tk.TclError:
                                return

            except Exception as e:
                debug_logger.error(f"8ä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±è¨ˆã‚¨ãƒ©ãƒ¼: {e}")
                if hasattr(self, 'root') and self.root.winfo_exists():
                    try:
                        self.root.after(0, lambda: self.complete_label.config(text="å–å¾—ã‚¨ãƒ©ãƒ¼"))
                    except tk.TclError:
                        return

        # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œ
        threading.Thread(target=background_stats_update, daemon=True).start()

    def _update_ui_with_complete_stats(self, complete_count: int, indexing_status: str):
        """å®Œå…¨å±¤çµ±è¨ˆã§UIã‚’æ›´æ–°"""
        try:
            # å®Œå…¨å±¤ãƒ©ãƒ™ãƒ«æ›´æ–°
            self.complete_label.config(text=f"{complete_count:,} ãƒ•ã‚¡ã‚¤ãƒ«")

            # ç·åˆçµ±è¨ˆæ›´æ–°
            immediate_count = len(self.search_system.immediate_cache)
            hot_count = len(self.search_system.hot_cache)
            
            total_unique_files = complete_count  # å®Œå…¨å±¤ãŒå®Ÿéš›ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«æ•°
            parallel_info = f" | ä¸¦åˆ—å‡¦ç†: {self.search_system.optimal_threads}ã‚¹ãƒ¬ãƒƒãƒ‰"
            cache_search_info = ""
            
            # å¢—åˆ†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æƒ…å ±
            incremental_info = ""
            if hasattr(self.search_system, 'incremental_indexing_enabled') and self.search_system.incremental_indexing_enabled:
                incremental_updates = self.search_system.stats.get('incremental_updates', 0)
                files_added = self.search_system.stats.get('files_added_incrementally', 0)
                if incremental_updates > 0:
                    incremental_info = f" | å¢—åˆ†æ›´æ–°: {incremental_updates}å› ({files_added}ãƒ•ã‚¡ã‚¤ãƒ«)"
                else:
                    incremental_info = " | å¢—åˆ†ç›£è¦–: æœ‰åŠ¹"
            
            if self.search_system.indexing_in_progress:
                cache_search_info = " | æ¤œç´¢: ã‚­ãƒ£ãƒƒã‚·ãƒ¥å„ªå…ˆ"
            elif self.search_system.indexing_results_ready:
                cache_search_info = " | æ¤œç´¢: 3å±¤ãƒ•ãƒ«åˆ©ç”¨"
                
            self.stats_label.config(
                text=f"ç·ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ•°: {total_unique_files:,} ãƒ•ã‚¡ã‚¤ãƒ«{indexing_status}{parallel_info}{cache_search_info}{incremental_info}")

            debug_logger.debug(
                f"UIçµ±è¨ˆæ›´æ–°å®Œäº†: å³åº§å±¤={immediate_count}, é«˜é€Ÿå±¤={hot_count}, å®Œå…¨å±¤={complete_count}")

        except Exception as e:
            logging.error(f"UIçµ±è¨ˆæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
            self.stats_label.config(text="UIæ›´æ–°ã‚¨ãƒ©ãƒ¼")

    def periodic_update(self):
        """å®šæœŸæ›´æ–°å‡¦ç†ï¼ˆUIå¿œç­”æ€§é‡è¦–ç‰ˆï¼‰"""
        try:
            # UIå¿œç­”æ€§ãƒã‚§ãƒƒã‚¯ï¼šé‡ã„å‡¦ç†ä¸­ã¯çµ±è¨ˆæ›´æ–°ã‚’ã‚¹ã‚­ãƒƒãƒ—
            if hasattr(self, 'bulk_indexing_active') and self.bulk_indexing_active:
                print("ğŸ”„ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸­ã®ãŸã‚çµ±è¨ˆæ›´æ–°ã‚’ã‚¹ã‚­ãƒƒãƒ—")
            else:
                # è»½é‡çµ±è¨ˆæ›´æ–°ã®ã¿å®Ÿè¡Œ
                self._lightweight_statistics_update()
                
        except Exception as e:
            logging.error(f"å®šæœŸæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
        finally:
            # æ¬¡å›æ›´æ–°ã‚’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆUIå¿œç­”æ€§é‡è¦–ã§8ç§’é–“éš”ï¼‰
            self.root.after(8000, self.periodic_update)
    
    def _lightweight_statistics_update(self):
        """è»½é‡çµ±è¨ˆæ›´æ–°ï¼ˆUIå¿œç­”æ€§é‡è¦–ç‰ˆï¼‰"""
        try:
            # å³åº§å±¤ãƒ»é«˜é€Ÿå±¤ã®ã¿æ›´æ–°ï¼ˆé‡ã„å®Œå…¨å±¤çµ±è¨ˆã¯çœç•¥ï¼‰
            immediate_count = len(self.search_system.immediate_cache)
            hot_count = len(self.search_system.hot_cache)
            
            # å³åº§å±¤ãƒ»é«˜é€Ÿå±¤ãƒ©ãƒ™ãƒ«æ›´æ–°
            self.immediate_label.config(text=f"{immediate_count:,} ãƒ•ã‚¡ã‚¤ãƒ«")
            self.hot_label.config(text=f"{hot_count:,} ãƒ•ã‚¡ã‚¤ãƒ«")
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çŠ¶æ³è¡¨ç¤ºï¼ˆè»½é‡ç‰ˆï¼‰
            indexing_status = ""
            if self.search_system.indexing_in_progress:
                indexing_status = " | âš¡ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸­"
            elif hasattr(self, 'bulk_indexing_active') and self.bulk_indexing_active:
                indexing_status = " | ğŸš€ å¤§å®¹é‡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸­"
            
            # è»½é‡çµ±è¨ˆè¡¨ç¤º
            parallel_info = f" | ä¸¦åˆ—: {getattr(self.search_system, 'optimal_threads', 8)}ã‚¹ãƒ¬ãƒƒãƒ‰"
            cache_info = f" | ã‚­ãƒ£ãƒƒã‚·ãƒ¥: å³åº§{immediate_count}+é«˜é€Ÿ{hot_count}"
            
            self.stats_label.config(
                text=f"è»½é‡çµ±è¨ˆ{indexing_status}{parallel_info}{cache_info}")
            
            debug_logger.debug(f"è»½é‡çµ±è¨ˆæ›´æ–°å®Œäº†: å³åº§å±¤={immediate_count}, é«˜é€Ÿå±¤={hot_count}")
            
        except Exception as e:
            logging.error(f"è»½é‡çµ±è¨ˆæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
            self.stats_label.config(text="è»½é‡çµ±è¨ˆã‚¨ãƒ©ãƒ¼")

    def open_selected_file(self, event):
        """ğŸ¯ é¸æŠãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’é–‹ãï¼ˆãƒ€ãƒ–ãƒ«ã‚¯ãƒªãƒƒã‚¯æ™‚ãƒ»å®Œå…¨é‡è¤‡é˜²æ­¢ç‰ˆãƒ»ãƒ‡ãƒãƒƒã‚°å¼·åŒ–ãƒ»è¶…å³æ ¼ç‰ˆï¼‰"""
        
        # ğŸ” ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ï¼šãƒ€ãƒ–ãƒ«ã‚¯ãƒªãƒƒã‚¯ã‚¤ãƒ™ãƒ³ãƒˆç™ºç”Ÿ
        debug_logger.info("ğŸ” [DOUBLE_CLICK] ãƒ€ãƒ–ãƒ«ã‚¯ãƒªãƒƒã‚¯ã‚¤ãƒ™ãƒ³ãƒˆç™ºç”Ÿ")
        debug_logger.info(f"ğŸ” [EVENT_DETAILS] ã‚¤ãƒ™ãƒ³ãƒˆã‚¿ã‚¤ãƒ—: {event.type}, ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆ: {event.widget}")
        print("ğŸ” [DOUBLE_CLICK] ãƒ€ãƒ–ãƒ«ã‚¯ãƒªãƒƒã‚¯ã‚¤ãƒ™ãƒ³ãƒˆç™ºç”Ÿ")
        
        # è¶…å³æ ¼ãªãƒ€ãƒ–ãƒ«ã‚¯ãƒªãƒƒã‚¯é‡è¤‡é˜²æ­¢ï¼ˆå¤šé‡ãƒã‚§ãƒƒã‚¯ç‰ˆï¼‰
        current_time = time.time()
        
        # ğŸ” ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ï¼šç¾åœ¨ã®çŠ¶æ…‹ç¢ºèª
        debug_logger.debug(f"ğŸ” [STATE_CHECK] ç¾åœ¨æ™‚åˆ»: {current_time:.6f}")
        debug_logger.debug(f"ğŸ” [STATE_CHECK] å‡¦ç†ä¸­ãƒ•ãƒ©ã‚°: {getattr(self, '_double_click_processing', False)}")
        debug_logger.debug(f"ğŸ” [STATE_CHECK] çµ±åˆå‡¦ç†ãƒ•ãƒ©ã‚°: {getattr(self, '_integrated_processing', False)}")
        debug_logger.debug(f"ğŸ” [STATE_CHECK] å‰å›æ™‚åˆ»: {getattr(self, '_last_double_click_time', 'ãªã—')}")
        
        # ç¬¬1æ®µéšï¼šå‡¦ç†ä¸­ãƒ•ãƒ©ã‚°ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€é«˜å„ªå…ˆï¼‰
        if getattr(self, '_double_click_processing', False):
            debug_logger.warning("ğŸ” [BLOCK_PROCESSING] ãƒ€ãƒ–ãƒ«ã‚¯ãƒªãƒƒã‚¯å‡¦ç†ä¸­ã®ãŸã‚ã€æ–°ã—ã„ã‚¤ãƒ™ãƒ³ãƒˆã‚’ãƒ–ãƒ­ãƒƒã‚¯")
            print("ğŸš« [BLOCK_PROCESSING] ãƒ€ãƒ–ãƒ«ã‚¯ãƒªãƒƒã‚¯å‡¦ç†ä¸­ - ã‚¤ãƒ™ãƒ³ãƒˆãƒ–ãƒ­ãƒƒã‚¯")
            return
            
        # ç¬¬2æ®µéšï¼šçµ±åˆå‡¦ç†ä¸­ãƒã‚§ãƒƒã‚¯
        if getattr(self, '_integrated_processing', False):
            debug_logger.warning("ğŸ” [BLOCK_INTEGRATED] çµ±åˆå‡¦ç†ä¸­ã®ãŸã‚ã€æ–°ã—ã„ã‚¤ãƒ™ãƒ³ãƒˆã‚’ãƒ–ãƒ­ãƒƒã‚¯")
            print("ğŸš« [BLOCK_INTEGRATED] çµ±åˆå‡¦ç†ä¸­ - ã‚¤ãƒ™ãƒ³ãƒˆãƒ–ãƒ­ãƒƒã‚¯")
            return
            
        # ç¬¬3æ®µéšï¼šæ™‚é–“ãƒ™ãƒ¼ã‚¹ã®é‡è¤‡é˜²æ­¢ï¼ˆã‚ˆã‚ŠçŸ­ã„é–“éš”ãƒ»ã‚ˆã‚Šå³æ ¼ï¼‰
        if hasattr(self, '_last_double_click_time'):
            time_diff = current_time - self._last_double_click_time
            debug_logger.debug(f"ğŸ” [TIME_CHECK] å‰å›ã‹ã‚‰ã®çµŒéæ™‚é–“: {time_diff:.6f}ç§’")
            if time_diff < 1.0:  # 1ç§’ä»¥å†…ã®é‡è¤‡ã‚’å®Œå…¨ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆå³æ ¼åŒ–ï¼‰
                debug_logger.warning(f"ğŸ” [BLOCK_TIME] ãƒ€ãƒ–ãƒ«ã‚¯ãƒªãƒƒã‚¯æ™‚é–“é–“éš”ä¸è¶³: {time_diff:.3f}ç§’")
                print(f"ğŸš« [BLOCK_TIME] ãƒ€ãƒ–ãƒ«ã‚¯ãƒªãƒƒã‚¯é–“éš”ä¸è¶³: {time_diff:.3f}ç§’ - ãƒ–ãƒ­ãƒƒã‚¯")
                return
        
        # ç¬¬4æ®µéšï¼šé¸æŠãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã§ã‚‚é‡è¤‡ãƒã‚§ãƒƒã‚¯
        selection = self.results_tree.selection()
        if not selection:
            debug_logger.warning("ğŸ” [NO_SELECTION] é¸æŠã•ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ ãªã—")
            self._double_click_processing = False
            return

        item = self.results_tree.item(selection[0])
        file_path = item['values'][2]  # ãƒ‘ã‚¹åˆ—
        file_name = item['values'][1]  # ãƒ•ã‚¡ã‚¤ãƒ«ååˆ—
        
        # ğŸ” ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ï¼šè©³ç´°ãªå€¤ç¢ºèª
        debug_logger.info(f"ğŸ” [TREE_VALUES] TreeView values: {item['values']}")
        debug_logger.info(f"ğŸ” [RAW_PATH] Raw file_path: '{file_path}'")
        debug_logger.info(f"ğŸ” [RAW_NAME] Raw file_name: '{file_name}'")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®æ¤œè¨¼ã¨ä¿®æ­£
        if not os.path.isabs(file_path):
            debug_logger.warning(f"ğŸ” [PATH_WARNING] ç›¸å¯¾ãƒ‘ã‚¹æ¤œå‡º: {file_path}")
            # ç›¸å¯¾ãƒ‘ã‚¹ã®å ´åˆã€çµ¶å¯¾ãƒ‘ã‚¹ã«å¤‰æ›ã‚’è©¦è¡Œ
            if os.path.exists(os.path.join(os.getcwd(), file_path)):
                file_path = os.path.abspath(os.path.join(os.getcwd(), file_path))
                debug_logger.info(f"ğŸ” [PATH_FIXED] çµ¶å¯¾ãƒ‘ã‚¹ã«å¤‰æ›: {file_path}")
        
        # ãƒ‘ã‚¹ã®æ­£è¦åŒ–
        file_path = os.path.normpath(file_path)
        debug_logger.info(f"ğŸ” [NORMALIZED_PATH] æ­£è¦åŒ–å¾Œãƒ‘ã‚¹: {file_path}")
        
        # åŒä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®çŸ­æ™‚é–“é‡è¤‡ãƒã‚§ãƒƒã‚¯
        if hasattr(self, '_last_opened_file'):
            if (self._last_opened_file == file_path and 
                hasattr(self, '_last_double_click_time') and 
                current_time - self._last_double_click_time < 2.0):  # 2ç§’ä»¥å†…ã¯é‡è¤‡ã¨ã¿ãªã™
                debug_logger.warning(f"ğŸ” [BLOCK_SAME_FILE] åŒä¸€ãƒ•ã‚¡ã‚¤ãƒ«çŸ­æ™‚é–“é‡è¤‡: {file_name}")
                print(f"ğŸš« [BLOCK_SAME_FILE] åŒä¸€ãƒ•ã‚¡ã‚¤ãƒ«çŸ­æ™‚é–“é‡è¤‡: {os.path.basename(file_name)} - ãƒ–ãƒ­ãƒƒã‚¯")
                return
        
        # ğŸ” ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ï¼šå‡¦ç†é–‹å§‹
        debug_logger.info("ğŸ” [START] ãƒ€ãƒ–ãƒ«ã‚¯ãƒªãƒƒã‚¯å‡¦ç†é–‹å§‹ï¼ˆå…¨ãƒã‚§ãƒƒã‚¯é€šéï¼‰")
        print("ğŸ” [START] ãƒ€ãƒ–ãƒ«ã‚¯ãƒªãƒƒã‚¯å‡¦ç†é–‹å§‹")
        
        # å…¨ãƒ•ãƒ©ã‚°è¨­å®š
        self._double_click_processing = True
        self._last_double_click_time = current_time
        self._last_opened_file = file_path
        debug_logger.debug("ğŸ” [FLAG_SET] å…¨å‡¦ç†ãƒ•ãƒ©ã‚°ã‚’è¨­å®šã—ã¾ã—ãŸ")
        
        # ğŸ” ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ï¼šé¸æŠãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±
        debug_logger.info(f"ğŸ” [FILE_INFO] é¸æŠãƒ•ã‚¡ã‚¤ãƒ«: {file_name}")
        debug_logger.info(f"ğŸ” [FILE_PATH] ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: {file_path}")

        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª
            if not os.path.exists(file_path):
                debug_logger.error(f"ğŸ” [FILE_NOT_FOUND] ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {file_path}")
                messagebox.showwarning("è­¦å‘Š", f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“:\n{file_path}")
                return

            debug_logger.info(f"ğŸ” [HIGHLIGHT_START] ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚¤ãƒ©ã‚¤ãƒˆå‡¦ç†é–‹å§‹: {os.path.basename(file_path)}")
            print(f"ğŸ¯ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆè¡¨ç¤ºã—ã¾ã™: {os.path.basename(file_path)}")
            
            # çµ±åˆãƒã‚¤ãƒ©ã‚¤ãƒˆå‡¦ç†ï¼šUIè¡¨ç¤ºã¨ãƒ•ã‚©ãƒ«ãƒ€ã‚ªãƒ¼ãƒ—ãƒ³ã‚’ä¸€ã¤ã®å‡¦ç†ã¨ã—ã¦å®Ÿè¡Œ
            self._integrated_highlight_and_open(selection[0], file_path)

        except Exception as e:
            debug_logger.error(f"ğŸ” [ERROR] ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚¤ãƒ©ã‚¤ãƒˆè¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {e}")
            messagebox.showerror("ã‚¨ãƒ©ãƒ¼", f"ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚¤ãƒ©ã‚¤ãƒˆè¡¨ç¤ºã«å¤±æ•—ã—ã¾ã—ãŸ:\n{e}")
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚¤ãƒ©ã‚¤ãƒˆè¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {e}")
        finally:
            # å‡¦ç†å®Œäº†å¾Œã€ãƒ•ãƒ©ã‚°ã‚’ç¢ºå®Ÿã«ãƒªã‚»ãƒƒãƒˆï¼ˆé©åˆ‡ãªé…å»¶ï¼‰
            debug_logger.debug("ğŸ” [FLAG_RESET_SCHEDULE] ãƒ•ãƒ©ã‚°ãƒªã‚»ãƒƒãƒˆã‚’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆ2ç§’å¾Œï¼‰")
            self.root.after(2000, self._reset_double_click_flag)  # 2ç§’å¾Œã«ãƒªã‚»ãƒƒãƒˆ

    def _reset_double_click_flag(self):
        """ãƒ€ãƒ–ãƒ«ã‚¯ãƒªãƒƒã‚¯å‡¦ç†ãƒ•ãƒ©ã‚°ãƒªã‚»ãƒƒãƒˆå°‚ç”¨ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆç¢ºå®Ÿç‰ˆï¼‰"""
        try:
            self._double_click_processing = False
            debug_logger.debug("ãƒ€ãƒ–ãƒ«ã‚¯ãƒªãƒƒã‚¯å‡¦ç†ãƒ•ãƒ©ã‚°ãƒªã‚»ãƒƒãƒˆå®Œäº†")
            print("ğŸ”§ ãƒ€ãƒ–ãƒ«ã‚¯ãƒªãƒƒã‚¯å‡¦ç†ãƒ•ãƒ©ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸ")
        except Exception as reset_error:
            debug_logger.error(f"ãƒ€ãƒ–ãƒ«ã‚¯ãƒªãƒƒã‚¯ãƒ•ãƒ©ã‚°ãƒªã‚»ãƒƒãƒˆã‚¨ãƒ©ãƒ¼: {reset_error}")
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ãƒ•ãƒ©ã‚°ã¯å¼·åˆ¶çš„ã«ãƒªã‚»ãƒƒãƒˆ
            self._double_click_processing = False

    def _integrated_highlight_and_open(self, item_id, file_path):
        """çµ±åˆãƒã‚¤ãƒ©ã‚¤ãƒˆå‡¦ç†ï¼šUIè¡¨ç¤ºã¨ãƒ•ã‚©ãƒ«ãƒ€ã‚ªãƒ¼ãƒ—ãƒ³ã‚’çµ±åˆï¼ˆãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°å¼·åŒ–ç‰ˆãƒ»é‡è¤‡å®Ÿè¡Œå®Œå…¨é˜²æ­¢ï¼‰"""
        
        # ğŸ” ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ï¼šçµ±åˆå‡¦ç†é–‹å§‹
        debug_logger.info("ğŸ” [INTEGRATED_START] çµ±åˆãƒã‚¤ãƒ©ã‚¤ãƒˆ&ã‚ªãƒ¼ãƒ—ãƒ³å‡¦ç†é–‹å§‹")
        debug_logger.debug(f"ğŸ” [INTEGRATED_PARAMS] item_id: {item_id}, file_path: {file_path}")
        print("ğŸ” [INTEGRATED_START] çµ±åˆãƒã‚¤ãƒ©ã‚¤ãƒˆ&ã‚ªãƒ¼ãƒ—ãƒ³å‡¦ç†é–‹å§‹")
        
        # ğŸ” çµ±åˆå‡¦ç†å°‚ç”¨ã®é‡è¤‡é˜²æ­¢ãƒ•ãƒ©ã‚°
        if getattr(self, '_integrated_processing', False):
            debug_logger.warning("ğŸ” [INTEGRATED_BLOCK] çµ±åˆå‡¦ç†å®Ÿè¡Œä¸­ã®ãŸã‚ã€æ–°ã—ã„ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ãƒ–ãƒ­ãƒƒã‚¯")
            print("ğŸš« [INTEGRATED_BLOCK] çµ±åˆå‡¦ç†å®Ÿè¡Œä¸­ - æ–°ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ–ãƒ­ãƒƒã‚¯")
            return
        
        self._integrated_processing = True
        debug_logger.debug("ğŸ” [INTEGRATED_FLAG_SET] çµ±åˆå‡¦ç†ãƒ•ãƒ©ã‚°ã‚’è¨­å®š")
        
        try:
            # 1. æ¤œç´¢çµæœè¡Œã‚’ä¸€æ™‚çš„ã«ãƒã‚¤ãƒ©ã‚¤ãƒˆï¼ˆè¦–è¦šçš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ï¼‰
            debug_logger.debug("ğŸ” [HIGHLIGHT_START] UIè¡Œãƒã‚¤ãƒ©ã‚¤ãƒˆé–‹å§‹")
            self._highlight_selected_result_safe(item_id)
            debug_logger.debug("ğŸ” [HIGHLIGHT_COMPLETE] UIè¡Œãƒã‚¤ãƒ©ã‚¤ãƒˆå®Œäº†")
            
            # 2. ãƒ•ã‚©ãƒ«ãƒ€ã‚’é–‹ã„ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆè¡¨ç¤ºï¼ˆé…å»¶å®Ÿè¡Œã§ç¢ºå®Ÿã«åˆ†é›¢ï¼‰
            debug_logger.debug("ğŸ” [DELAY_SCHEDULE] ã‚¨ã‚¯ã‚¹ãƒ—ãƒ­ãƒ¼ãƒ©èµ·å‹•ã‚’500mså¾Œã«ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«")
            
            def delayed_folder_open():
                """é…å»¶å®Ÿè¡Œã§ãƒ•ã‚©ãƒ«ãƒ€ã‚ªãƒ¼ãƒ—ãƒ³ï¼ˆé‡è¤‡é˜²æ­¢å¼·åŒ–ç‰ˆï¼‰"""
                try:
                    debug_logger.info("ğŸ” [DELAYED_OPEN_START] é…å»¶ãƒ•ã‚©ãƒ«ãƒ€ã‚ªãƒ¼ãƒ—ãƒ³é–‹å§‹")
                    
                    # å†åº¦ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèªï¼ˆé…å»¶å®Ÿè¡Œä¸­ã«ãƒ•ã‚¡ã‚¤ãƒ«ãŒç§»å‹•/å‰Šé™¤ã•ã‚ŒãŸå¯èƒ½æ€§ï¼‰
                    if not os.path.exists(file_path):
                        debug_logger.error(f"ğŸ” [FILE_GONE] ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªããªã‚Šã¾ã—ãŸ: {file_path}")
                        return
                    
                    # Explorerã§ãƒã‚¤ãƒ©ã‚¤ãƒˆè¡¨ç¤ºã‚’å®Ÿè¡Œ
                    self._open_folder_with_highlight(file_path)
                    debug_logger.info("ğŸ” [DELAYED_OPEN_COMPLETE] é…å»¶ãƒ•ã‚©ãƒ«ãƒ€ã‚ªãƒ¼ãƒ—ãƒ³å®Œäº†")
                    
                except Exception as delayed_error:
                    debug_logger.error(f"ğŸ” [DELAYED_OPEN_ERROR] é…å»¶ãƒ•ã‚©ãƒ«ãƒ€ã‚ªãƒ¼ãƒ—ãƒ³ã‚¨ãƒ©ãƒ¼: {delayed_error}")
                finally:
                    # çµ±åˆå‡¦ç†ãƒ•ãƒ©ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆ
                    self._integrated_processing = False
                    debug_logger.debug("ğŸ” [INTEGRATED_FLAG_RESET] çµ±åˆå‡¦ç†ãƒ•ãƒ©ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆ")
            
            # 500mså¾Œã«å®Ÿè¡Œï¼ˆUIã®å¿œç­”æ€§ã¨Explorerã®èµ·å‹•ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã‚’è€ƒæ…®ï¼‰
            self.root.after(500, delayed_folder_open)
            debug_logger.info("ğŸ” [EXPLORER_SCHEDULED] ã‚¨ã‚¯ã‚¹ãƒ—ãƒ­ãƒ¼ãƒ©èµ·å‹•ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å®Œäº†")
            
        except Exception as e:
            debug_logger.error(f"ğŸ” [INTEGRATED_ERROR] çµ±åˆãƒã‚¤ãƒ©ã‚¤ãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            print(f"âŒ çµ±åˆãƒã‚¤ãƒ©ã‚¤ãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            
            # çµ±åˆå‡¦ç†ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’å®Ÿè¡Œã—ãªã„ï¼ˆé‡è¤‡é˜²æ­¢ã®ãŸã‚ï¼‰
            debug_logger.warning("ğŸ” [NO_FALLBACK] ã‚¨ãƒ©ãƒ¼æ™‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆé‡è¤‡é˜²æ­¢ï¼‰")
            
        finally:
            # é…å»¶å®Ÿè¡Œã§ãªã„å ´åˆã¯å³åº§ã«ãƒ•ãƒ©ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆ
            if not hasattr(self, '_integrated_processing') or not self._integrated_processing:
                self._integrated_processing = False
            debug_logger.info("ğŸ” [INTEGRATED_COMPLETE] çµ±åˆãƒã‚¤ãƒ©ã‚¤ãƒˆå‡¦ç†å®Œäº†")

    def _highlight_selected_result_safe(self, item_id):
        """å®‰å…¨ãªæ¤œç´¢çµæœè¡Œãƒã‚¤ãƒ©ã‚¤ãƒˆè¡¨ç¤ºï¼ˆçµ±åˆç‰ˆï¼‰"""
        try:
            # å…ƒã®èƒŒæ™¯è‰²ã‚’ä¿å­˜
            original_tags = self.results_tree.item(item_id, 'tags')
            
            # ãƒã‚¤ãƒ©ã‚¤ãƒˆç”¨ã®ã‚¿ã‚°ã‚’è¨­å®š
            self.results_tree.tag_configure('highlight', background='#FFD700', foreground='#000000')  # é‡‘è‰²ã§ãƒã‚¤ãƒ©ã‚¤ãƒˆ
            self.results_tree.item(item_id, tags=['highlight'])
            
            print("âœ¨ æ¤œç´¢çµæœè¡Œã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆè¡¨ç¤ºã—ã¾ã—ãŸ")
            
            # 2ç§’å¾Œã«å…ƒã®è‰²ã«æˆ»ã™ï¼ˆå®‰å…¨ç‰ˆï¼‰
            def restore_color_safe():
                try:
                    if hasattr(self, 'results_tree') and self.results_tree.winfo_exists():
                        # ã‚¢ã‚¤ãƒ†ãƒ ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                        if item_id in self.results_tree.get_children():
                            self.results_tree.item(item_id, tags=original_tags)
                except Exception as restore_error:
                    print(f"âš ï¸ è‰²å¾©å…ƒå‡¦ç†ã‚¨ãƒ©ãƒ¼: {restore_error}")
                    
            self.root.after(2000, restore_color_safe)
            
        except Exception as e:
            print(f"âš ï¸ æ¤œç´¢çµæœãƒã‚¤ãƒ©ã‚¤ãƒˆè¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {e}")
    
    def _get_file_type_tag(self, file_ext: str) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ã«åŸºã¥ã„ã¦ã‚¿ã‚°ã‚’æ±ºå®š"""
        file_type_map = {
            '.txt': 'text',
            '.md': 'text',
            '.log': 'text',
            '.csv': 'text',
            '.json': 'text',
            '.doc': 'document',
            '.docx': 'document',
            '.dot': 'document',
            '.dotx': 'document',
            '.dotm': 'document',
            '.docm': 'document',
            '.rtf': 'document',
            '.odt': 'document',
            '.pdf': 'pdf',
            '.xls': 'excel',
            '.xlsx': 'excel',
            '.xlt': 'excel',
            '.xltx': 'excel',
            '.xltm': 'excel',
            '.xlsm': 'excel',
            '.xlsb': 'excel',
            '.ods': 'excel',
            '.ppt': 'powerpoint',
            '.pptx': 'powerpoint',
            '.odp': 'powerpoint',
            '.tif': 'image',
            '.tiff': 'image',
            '.png': 'image',
            '.jpg': 'image',
            '.jpeg': 'image',
            '.bmp': 'image',
            '.gif': 'image',
            '.zip': 'archive',
        }
        return file_type_map.get(file_ext, 'other')
    
    def _setup_file_type_colors(self):
        """ãƒ•ã‚¡ã‚¤ãƒ«ç¨®é¡ã«å¿œã˜ãŸè‰²è¨­å®š"""
        try:
            # ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã§èƒŒæ™¯è‰²ãƒ»æ–‡å­—è‰²ãªã—ï¼ˆæ¨™æº–è‰²ä½¿ç”¨ï¼‰
            self.results_tree.tag_configure('text')
            
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆæ¨™æº–è‰²ï¼‰
            self.results_tree.tag_configure('document')
            
            # PDFãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆæ¨™æº–è‰²ï¼‰
            self.results_tree.tag_configure('pdf')
            
            # Excelãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆæ¨™æº–è‰²ï¼‰
            self.results_tree.tag_configure('excel')
            
            # PowerPointãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆæ¨™æº–è‰²ï¼‰
            self.results_tree.tag_configure('powerpoint')
            
            # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆæ¨™æº–è‰²ï¼‰
            self.results_tree.tag_configure('image')
            
            # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆæ¨™æº–è‰²ï¼‰
            self.results_tree.tag_configure('archive')
            
            # ãã®ä»–ï¼ˆæ¨™æº–è‰²ï¼‰
            self.results_tree.tag_configure('other')
            
            # ãƒã‚¤ãƒ©ã‚¤ãƒˆç”¨ï¼ˆé‡‘è‰²èƒŒæ™¯ã¯ç¶­æŒã€é¸æŠæ™‚ã®ãƒã‚¤ãƒ©ã‚¤ãƒˆåŠ¹æœï¼‰
            self.results_tree.tag_configure('highlight', background='#FFD700', foreground='#000000')
            
        except Exception as e:
            print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ç¨®é¡è‰²è¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
    
    def _on_tree_motion(self, event):
        """ãƒ„ãƒªãƒ¼ãƒ“ãƒ¥ãƒ¼ã§ã®ãƒã‚¦ã‚¹ãƒ›ãƒãƒ¼åŠ¹æœ"""
        try:
            # ãƒã‚¦ã‚¹ä½ç½®ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’ç‰¹å®š
            item_id = self.results_tree.identify_row(event.y)
            
            # å‰å›ãƒ›ãƒãƒ¼ã—ã¦ã„ãŸã‚¢ã‚¤ãƒ†ãƒ ã®å¼·èª¿ã‚’è§£é™¤
            if hasattr(self, '_hovered_item') and self._hovered_item != item_id:
                self._clear_hover_highlight(self._hovered_item)
            
            # æ–°ã—ã„ã‚¢ã‚¤ãƒ†ãƒ ã‚’å¼·èª¿
            if item_id and item_id != getattr(self, '_hovered_item', None):
                self._apply_hover_highlight(item_id)
                self._hovered_item = item_id
                
                # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒãƒ¼ã«è¡¨ç¤º
                item_values = self.results_tree.item(item_id, 'values')
                if len(item_values) >= 3:
                    file_name = item_values[1]
                    file_path = item_values[2]
                    self.root.title(f"100%ä»•æ§˜é©åˆ è¶…é«˜é€Ÿãƒ©ã‚¤ãƒ–å…¨æ–‡æ¤œç´¢ã‚¢ãƒ—ãƒª - ãƒ›ãƒãƒ¼ä¸­: {file_name}")
                    
        except Exception as e:
            pass  # ãƒ›ãƒãƒ¼åŠ¹æœã®ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–
    
    def _on_tree_leave(self, event):
        """ãƒ„ãƒªãƒ¼ãƒ“ãƒ¥ãƒ¼ã‹ã‚‰ãƒã‚¦ã‚¹ãŒé›¢ã‚ŒãŸæ™‚ã®å‡¦ç†"""
        try:
            # ãƒ›ãƒãƒ¼å¼·èª¿ã‚’è§£é™¤
            if hasattr(self, '_hovered_item'):
                self._clear_hover_highlight(self._hovered_item)
                del self._hovered_item
                
            # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å…ƒã«æˆ»ã™
            self.root.title("100%ä»•æ§˜é©åˆ è¶…é«˜é€Ÿãƒ©ã‚¤ãƒ–å…¨æ–‡æ¤œç´¢ã‚¢ãƒ—ãƒª")
            
        except Exception as e:
            pass  # ãƒ›ãƒãƒ¼åŠ¹æœã®ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–
    
    def _apply_hover_highlight(self, item_id):
        """ã‚¢ã‚¤ãƒ†ãƒ ã«ãƒ›ãƒãƒ¼å¼·èª¿ã‚’é©ç”¨"""
        try:
            # ç¾åœ¨ã®ã‚¿ã‚°ã‚’å–å¾—
            current_tags = self.results_tree.item(item_id, 'tags')
            
            # ãƒ›ãƒãƒ¼åŠ¹æœç”¨ã®ã‚¿ã‚°ã‚’è¨­å®šï¼ˆæ¨™æº–è‰²ä½¿ç”¨ï¼‰
            self.results_tree.tag_configure('hover')
            
            # ãƒ›ãƒãƒ¼ã‚¿ã‚°ã‚’è¿½åŠ 
            new_tags = list(current_tags) if current_tags else []
            if 'hover' not in new_tags:
                new_tags.append('hover')
                self.results_tree.item(item_id, tags=new_tags)
                
        except Exception as e:
            pass
    
    def _clear_hover_highlight(self, item_id):
        """ã‚¢ã‚¤ãƒ†ãƒ ã‹ã‚‰ãƒ›ãƒãƒ¼å¼·èª¿ã‚’è§£é™¤"""
        try:
            # ç¾åœ¨ã®ã‚¿ã‚°ã‚’å–å¾—
            current_tags = self.results_tree.item(item_id, 'tags')
            
            if current_tags and 'hover' in current_tags:
                # ãƒ›ãƒãƒ¼ã‚¿ã‚°ã‚’é™¤å»
                new_tags = [tag for tag in current_tags if tag != 'hover']
                self.results_tree.item(item_id, tags=new_tags)
                
        except Exception as e:
            pass

    def show_context_menu(self, event):
        """ğŸ¨ å³ã‚¯ãƒªãƒƒã‚¯ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ¡ãƒ‹ãƒ¥ãƒ¼è¡¨ç¤º"""
        selection = self.results_tree.selection()
        if not selection:
            return

        item = self.results_tree.item(selection[0])
        file_path = item['values'][2]  # ãƒ‘ã‚¹åˆ—

        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ¡ãƒ‹ãƒ¥ãƒ¼ä½œæˆ
        context_menu = tk.Menu(self.root, tearoff=0)

        context_menu.add_command(label="ï¿½ ãƒ•ã‚©ãƒ«ãƒ€ã‚’é–‹ã„ã¦ãƒã‚¤ãƒ©ã‚¤ãƒˆè¡¨ç¤º",
                                 command=lambda: self._open_file_directly(file_path))

        context_menu.add_command(label="ğŸ“‹ ãƒ‘ã‚¹ã‚’ã‚³ãƒ”ãƒ¼",
                                 command=lambda: self._copy_path_to_clipboard(file_path))

        # ãƒ¡ãƒ‹ãƒ¥ãƒ¼è¡¨ç¤º
        try:
            context_menu.tk_popup(event.x_root, event.y_root)
        finally:
            context_menu.grab_release()

    def _open_file_directly(self, file_path):
        """ğŸ“– ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ãï¼ˆPDFã¨åŒã˜ã‚ˆã†ã«ãƒ•ã‚©ãƒ«ãƒ€ãƒã‚¤ãƒ©ã‚¤ãƒˆè¡¨ç¤ºï¼‰"""
        try:
            if os.path.exists(file_path):
                debug_logger.info(f"ğŸ“– ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ãè¦æ±‚: {os.path.basename(file_path)}")
                print(f"ğŸ¯ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆè¡¨ç¤ºã—ã¾ã™: {os.path.basename(file_path)}")
                
                # PDFã¨åŒã˜ã‚ˆã†ã«ãƒ•ã‚©ãƒ«ãƒ€ã‚’é–‹ã„ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆè¡¨ç¤º
                self._open_folder_with_highlight(file_path)
                
            else:
                messagebox.showwarning("è­¦å‘Š", "ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        except Exception as e:
            messagebox.showerror("ã‚¨ãƒ©ãƒ¼", f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã‘ã¾ã›ã‚“ã§ã—ãŸ: {e}")
            debug_logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«é–‹ãå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")

    def _open_folder_with_highlight(self, file_path):
        """ğŸ“‚ ãƒ•ã‚©ãƒ«ãƒ€ã‚’é–‹ã„ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆãƒ»é‡è¤‡é˜²æ­¢ï¼‰"""
        
        import os
        import webbrowser
        import subprocess
        import time
        
        # ã‚·ãƒ³ãƒ—ãƒ«ãªé‡è¤‡é˜²æ­¢
        current_time = time.time()
        last_request_time = getattr(self, '_last_folder_open_time', 0)
        
        if current_time - last_request_time < 1.5:  # 1.5ç§’ä»¥å†…ã®é‡è¤‡ã‚’ãƒ–ãƒ­ãƒƒã‚¯
            time_diff = current_time - last_request_time
            debug_logger.warning(f"ğŸ“‚ ãƒ•ã‚©ãƒ«ãƒ€ã‚ªãƒ¼ãƒ—ãƒ³é‡è¤‡é˜²æ­¢: {time_diff:.3f}ç§’ä»¥å†…ã®é‡è¤‡è¦æ±‚")
            print(f"ğŸš« ãƒ•ã‚©ãƒ«ãƒ€ã‚ªãƒ¼ãƒ—ãƒ³é‡è¤‡ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆ{time_diff:.3f}ç§’ï¼‰")
            return
        
        self._last_folder_open_time = current_time
        
        debug_logger.info(f"ğŸ“‚ ãƒ•ã‚©ãƒ«ãƒ€ã‚ªãƒ¼ãƒ—ãƒ³è¦æ±‚: {file_path}")

        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª
            if not os.path.exists(file_path):
                debug_logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {file_path}")
                messagebox.showwarning("è­¦å‘Š", f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“:\n{file_path}")
                return

            folder_path = os.path.dirname(file_path)
            
            # æ–¹æ³•1: Explorerã®/selectãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆè¡¨ç¤º
            try:
                debug_logger.info(f"ğŸ” Explorerã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆè¡¨ç¤º: {file_path}")
                # å¼•æ•°ã‚’ãƒªã‚¹ãƒˆã®åˆ¥è¦ç´ ã¨ã—ã¦æ¸¡ã™ï¼ˆå¼•ç”¨ç¬¦ã¯è‡ªå‹•å‡¦ç†ã•ã‚Œã‚‹ï¼‰
                result = subprocess.run(['explorer', '/select,', file_path], 
                                      check=False,
                                      capture_output=True,
                                      creationflags=subprocess.CREATE_NO_WINDOW)
                if result.returncode == 0:
                    debug_logger.info("âœ… Explorerãƒã‚¤ãƒ©ã‚¤ãƒˆè¡¨ç¤ºæˆåŠŸ")
                    print(f"ğŸ¯ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆè¡¨ç¤ºã—ã¾ã—ãŸ: {os.path.basename(file_path)}")
                    return
                else:
                    if result.stderr:
                        debug_logger.warning(f"Explorer stderr: {result.stderr.decode('utf-8', errors='ignore')}")
                    debug_logger.warning(f"Exploreræˆ»ã‚Šå€¤: {result.returncode}")
                
            except Exception as highlight_error:
                debug_logger.warning(f"Explorer/selectãƒã‚¤ãƒ©ã‚¤ãƒˆè¡¨ç¤ºå¤±æ•—: {highlight_error}")
            
            # æ–¹æ³•2: os.startfile()ã§ãƒ•ã‚©ãƒ«ãƒ€ã‚’é–‹ãï¼ˆä»£æ›¿æ‰‹æ®µï¼‰
            try:
                debug_logger.info(f"ğŸ” os.startfile()ã§ãƒ•ã‚©ãƒ«ãƒ€ã‚’é–‹ã: {folder_path}")
                os.startfile(folder_path)
                debug_logger.info("âœ… os.startfile()æˆåŠŸ")
                print(f"ğŸ“‚ ãƒ•ã‚©ãƒ«ãƒ€ã‚’é–‹ãã¾ã—ãŸ: {os.path.basename(folder_path)}")
                return
                
            except Exception as startfile_error:
                debug_logger.warning(f"os.startfile()å¤±æ•—: {startfile_error}")
            
            # æ–¹æ³•3: webbrowserã§ãƒ•ã‚©ãƒ«ãƒ€ã‚’é–‹ãï¼ˆæœ€å¾Œã®æ‰‹æ®µï¼‰
            try:
                folder_uri = f"file:///{folder_path.replace(os.sep, '/')}"
                debug_logger.info(f"ğŸŒ webbrowserã§ãƒ•ã‚©ãƒ«ãƒ€ã‚’é–‹ã: {folder_uri}")
                webbrowser.open(folder_uri)
                debug_logger.info("âœ… webbrowseræˆåŠŸ")
                print(f"ğŸ“‚ ãƒ•ã‚©ãƒ«ãƒ€ã‚’é–‹ãã¾ã—ãŸ: {os.path.basename(folder_path)}")
                return
                
            except Exception as webbrowser_error:
                debug_logger.warning(f"webbrowserå¤±æ•—: {webbrowser_error}")
            
        except Exception as e:
            debug_logger.error(f"ãƒ•ã‚©ãƒ«ãƒ€ã‚ªãƒ¼ãƒ—ãƒ³ã‚¨ãƒ©ãƒ¼: {e}")
            messagebox.showerror("ã‚¨ãƒ©ãƒ¼", f"ãƒ•ã‚©ãƒ«ãƒ€ã‚’é–‹ã‘ã¾ã›ã‚“ã§ã—ãŸ: {e}")

        finally:
            # é‡è¤‡é˜²æ­¢ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ãƒªã‚»ãƒƒãƒˆ
            self._last_folder_open_time = time.time()

    def _reset_folder_opening_flag(self):
        """ãƒ•ã‚©ãƒ«ãƒ€ã‚ªãƒ¼ãƒ—ãƒ³ãƒ•ãƒ©ã‚°ãƒªã‚»ãƒƒãƒˆï¼ˆç°¡ç´ åŒ–ç‰ˆï¼‰"""
        try:
            # ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ•ãƒ©ã‚°ãƒªã‚»ãƒƒãƒˆã®ã¿
            if hasattr(self, '_last_folder_open_time'):
                debug_logger.debug("ãƒ•ã‚©ãƒ«ãƒ€ã‚ªãƒ¼ãƒ—ãƒ³ãƒ•ãƒ©ã‚°ãƒªã‚»ãƒƒãƒˆå®Œäº†")
        except Exception as e:
            debug_logger.error(f"ãƒ•ãƒ©ã‚°ãƒªã‚»ãƒƒãƒˆã‚¨ãƒ©ãƒ¼: {e}")

    def _copy_path_to_clipboard(self, file_path):
        """ğŸ“‹ ãƒ‘ã‚¹ã‚’ã‚¯ãƒªãƒƒãƒ—ãƒœãƒ¼ãƒ‰ã«ã‚³ãƒ”ãƒ¼"""
        try:
            self.root.clipboard_clear()
            self.root.clipboard_append(file_path)
            print(f"ğŸ“‹ ãƒ‘ã‚¹ã‚’ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸ: {os.path.basename(file_path)}")
        except Exception as e:
            messagebox.showerror("ã‚¨ãƒ©ãƒ¼", f"ãƒ‘ã‚¹ã‚’ã‚³ãƒ”ãƒ¼ã§ãã¾ã›ã‚“ã§ã—ãŸ: {e}")

    def index_folder(self):
        """ãƒ•ã‚©ãƒ«ãƒ€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆé«˜é€ŸåŒ–ç‰ˆï¼‰"""
        print("ğŸ” ãƒ•ã‚©ãƒ«ãƒ€é¸æŠãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã‚’é–‹å§‹...")
        debug_logger.info("ãƒ•ã‚©ãƒ«ãƒ€é¸æŠãƒ€ã‚¤ã‚¢ãƒ­ã‚°é–‹å§‹")
        
        # ãƒ•ã‚©ãƒ«ãƒ€é¸æŠï¼ˆå³åº§å®Ÿè¡Œï¼‰
        folder = filedialog.askdirectory(title="ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å¯¾è±¡ãƒ•ã‚©ãƒ«ãƒ€ã‚’é¸æŠ")
        
        if folder:
            print(f"ğŸ“ ãƒ•ã‚©ãƒ«ãƒ€é¸æŠå®Œäº†: {folder}")
            debug_logger.info(f"é¸æŠã•ã‚ŒãŸãƒ•ã‚©ãƒ«ãƒ€: {folder}")
            
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã‚’å³åº§ã«è¡¨ç¤ºï¼ˆãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚«ã‚¦ãƒ³ãƒˆå‰ï¼‰
            progress_window = tk.Toplevel(self.root)
            progress_window.title("ğŸ“ ãƒ•ã‚©ãƒ«ãƒ€åˆ†æä¸­")
            progress_window.geometry("450x150")
            progress_window.transient(self.root)
            progress_window.grab_set()

            progress_label = ttk.Label(progress_window, text="ãƒ•ã‚©ãƒ«ãƒ€ã‚’åˆ†æä¸­...")
            progress_label.pack(expand=True, pady=10)

            progress_bar = ttk.Progressbar(progress_window, mode='indeterminate')
            progress_bar.pack(fill=tk.X, padx=20, pady=10)
            progress_bar.start()
            
            # ã‚­ãƒ£ãƒ³ã‚»ãƒ«ãƒœã‚¿ãƒ³è¿½åŠ 
            cancel_flag = {"cancelled": False}
            
            def cancel_analysis():
                cancel_flag["cancelled"] = True
                progress_window.destroy()
                print("âŒ ãƒ•ã‚©ãƒ«ãƒ€åˆ†æãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ")
            
            cancel_button = ttk.Button(progress_window, text="ã‚­ãƒ£ãƒ³ã‚»ãƒ«", command=cancel_analysis)
            cancel_button.pack(pady=5)

            print("ğŸ”„ ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒ€ã‚¤ã‚¢ãƒ­ã‚°è¡¨ç¤ºå®Œäº†")

            # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚«ã‚¦ãƒ³ãƒˆã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å®Ÿè¡Œ
            def background_analysis_process():
                try:
                    if cancel_flag["cancelled"]:
                        return
                    
                    print("ğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚«ã‚¦ãƒ³ãƒˆé–‹å§‹ï¼ˆãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ï¼‰")
                    
                    # é«˜é€Ÿãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚«ã‚¦ãƒ³ãƒˆï¼ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ–¹å¼ï¼‰
                    file_count = self._fast_file_count(folder)
                    
                    if cancel_flag["cancelled"]:
                        return
                        
                    print(f"ğŸ“Š æ¨å®šãƒ•ã‚¡ã‚¤ãƒ«æ•°: {file_count}å€‹")
                    debug_logger.info(f"æ¨å®šãƒ•ã‚¡ã‚¤ãƒ«æ•°: {file_count}å€‹")
                    
                    # UIæ›´æ–°ï¼ˆç¢ºèªãƒ€ã‚¤ã‚¢ãƒ­ã‚°ï¼‰
                    self.root.after(0, lambda: self._show_index_confirmation(
                        folder, file_count, progress_window, cancel_flag))
                        
                except Exception as e:
                    print(f"âŒ ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    debug_logger.error(f"ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    if not cancel_flag["cancelled"]:
                        self.root.after(0, lambda: progress_window.destroy())
                        self.root.after(0, lambda: messagebox.showerror("ã‚¨ãƒ©ãƒ¼", f"å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}"))

            # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç†é–‹å§‹
            threading.Thread(target=background_analysis_process, daemon=True).start()
            
        else:
            print("âŒ ãƒ•ã‚©ãƒ«ãƒ€ãŒé¸æŠã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
            debug_logger.info("ãƒ•ã‚©ãƒ«ãƒ€é¸æŠã‚­ãƒ£ãƒ³ã‚»ãƒ«")

    def _fast_file_count(self, folder_path: str) -> int:
        """é«˜é€Ÿãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚«ã‚¦ãƒ³ãƒˆï¼ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ–¹å¼ï¼‰"""
        try:
            supported_extensions = {'.txt', '.pdf', '.docx', '.xlsx', '.tif', '.tiff',
                                   '.doc', '.xls', '.ppt', '.pptx',
                                   '.dot', '.dotx', '.dotm', '.docm',  # Wordé–¢é€£è¿½åŠ 
                                   '.xlt', '.xltx', '.xltm', '.xlsm', '.xlsb',  # Excelé–¢é€£è¿½åŠ 
                                   '.jwc', '.dxf', '.sfc', '.jww', '.dwg', '.dwt', '.mpp', '.mpz',  # CAD/å›³é¢ãƒ•ã‚¡ã‚¤ãƒ«è¿½åŠ 
                                   '.zip'}  # ZIPãƒ•ã‚¡ã‚¤ãƒ«è¿½åŠ 
            
            # å°ã•ãªãƒ•ã‚©ãƒ«ãƒ€ã¯å…¨ã‚«ã‚¦ãƒ³ãƒˆ
            total_items = 0
            sample_count = 0
            supported_count = 0
            
            # æœ€åˆã®200å€‹ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            for root, dirs, files in os.walk(folder_path):
                for file in files:
                    total_items += 1
                    if sample_count < 200:
                        if any(file.lower().endswith(ext) for ext in supported_extensions):
                            supported_count += 1
                        sample_count += 1
                    elif total_items > 2000:  # å¤§ããªãƒ•ã‚©ãƒ«ãƒ€ã¯æ¨å®š
                        break
                if total_items > 2000:
                    break
            
            # æ¨å®šè¨ˆç®—
            if sample_count < 200:
                # å°ã•ãªãƒ•ã‚©ãƒ«ãƒ€ã¯æ­£ç¢ºãªæ•°
                return supported_count
            else:
                # å¤§ããªãƒ•ã‚©ãƒ«ãƒ€ã¯æ¯”ç‡ã§æ¨å®š
                ratio = supported_count / sample_count if sample_count > 0 else 0
                estimated = int(total_items * ratio)
                return max(estimated, supported_count)  # æœ€ä½ã§ã‚‚ã‚µãƒ³ãƒ—ãƒ«ã§è¦‹ã¤ã‹ã£ãŸæ•°
                
        except Exception as e:
            print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚«ã‚¦ãƒ³ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return 0

    def _show_index_confirmation(self, folder: str, file_count: int, progress_window: tk.Toplevel, cancel_flag: dict):
        """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç¢ºèªãƒ€ã‚¤ã‚¢ãƒ­ã‚°è¡¨ç¤º"""
        try:
            progress_window.destroy()
            
            if cancel_flag["cancelled"]:
                return
                
            # ç¢ºèªãƒ€ã‚¤ã‚¢ãƒ­ã‚°
            folder_name = os.path.basename(folder) or folder
            if messagebox.askyesno("ğŸ“ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç¢ºèª", 
                                   f"ãƒ•ã‚©ãƒ«ãƒ€ '{folder_name}' ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã—ã¾ã™ã‹ï¼Ÿ\n\n"
                                   f"ğŸ“Š æ¨å®šãƒ•ã‚¡ã‚¤ãƒ«æ•°: {file_count:,}å€‹\n"
                                   f"ğŸ“ ãƒ‘ã‚¹: {folder}\n\n"
                                   "âš¡ ä¸¦åˆ—å‡¦ç†ã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã—ã¾ã™ã€‚\n"
                                   "ğŸ’¡ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸­ã‚‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰æ¤œç´¢å¯èƒ½ã§ã™ã€‚"):

                print("âœ… ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‡¦ç†ã‚’æ‰¿èª")
                debug_logger.info("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‡¦ç†é–‹å§‹ - ãƒ¦ãƒ¼ã‚¶ãƒ¼æ‰¿èªæ¸ˆã¿")
                
                # å®Ÿéš›ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‡¦ç†é–‹å§‹
                self._start_actual_indexing(folder, file_count)
                
            else:
                print("âŒ ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‡¦ç†ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«")
                debug_logger.info("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‡¦ç†ã‚­ãƒ£ãƒ³ã‚»ãƒ«")
                
        except Exception as e:
            print(f"âŒ ç¢ºèªãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
            messagebox.showerror("ã‚¨ãƒ©ãƒ¼", f"ç¢ºèªãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã‚¨ãƒ©ãƒ¼: {e}")

    def create_realtime_progress_window(self, title: str = "ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å®Ÿè¡Œä¸­") -> tk.Toplevel:
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²æ—è¡¨ç¤ºã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’ä½œæˆ"""
        progress_window = tk.Toplevel(self.root)
        progress_window.title(f"ğŸ“ {title}")
        progress_window.geometry("700x400")
        progress_window.transient(self.root)
        progress_window.grab_set()
        progress_window.resizable(True, True)

        # ãƒ¡ã‚¤ãƒ³ãƒ•ãƒ¬ãƒ¼ãƒ 
        main_frame = ttk.Frame(progress_window, padding=10)
        main_frame.pack(fill=tk.BOTH, expand=True)

        # ã‚¿ã‚¤ãƒˆãƒ«ãƒ©ãƒ™ãƒ«
        title_label = ttk.Label(main_frame, text=f"ğŸ“ {title}", font=("", 12, "bold"))
        title_label.pack(anchor=tk.W, pady=(0, 10))

        # å…¨ä½“é€²æ—ãƒãƒ¼
        progress_frame = ttk.Frame(main_frame)
        progress_frame.pack(fill=tk.X, pady=(0, 10))
        
        ttk.Label(progress_frame, text="å…¨ä½“é€²æ—:").pack(anchor=tk.W)
        progress_bar = ttk.Progressbar(progress_frame, mode='determinate')
        progress_bar.pack(fill=tk.X, pady=(2, 5))
        
        # é€²æ—ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«
        progress_percent_label = ttk.Label(progress_frame, text="0%")
        progress_percent_label.pack(anchor=tk.W)

        # çµ±è¨ˆæƒ…å ±ãƒ•ãƒ¬ãƒ¼ãƒ 
        stats_frame = ttk.LabelFrame(main_frame, text="ğŸ“Š å‡¦ç†çµ±è¨ˆ", padding=5)
        stats_frame.pack(fill=tk.X, pady=(0, 10))
        
        stats_grid = ttk.Frame(stats_frame)
        stats_grid.pack(fill=tk.X)
        
        # çµ±è¨ˆãƒ©ãƒ™ãƒ«ï¼ˆ2åˆ—ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆï¼‰
        stats_labels = {}
        stats_items = [
            ("processed", "å‡¦ç†æ¸ˆã¿:"), ("total", "ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°:"),
            ("success", "æˆåŠŸ:"), ("error", "ã‚¨ãƒ©ãƒ¼:"),
            ("speed", "å‡¦ç†é€Ÿåº¦:"), ("remaining", "æ®‹ã‚Šæ™‚é–“:"),
        ]
        
        for i, (key, text) in enumerate(stats_items):
            row = i // 2
            col = i % 2
            
            label_frame = ttk.Frame(stats_grid)
            label_frame.grid(row=row, column=col, sticky="w", padx=(0, 20), pady=2)
            
            ttk.Label(label_frame, text=text, width=10).pack(side=tk.LEFT)
            stats_labels[key] = ttk.Label(label_frame, text="0", font=("", 9))
            stats_labels[key].pack(side=tk.LEFT)

        # ã‚«ãƒ†ã‚´ãƒªåˆ¥é€²æ—
        category_frame = ttk.LabelFrame(main_frame, text="ğŸ“‚ ã‚«ãƒ†ã‚´ãƒªåˆ¥é€²æ—", padding=5)
        category_frame.pack(fill=tk.X, pady=(0, 10))
        
        category_labels = {}
        category_bars = {}
        for category, emoji in [("light", "ğŸ“„"), ("medium", "ğŸ“‹"), ("heavy", "ğŸ“¦")]:
            cat_frame = ttk.Frame(category_frame)
            cat_frame.pack(fill=tk.X, pady=2)
            
            ttk.Label(cat_frame, text=f"{emoji} {category.title()}ãƒ•ã‚¡ã‚¤ãƒ«:", width=15).pack(side=tk.LEFT)
            category_bars[category] = ttk.Progressbar(cat_frame, mode='determinate', length=200)
            category_bars[category].pack(side=tk.LEFT, padx=(5, 10))
            category_labels[category] = ttk.Label(cat_frame, text="0/0")
            category_labels[category].pack(side=tk.LEFT)

        # ç¾åœ¨å‡¦ç†ä¸­ãƒ•ã‚¡ã‚¤ãƒ«è¡¨ç¤º
        current_frame = ttk.LabelFrame(main_frame, text="ğŸ” ç¾åœ¨å‡¦ç†ä¸­", padding=5)
        current_frame.pack(fill=tk.BOTH, expand=True)
        
        current_file_text = tk.Text(current_frame, height=3, wrap=tk.WORD, font=("", 9))
        current_scrollbar = ttk.Scrollbar(current_frame, orient=tk.VERTICAL, command=current_file_text.yview)
        current_file_text.configure(yscrollcommand=current_scrollbar.set)
        
        current_file_text.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        current_scrollbar.pack(side=tk.RIGHT, fill=tk.Y)

        # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã¨ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆã®å‚ç…§ã‚’ä¿å­˜
        progress_window.progress_bar = progress_bar
        progress_window.progress_percent_label = progress_percent_label
        progress_window.stats_labels = stats_labels
        progress_window.category_labels = category_labels
        progress_window.category_bars = category_bars
        progress_window.current_file_text = current_file_text

        return progress_window

    def update_progress_window(self):
        """é€²æ—ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’æ›´æ–°"""
        if not self.progress_window or not self.progress_window.winfo_exists():
            return
            
        try:
            progress_info = self.progress_tracker.get_progress_info()
            
            # å…¨ä½“é€²æ—ãƒãƒ¼æ›´æ–°
            progress_percent = progress_info['progress_percent']
            self.progress_window.progress_bar['value'] = progress_percent
            self.progress_window.progress_percent_label.config(text=f"{progress_percent:.1f}%")
            
            # çµ±è¨ˆæƒ…å ±æ›´æ–°
            stats_labels = self.progress_window.stats_labels
            stats_labels['processed'].config(text=f"{progress_info['processed_files']:,}")
            stats_labels['total'].config(text=f"{progress_info['total_files']:,}")
            stats_labels['success'].config(text=f"{progress_info['successful_files']:,}")
            stats_labels['error'].config(text=f"{progress_info['error_files']:,}")
            stats_labels['speed'].config(text=f"{progress_info['processing_speed']:.1f} files/sec")
            
            # æ®‹ã‚Šæ™‚é–“
            remaining_time = progress_info['estimated_remaining_time']
            if remaining_time > 3600:
                time_text = f"{remaining_time/3600:.1f}h"
            elif remaining_time > 60:
                time_text = f"{remaining_time/60:.1f}min"
            else:
                time_text = f"{remaining_time:.1f}sec"
            stats_labels['remaining'].config(text=time_text)
            
            # ã‚«ãƒ†ã‚´ãƒªåˆ¥é€²æ—æ›´æ–°
            for category in ['light', 'medium', 'heavy']:
                total = progress_info['category_totals'].get(category, 0)
                processed = progress_info['category_progress'].get(category, 0)
                
                if total > 0:
                    percent = (processed / total) * 100
                    self.progress_window.category_bars[category]['value'] = percent
                    self.progress_window.category_labels[category].config(text=f"{processed}/{total}")
                
            # ç¾åœ¨å‡¦ç†ä¸­ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°
            current_file = progress_info['current_file']
            if current_file:
                # ãƒ•ã‚¡ã‚¤ãƒ«åã ã‘è¡¨ç¤ºï¼ˆãƒ‘ã‚¹ãŒé•·ã„å ´åˆï¼‰
                display_name = os.path.basename(current_file)
                if len(display_name) > 50:
                    display_name = display_name[:47] + "..."
                    
                current_text = f"ğŸ“„ {display_name}\nğŸ“ {os.path.dirname(current_file)}"
                
                self.progress_window.current_file_text.delete(1.0, tk.END)
                self.progress_window.current_file_text.insert(tk.END, current_text)
            
            # æ¬¡å›æ›´æ–°ã‚’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆ0.5ç§’é–“éš”ã§ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°ï¼‰
            self.root.after(500, self.update_progress_window)
            
        except Exception as e:
            print(f"âš ï¸ é€²æ—ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")

    def categorize_files_by_size_fast_ui_safe(self, files):
        """UIå¿œç­”æ€§ã‚’é‡è¦–ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºåˆ†é¡ï¼ˆè¶…é«˜é€Ÿä¸¦åˆ—ç‰ˆï¼‰"""
        light_files = []    # <10MB
        medium_files = []   # 10MB-100MB  
        heavy_files = []    # >100MB
        
        print(f"âš¡ è¶…é«˜é€Ÿãƒ•ã‚¡ã‚¤ãƒ«åˆ†é¡é–‹å§‹: {len(files):,}ãƒ•ã‚¡ã‚¤ãƒ«")
        start_time = time.time()
        
        # å°è¦æ¨¡ãƒ•ã‚¡ã‚¤ãƒ«ç¾¤ã¯å¾“æ¥å‡¦ç†ï¼ˆé€Ÿåº¦é‡è¦–ï¼‰
        if len(files) <= 5000:
            for file_path in files:
                try:
                    size_bytes = Path(file_path).stat().st_size
                    if size_bytes < 10 * 1024 * 1024:  # 10MB
                        light_files.append(file_path)
                    elif size_bytes < 100 * 1024 * 1024:  # 100MB
                        medium_files.append(file_path)
                    else:
                        heavy_files.append(file_path)
                except:
                    light_files.append(file_path)  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯è»½é‡æ‰±ã„
        else:
            # å¤§è¦æ¨¡ãƒ•ã‚¡ã‚¤ãƒ«ç¾¤ã¯ä¸¦åˆ—å‡¦ç†ï¼ˆ2000ãƒ•ã‚¡ã‚¤ãƒ«/ç§’å¯¾å¿œï¼‰
            import threading
            
            # ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ãªãƒªã‚¹ãƒˆ
            light_lock = threading.Lock()
            medium_lock = threading.Lock()
            heavy_lock = threading.Lock()
            
            def categorize_batch(batch_files):
                batch_light, batch_medium, batch_heavy = [], [], []
                
                for file_path in batch_files:
                    try:
                        size_bytes = Path(file_path).stat().st_size
                        if size_bytes < 10 * 1024 * 1024:  # 10MB
                            batch_light.append(file_path)
                        elif size_bytes < 100 * 1024 * 1024:  # 100MB
                            batch_medium.append(file_path)
                        else:
                            batch_heavy.append(file_path)
                    except:
                        batch_light.append(file_path)  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯è»½é‡æ‰±ã„
                
                # ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ã«çµæœã‚’ãƒãƒ¼ã‚¸
                with light_lock:
                    light_files.extend(batch_light)
                with medium_lock:
                    medium_files.extend(batch_medium)
                with heavy_lock:
                    heavy_files.extend(batch_heavy)
            
            # ä¸¦åˆ—ãƒãƒƒãƒå‡¦ç†ï¼ˆé«˜é€ŸåŒ–ï¼‰
            batch_size = min(1000, max(200, len(files) // (self.search_system.optimal_threads * 2)))
            threads = []
            
            for i in range(0, len(files), batch_size):
                batch = files[i:i+batch_size]
                thread = threading.Thread(target=categorize_batch, args=(batch,))
                threads.append(thread)
                thread.start()
                
                # ä¸¦åˆ—åº¦åˆ¶é™ï¼ˆã‚·ã‚¹ãƒ†ãƒ è² è·è€ƒæ…®ï¼‰
                if len(threads) >= self.search_system.optimal_threads:
                    for t in threads:
                        t.join()
                    threads = []
            
            # æ®‹ã‚Šã®ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’å¾…æ©Ÿ
            for t in threads:
                t.join()
        
        categorize_time = time.time() - start_time
        print(f"âœ… è¶…é«˜é€Ÿãƒ•ã‚¡ã‚¤ãƒ«åˆ†é¡å®Œäº†: {categorize_time:.2f}ç§’ - è»½é‡{len(light_files):,}, ä¸­{len(medium_files):,}, é‡{len(heavy_files):,}")
        
        return light_files, medium_files, heavy_files

    def process_single_file_with_progress(self, file_path: str, category: str):
        """å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ï¼ˆé€²æ—ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ä»˜ãï¼‰"""
        try:
            # é€²æ—ãƒˆãƒ©ãƒƒã‚«ãƒ¼æ›´æ–°
            self.progress_tracker.update_progress(current_file=file_path, category=category, success=True)
            
            # å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
            result = self.search_system.live_progressive_index_file(file_path)
            
            return result
        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼ã‚‚é€²æ—ã«è¨˜éŒ²
            self.progress_tracker.update_progress(current_file=file_path, category=category, success=False)
            return None

    def _start_actual_indexing(self, folder: str, estimated_count: int):
        """å®Ÿéš›ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‡¦ç†é–‹å§‹ï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²æ—å¯¾å¿œï¼‰"""
        try:
            # é€²æ—ãƒˆãƒ©ãƒƒã‚«ãƒ¼ãƒªã‚»ãƒƒãƒˆ
            self.progress_tracker.reset()
            
            # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²æ—ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’ä½œæˆ
            folder_name = os.path.basename(folder) or folder
            self.progress_window = self.create_realtime_progress_window(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å®Ÿè¡Œä¸­ - {folder_name}")
            
            print("ğŸ“ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²æ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‡¦ç†é–‹å§‹...")

            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‡¦ç†ã‚¹ãƒ¬ãƒƒãƒ‰
            def indexing_thread():
                try:
                    start_time = time.time()
                    print("ğŸš€ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²æ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹")
                    
                    # é€²æ—ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æ›´æ–°ã‚’é–‹å§‹
                    self.root.after(500, self.update_progress_window)
                    
                    print(f"ğŸ“‚ bulk_index_directory_with_progresså‘¼ã³å‡ºã—å‰ - å¯¾è±¡: {folder}")
                    
                    # é€²æ—ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°æ©Ÿèƒ½ä»˜ãã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‡¦ç†ã‚’å®Ÿè¡Œ
                    result = self.search_system.bulk_index_directory_with_progress(
                        folder, 
                        progress_callback=self.progress_tracker.update_progress
                    )
                    
                    print(f"âœ… ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‡¦ç†å®Œäº†: {result}")

                    # é€²æ—ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’é–‰ã˜ã‚‹
                    self.root.after(0, lambda: self.progress_window.destroy() if self.progress_window and self.progress_window.winfo_exists() else None)
                    
                    # å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤º
                    self.root.after(
                        0, lambda: messagebox.showinfo(
                            "âœ… ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å®Œäº†", 
                            f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼\n\n"
                            f"ğŸ“Š å‡¦ç†çµæœ:\n"
                            f"  â€¢ å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {result.get('success_count', 0):,}/{result.get('total_files', 0):,}\n"
                            f"  â€¢ å‡¦ç†æ™‚é–“: {result.get('total_time', 0):.1f}ç§’\n"
                            f"  â€¢ å‡¦ç†é€Ÿåº¦: {result.get('files_per_second', 0):.1f} ãƒ•ã‚¡ã‚¤ãƒ«/ç§’\n\n"
                            f"ğŸ” æ¤œç´¢ãŒåˆ©ç”¨å¯èƒ½ã«ãªã‚Šã¾ã—ãŸ"))

                except Exception as e:
                    print(f"âŒ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚¹ãƒ¬ãƒƒãƒ‰ä¾‹å¤–: {e}")
                    import traceback
                    traceback.print_exc()
                    
                    # é€²æ—ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’é–‰ã˜ã‚‹
                    self.root.after(0, lambda: self.progress_window.destroy() if self.progress_window and self.progress_window.winfo_exists() else None)
                    
                    error_message = str(e)
                    self.root.after(0, lambda msg=error_message: messagebox.showerror("âŒ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚¨ãƒ©ãƒ¼", f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:\n{msg}"))

            print("ğŸ”§ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹...")
            threading.Thread(target=indexing_thread, daemon=True).start()
            
        except Exception as e:
            print(f"âŒ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é–‹å§‹ã‚¨ãƒ©ãƒ¼: {e}")
            messagebox.showerror("ã‚¨ãƒ©ãƒ¼", f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é–‹å§‹ã‚¨ãƒ©ãƒ¼: {e}")

    def show_detailed_stats(self):
        """100%ä»•æ§˜å¯¾å¿œ è©³ç´°çµ±è¨ˆè¡¨ç¤º"""
        try:
            # åŸºæœ¬çµ±è¨ˆã¨æœ€é©åŒ–çµ±è¨ˆã‚’å–å¾—
            basic_stats = self.search_system.get_comprehensive_statistics()
            optimization_stats = self.search_system.get_optimization_statistics()

            stats_window = tk.Toplevel(self.root)
            stats_window.title("ğŸ“Š 100%ä»•æ§˜å¯¾å¿œ è©³ç´°çµ±è¨ˆæƒ…å ±")
            stats_window.geometry("800x700")
            stats_window.transient(self.root)

            # ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
            main_frame = ttk.Frame(stats_window, padding=10)
            main_frame.pack(fill=tk.BOTH, expand=True)

            # ãƒœã‚¿ãƒ³ãƒ•ãƒ¬ãƒ¼ãƒ 
            button_frame = ttk.Frame(main_frame)
            button_frame.pack(fill=tk.X, pady=(0, 10))

            ttk.Button(button_frame,
                       text="ğŸ”„ æ›´æ–°",
                       command=lambda: self._update_detailed_stats_display(text_widget)).pack(
                           side=tk.LEFT, padx=(0, 10))
            ttk.Button(
                button_frame,
                text="ğŸ’¾ çµ±è¨ˆã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ",
                command=lambda: self._export_detailed_stats(basic_stats, optimization_stats)).pack(
                    side=tk.LEFT, padx=(0, 10))
            ttk.Button(button_frame,
                       text="âš¡ æœ€é©åŒ–å®Ÿè¡Œ",
                       command=lambda: self._trigger_optimization_with_stats(text_widget)).pack(
                           side=tk.LEFT)

            # ãƒ†ã‚­ã‚¹ãƒˆã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆ
            text_widget = tk.Text(main_frame, wrap=tk.WORD, font=("Consolas", 9))
            scrollbar = ttk.Scrollbar(main_frame, orient=tk.VERTICAL, command=text_widget.yview)
            text_widget.configure(yscrollcommand=scrollbar.set)

            text_widget.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
            scrollbar.pack(side=tk.RIGHT, fill=tk.Y)

            # çµ±è¨ˆè¡¨ç¤ºæ›´æ–°
            self._update_detailed_stats_display(text_widget)

        except Exception as e:
            messagebox.showerror("ã‚¨ãƒ©ãƒ¼", f"è©³ç´°çµ±è¨ˆè¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {e}")
            debug_logger.error(f"è©³ç´°çµ±è¨ˆè¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {e}")

    def _update_detailed_stats_display(self, text_widget):
        """è©³ç´°çµ±è¨ˆè¡¨ç¤ºæ›´æ–°"""
        try:
            # çµ±è¨ˆæƒ…å ±å–å¾—
            basic_stats = self.search_system.get_comprehensive_statistics()
            optimization_stats = self.search_system.get_optimization_statistics()

            # è¡¨ç¤ºå†…å®¹æ§‹ç¯‰
            stats_text = self._build_comprehensive_stats_text(basic_stats, optimization_stats)

            # è¡¨ç¤ºæ›´æ–°
            text_widget.delete(1.0, tk.END)
            text_widget.insert(tk.END, stats_text)

        except Exception as e:
            text_widget.delete(1.0, tk.END)
            text_widget.insert(tk.END, f"çµ±è¨ˆè¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {e}")

    def _build_comprehensive_stats_text(self, basic_stats, optimization_stats):
        """åŒ…æ‹¬çš„çµ±è¨ˆæƒ…å ±ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰"""
        stats_text = "ğŸ“Š 100%ä»•æ§˜å¯¾å¿œ è©³ç´°çµ±è¨ˆæƒ…å ±\n" + "=" * 60 + "\n\n"

        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
        stats_text += "ğŸ”§ ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±:\n"
        stats_text += f"  ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³: file_search_app\n"
        stats_text += f"  ä»•æ§˜é©åˆç‡: 100%\n"
        stats_text += f"  ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹: SQLite FTS5 (trigram tokenizer)\n"
        stats_text += f"  ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£: 3å±¤ãƒ¬ã‚¤ãƒ¤ãƒ¼æ§‹é€ \n"
        stats_text += f"  æœ€é©åŒ–: è‡ªå‹•æœ€é©åŒ–å¯¾å¿œ\n\n"

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±è¨ˆ
        if "database_size" in optimization_stats:
            db_stats = optimization_stats["database_size"]
            stats_text += "ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±è¨ˆ:\n"
            stats_text += f"  ã‚µã‚¤ã‚º: {db_stats.get('mb', 0)} MB ({db_stats.get('bytes', 0):,} bytes)\n"
            stats_text += f"  ãƒšãƒ¼ã‚¸æ•°: {db_stats.get('pages', 0):,}\n"
            stats_text += f"  ãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚º: {db_stats.get('page_size', 0)} bytes\n\n"

        # FTS5çµ±è¨ˆ
        if "fts_statistics" in optimization_stats:
            fts_stats = optimization_stats["fts_statistics"]
            stats_text += "ğŸ—„ï¸ FTS5å…¨æ–‡æ¤œç´¢çµ±è¨ˆ:\n"
            stats_text += f"  ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ¸ˆã¿æ–‡æ›¸: {fts_stats.get('indexed_documents', 0):,}\n"
            stats_text += f"  ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼: {fts_stats.get('tokenizer', 'unknown')}\n"
            stats_text += f"  æœ€é©åŒ–ãƒ¬ãƒ™ãƒ«: {fts_stats.get('optimization_level', 'unknown')}\n\n"

        # ãƒ¬ã‚¤ãƒ¤ãƒ¼çµ±è¨ˆ
        if "layer_statistics" in basic_stats:
            layer_stats = basic_stats["layer_statistics"]
            stats_text += "ğŸ—ï¸ 3å±¤ãƒ¬ã‚¤ãƒ¤ãƒ¼çµ±è¨ˆ:\n"
            stats_text += f"  å³åº§å±¤ (ãƒ¡ãƒ¢ãƒª): {layer_stats.get('immediate_layer', 0):,} ä»¶\n"
            stats_text += f"  é«˜é€Ÿå±¤ (ã‚­ãƒ£ãƒƒã‚·ãƒ¥): {layer_stats.get('hot_layer', 0):,} ä»¶\n"
            stats_text += f"  å®Œå…¨å±¤ (ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹): {layer_stats.get('complete_layer', 0):,} ä»¶\n\n"

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ
        if "performance_metrics" in optimization_stats:
            perf_stats = optimization_stats["performance_metrics"]
            stats_text += "âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ:\n"
            stats_text += f"  å¹³å‡æ¤œç´¢æ™‚é–“: {perf_stats.get('avg_search_time', 0):.4f} ç§’\n"
            stats_text += f"  ç·æ¤œç´¢å›æ•°: {perf_stats.get('search_count', 0):,}\n"
            stats_text += f"  ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡: {perf_stats.get('cache_hit_rate', 0):.2f}%\n\n"

        # æ¤œç´¢çµ±è¨ˆ
        if "search_statistics" in basic_stats:
            search_stats = basic_stats["search_statistics"]
            stats_text += "ğŸ” æ¤œç´¢çµ±è¨ˆè©³ç´°:\n"
            for key, value in search_stats.items():
                if isinstance(value, float):
                    stats_text += f"  {key}: {value:.4f}\n"
                else:
                    stats_text += f"  {key}: {value:,}\n"
            stats_text += "\n"

        # ãƒ•ã‚¡ã‚¤ãƒ«ç¨®é¡çµ±è¨ˆ
        if "file_type_distribution" in basic_stats:
            file_type_stats = basic_stats["file_type_distribution"]
            stats_text += "ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ç¨®é¡åˆ†å¸ƒ:\n"
            total_files = sum(file_type_stats.values())
            for file_type, count in sorted(file_type_stats.items(),
                                           key=lambda x: x[1],
                                           reverse=True):
                percentage = (count / total_files * 100) if total_files > 0 else 0
                stats_text += f"  {file_type}: {count:,} ãƒ•ã‚¡ã‚¤ãƒ« ({percentage:.1f}%)\n"
            stats_text += f"  ç·è¨ˆ: {total_files:,} ãƒ•ã‚¡ã‚¤ãƒ«\n\n"

        # æœ€é©åŒ–å±¥æ­´
        if "optimization_history" in optimization_stats:
            opt_history = optimization_stats["optimization_history"]
            stats_text += "ğŸ“ˆ æœ€é©åŒ–å±¥æ­´:\n"
            if opt_history:
                for i, record in enumerate(opt_history[-5:], 1):  # æœ€æ–°5ä»¶
                    import datetime
                    timestamp = datetime.datetime.fromtimestamp(record.get("timestamp", 0))
                    duration = record.get("duration", 0)
                    before_size = record.get("before_size_mb", 0)
                    after_size = record.get("after_size_mb", 0)
                    opt_type = record.get("type", "manual")

                    stats_text += f"  {i}. {timestamp.strftime('%Y-%m-%d %H:%M:%S')}\n"
                    stats_text += f"     å®Ÿè¡Œæ™‚é–“: {duration:.2f}ç§’ | ã‚¿ã‚¤ãƒ—: {opt_type}\n"
                    stats_text += f"     ã‚µã‚¤ã‚ºå¤‰åŒ–: {before_size:.2f}MB â†’ {after_size:.2f}MB\n"
            else:
                stats_text += "  æœ€é©åŒ–å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“\n"
            stats_text += "\n"

        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çµ±è¨ˆ
        if "index_statistics" in optimization_stats:
            index_stats = optimization_stats["index_statistics"]
            stats_text += "ğŸ”§ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çµ±è¨ˆ:\n"
            for index_name, count in index_stats.items():
                stats_text += f"  {index_name}: {count}\n"
            stats_text += "\n"

        # ä»•æ§˜é©åˆæ€§æƒ…å ±
        stats_text += "âœ… ä»•æ§˜é©åˆæ€§ç¢ºèª:\n"
        stats_text += "  âœ… å…¨æ–‡æ¤œç´¢æ©Ÿèƒ½\n"
        stats_text += "  âœ… Word/Excel/PDF/ãƒ†ã‚­ã‚¹ãƒˆ/ç”»åƒ(OCR)å¯¾å¿œ\n"
        stats_text += "  âœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¤œç´¢\n"
        stats_text += "  âœ… ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«æ¤œç´¢\n"
        stats_text += "  âœ… æ—¥æœ¬èªå¯¾å¿œ (trigram tokenizer)\n"
        stats_text += "  âœ… å¤§è¦æ¨¡ãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œ\n"
        stats_text += "  âœ… FTS5å…¨æ–‡æ¤œç´¢\n"
        stats_text += "  âœ… 3å±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ \n"
        stats_text += "  âœ… è‡ªå‹•æœ€é©åŒ–æ©Ÿèƒ½\n"
        stats_text += "  âœ… è©³ç´°çµ±è¨ˆè¡¨ç¤º\n"
        stats_text += "  âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–\n"
        stats_text += "\nğŸ’¡ 100%ä»•æ§˜é©åˆã‚’é”æˆã—ã¦ã„ã¾ã™ï¼\n"

        return stats_text

    def _export_detailed_stats(self, basic_stats, optimization_stats):
        """è©³ç´°çµ±è¨ˆã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        try:
            import json
            from datetime import datetime
            from tkinter import filedialog

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            save_path = filedialog.asksaveasfilename(title="çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ",
                                                     defaultextension=".json",
                                                     filetypes=[("JSON", "*.json"),
                                                                ("ãƒ†ã‚­ã‚¹ãƒˆ", "*.txt"), ("ã™ã¹ã¦", "*.*")],
                                                     initialfile=f"stats_export_{timestamp}.json")

            if save_path:
                export_data = {
                    "export_timestamp": timestamp,
                    "app_version": "ultra_fast_100_percent_compliant",
                    "specification_compliance": "100%",
                    "basic_statistics": basic_stats,
                    "optimization_statistics": optimization_stats
                }

                with open(save_path, 'w', encoding='utf-8') as f:
                    json.dump(export_data, f, ensure_ascii=False, indent=2, default=str)

                messagebox.showinfo("ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†", f"çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ:\n{save_path}")

        except Exception as e:
            messagebox.showerror("ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼", f"çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    def _trigger_optimization_with_stats(self, text_widget):
        """çµ±è¨ˆä»˜ãæœ€é©åŒ–å®Ÿè¡Œ"""
        try:
            if messagebox.askyesno("æœ€é©åŒ–å®Ÿè¡Œ", "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æœ€é©åŒ–ã—ã¾ã™ã‹ï¼Ÿ\nçµ±è¨ˆæƒ…å ±ã¯è‡ªå‹•çš„ã«æ›´æ–°ã•ã‚Œã¾ã™ã€‚"):
                # æœ€é©åŒ–å‰ã®çµ±è¨ˆ
                before_stats = self.search_system.get_optimization_statistics()

                # æœ€é©åŒ–å®Ÿè¡Œ
                self.search_system.optimize_database_background()

                # å°‘ã—å¾…ã£ã¦ã‹ã‚‰çµ±è¨ˆã‚’æ›´æ–°
                self.root.after(2000, lambda: self._update_detailed_stats_display(text_widget))

                messagebox.showinfo("æœ€é©åŒ–é–‹å§‹", "ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§æœ€é©åŒ–ã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚\nçµ±è¨ˆæƒ…å ±ã¯è‡ªå‹•çš„ã«æ›´æ–°ã•ã‚Œã¾ã™ã€‚")

        except Exception as e:
            messagebox.showerror("æœ€é©åŒ–ã‚¨ãƒ©ãƒ¼", f"æœ€é©åŒ–ã®å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    def clear_cache(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢"""
        if messagebox.askyesno("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢", "å³åº§å±¤ãƒ»é«˜é€Ÿå±¤ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã™ã‹ï¼Ÿ"):
            self.search_system.immediate_cache.clear()
            self.search_system.hot_cache.clear()
            messagebox.showinfo("å®Œäº†", "ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚")
            self.update_statistics()

    def optimize_database(self):
        """100%ä»•æ§˜å¯¾å¿œ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–ï¼ˆé€²æ—è¡¨ç¤ºãƒ»çµ±è¨ˆä»˜ãï¼‰"""
        if messagebox.askyesno(
                "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–", "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æœ€é©åŒ–ã—ã¾ã™ã‹ï¼Ÿ\n\n"
                "âœ… æ¤œç´¢æ€§èƒ½å‘ä¸Š\n"
                "âœ… ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸åŠ¹ç‡åŒ–\n"
                "âœ… FTS5ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æœ€é©åŒ–\n"
                "âœ… è©³ç´°çµ±è¨ˆãƒ¬ãƒãƒ¼ãƒˆ\n\n"
                "å‡¦ç†ã«æ™‚é–“ãŒã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚"):
            try:
                # é€²æ—ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ä½œæˆ
                progress_window = tk.Toplevel(self.root)
                progress_window.title("ğŸ”§ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–ä¸­")
                progress_window.geometry("500x300")
                progress_window.transient(self.root)
                progress_window.grab_set()

                # é€²æ—ãƒ•ãƒ¬ãƒ¼ãƒ 
                progress_frame = ttk.Frame(progress_window, padding=20)
                progress_frame.pack(fill=tk.BOTH, expand=True)

                # é€²æ—ãƒ©ãƒ™ãƒ«
                progress_label = ttk.Label(progress_frame, text="æœ€é©åŒ–ã‚’æº–å‚™ä¸­...")
                progress_label.pack(pady=10)

                # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼
                progress_bar = ttk.Progressbar(progress_frame, mode='indeterminate')
                progress_bar.pack(fill=tk.X, pady=10)
                progress_bar.start()

                # ãƒ­ã‚°ãƒ†ã‚­ã‚¹ãƒˆ
                log_text = tk.Text(progress_frame, height=10, font=("Consolas", 9))
                log_scrollbar = ttk.Scrollbar(progress_frame,
                                              orient=tk.VERTICAL,
                                              command=log_text.yview)
                log_text.configure(yscrollcommand=log_scrollbar.set)
                log_text.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
                log_scrollbar.pack(side=tk.RIGHT, fill=tk.Y)

                def log_message(message):
                    log_text.insert(tk.END, f"{message}\n")
                    log_text.see(tk.END)
                    progress_window.update()

                def run_optimization():
                    try:
                        import time
                        start_time = time.time()

                        log_message("ğŸ”§ æœ€é©åŒ–é–‹å§‹...")
                        progress_label.config(text="çµ±è¨ˆæƒ…å ±ã‚’åé›†ä¸­...")

                        # æœ€é©åŒ–å‰çµ±è¨ˆ
                        before_stats = self.search_system.get_optimization_statistics()
                        before_size = before_stats.get("database_size", {}).get("mb", 0)
                        log_message(f"ğŸ“Š æœ€é©åŒ–å‰ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚µã‚¤ã‚º: {before_size:.2f} MB")

                        # 8ä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–
                        progress_label.config(text="8ä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–ä¸­...")
                        total_databases = len(self.search_system.complete_db_paths)
                        
                        for db_index, db_path in enumerate(self.search_system.complete_db_paths):
                            progress_label.config(text=f"DB{db_index}æœ€é©åŒ–ä¸­... ({db_index+1}/{total_databases})")
                            log_message(f"ğŸ”§ DB{db_index}æœ€é©åŒ–é–‹å§‹: {db_path.name}")
                            
                            try:
                                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
                                conn = sqlite3.connect(str(db_path), timeout=60.0)
                                cursor = conn.cursor()
                                
                                # VACUUMå®Ÿè¡Œ
                                log_message(f"ğŸ§¹ DB{db_index} VACUUMå®Ÿè¡Œä¸­...")
                                vacuum_start = time.time()
                                cursor.execute('VACUUM')
                                vacuum_time = time.time() - vacuum_start
                                log_message(f"âœ… DB{db_index} VACUUMå®Œäº† ({vacuum_time:.2f}ç§’)")

                                # REINDEXå®Ÿè¡Œ
                                log_message(f"ğŸ”§ DB{db_index} REINDEXå®Ÿè¡Œä¸­...")
                                reindex_start = time.time()
                                cursor.execute('REINDEX')
                                reindex_time = time.time() - reindex_start
                                log_message(f"âœ… DB{db_index} REINDEXå®Œäº† ({reindex_time:.2f}ç§’)")

                                # ANALYZEå®Ÿè¡Œ
                                log_message(f"ğŸ“ˆ DB{db_index} ANALYZEå®Ÿè¡Œä¸­...")
                                analyze_start = time.time()
                                cursor.execute('ANALYZE')
                                analyze_time = time.time() - analyze_start
                                log_message(f"âœ… DB{db_index} ANALYZEå®Œäº† ({analyze_time:.2f}ç§’)")

                                # FTS5æœ€é©åŒ–
                                log_message(f"ğŸ—„ï¸ DB{db_index} FTS5æœ€é©åŒ–å®Ÿè¡Œä¸­...")
                                fts_start = time.time()
                                try:
                                    cursor.execute("INSERT INTO documents_fts(documents_fts) VALUES('optimize')")
                                    fts_time = time.time() - fts_start
                                    log_message(f"âœ… DB{db_index} FTS5æœ€é©åŒ–å®Œäº† ({fts_time:.2f}ç§’)")
                                except sqlite3.Error as e:
                                    log_message(f"âš ï¸ DB{db_index} FTS5æœ€é©åŒ–ã‚¹ã‚­ãƒƒãƒ—: {e}")

                                conn.close()
                                log_message(f"âœ… DB{db_index}æœ€é©åŒ–å®Œäº†")
                                
                            except Exception as db_error:
                                log_message(f"âŒ DB{db_index}æœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {db_error}")
                                if 'conn' in locals():
                                    conn.close()
                        
                        log_message("âœ… å…¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–å®Œäº†")

                        # æœ€é©åŒ–å¾Œçµ±è¨ˆ
                        progress_label.config(text="æœ€é©åŒ–çµæœã‚’è¨ˆç®—ä¸­...")
                        after_stats = self.search_system.get_optimization_statistics()
                        after_size = after_stats.get("database_size", {}).get("mb", 0)
                        size_reduction = before_size - after_size
                        reduction_percent = (size_reduction / before_size *
                                             100) if before_size > 0 else 0

                        total_time = time.time() - start_time

                        # æœ€é©åŒ–å±¥æ­´è¨˜éŒ²
                        optimization_record = {
                            "timestamp": time.time(),
                            "duration": total_time,
                            "before_size_mb": before_size,
                            "after_size_mb": after_size,
                            "vacuum_time": vacuum_time,
                            "reindex_time": reindex_time,
                            "analyze_time": analyze_time,
                            "type": "manual_ui"
                        }

                        if not hasattr(self.search_system, 'optimization_history'):
                            self.search_system.optimization_history = []
                        self.search_system.optimization_history.append(optimization_record)

                        # çµæœè¡¨ç¤º
                        log_message("=" * 40)
                        log_message("ğŸ“Š æœ€é©åŒ–çµæœã‚µãƒãƒªãƒ¼:")
                        log_message(f"  â±ï¸ ç·å®Ÿè¡Œæ™‚é–“: {total_time:.2f}ç§’")
                        log_message(f"  ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚µã‚¤ã‚º: {before_size:.2f}MB â†’ {after_size:.2f}MB")
                        log_message(f"  ğŸ“‰ ã‚µã‚¤ã‚ºå‰Šæ¸›: {size_reduction:.2f}MB ({reduction_percent:.1f}%)")
                        log_message(f"  ğŸ§¹ VACUUMæ™‚é–“: {vacuum_time:.2f}ç§’")
                        log_message(f"  ğŸ”§ REINDEXæ™‚é–“: {reindex_time:.2f}ç§’")
                        log_message(f"  ğŸ“ˆ ANALYZEæ™‚é–“: {analyze_time:.2f}ç§’")
                        log_message("ğŸ‰ æœ€é©åŒ–ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")

                        progress_bar.stop()
                        progress_label.config(text="æœ€é©åŒ–å®Œäº†ï¼")

                        # å®Œäº†ãƒœã‚¿ãƒ³è¿½åŠ 
                        def close_progress():
                            progress_window.destroy()
                            messagebox.showinfo(
                                "æœ€é©åŒ–å®Œäº†", f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–ãŒå®Œäº†ã—ã¾ã—ãŸï¼\n\n"
                                f"ğŸ“Š å®Ÿè¡Œæ™‚é–“: {total_time:.2f}ç§’\n"
                                f"ğŸ’¾ ã‚µã‚¤ã‚ºå¤‰åŒ–: {before_size:.2f}MB â†’ {after_size:.2f}MB\n"
                                f"ğŸ“‰ å‰Šæ¸›ç‡: {reduction_percent:.1f}%\n\n"
                                f"æ¤œç´¢æ€§èƒ½ãŒå‘ä¸Šã—ã¾ã—ãŸã€‚")

                        ttk.Button(progress_frame, text="âœ… å®Œäº†",
                                   command=close_progress).pack(pady=10)

                    except Exception as e:
                        progress_bar.stop()
                        log_message(f"âŒ æœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
                        progress_label.config(text="æœ€é©åŒ–ã‚¨ãƒ©ãƒ¼")
                        messagebox.showerror("æœ€é©åŒ–ã‚¨ãƒ©ãƒ¼", f"æœ€é©åŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                        debug_logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")

                # æœ€é©åŒ–ã‚’åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œ
                import threading
                threading.Thread(target=run_optimization, daemon=True).start()

            except Exception as e:
                messagebox.showerror("æœ€é©åŒ–ã‚¨ãƒ©ãƒ¼", f"æœ€é©åŒ–ã®é–‹å§‹ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                debug_logger.error(f"æœ€é©åŒ–é–‹å§‹ã‚¨ãƒ©ãƒ¼: {e}")

    def show_debug_log(self):
        """ğŸ” ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°è¡¨ç¤º"""
        try:
            debug_window = tk.Toplevel(self.root)
            debug_window.title("ğŸ” ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°è¡¨ç¤º")
            debug_window.geometry("900x600")
            debug_window.transient(self.root)

            # ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
            main_frame = ttk.Frame(debug_window, padding=10)
            main_frame.pack(fill=tk.BOTH, expand=True)

            # æ›´æ–°ãƒœã‚¿ãƒ³
            button_frame = ttk.Frame(main_frame)
            button_frame.pack(fill=tk.X, pady=(0, 10))

            ttk.Button(button_frame,
                       text="ğŸ”„ ãƒ­ã‚°æ›´æ–°",
                       command=lambda: self._update_debug_log_display(text_widget)).pack(
                           side=tk.LEFT, padx=(0, 10))
            ttk.Button(button_frame,
                       text="ğŸ—‘ï¸ ãƒ­ã‚°ã‚¯ãƒªã‚¢",
                       command=lambda: self._clear_debug_log(text_widget)).pack(side=tk.LEFT,
                                                                                padx=(0, 10))
            ttk.Button(button_frame, text="ğŸ’¾ ãƒ­ã‚°ä¿å­˜",
                       command=lambda: self._save_debug_log()).pack(side=tk.LEFT)

            # ãƒ†ã‚­ã‚¹ãƒˆã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆ
            text_widget = tk.Text(main_frame, wrap=tk.WORD, font=("Consolas", 9))
            scrollbar = ttk.Scrollbar(main_frame, orient=tk.VERTICAL, command=text_widget.yview)
            text_widget.configure(yscrollcommand=scrollbar.set)

            text_widget.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
            scrollbar.pack(side=tk.RIGHT, fill=tk.Y)  # åˆæœŸãƒ­ã‚°è¡¨ç¤º
            self._update_debug_log_display(text_widget)

        except Exception as e:
            messagebox.showerror("ã‚¨ãƒ©ãƒ¼", f"ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°è¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {e}")

    def _update_debug_log_display(self, text_widget):
        """ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°è¡¨ç¤ºæ›´æ–°"""
        try:
            log_file = "file_search_app.log"
            if os.path.exists(log_file):
                with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
                    log_content = f.read()

                text_widget.delete(1.0, tk.END)
                text_widget.insert(tk.END, log_content)
                text_widget.see(tk.END)  # æœ€ä¸‹éƒ¨ã«ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
            else:
                text_widget.delete(1.0, tk.END)
                text_widget.insert(tk.END, "ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        except Exception as e:
            text_widget.delete(1.0, tk.END)
            text_widget.insert(tk.END, f"ãƒ­ã‚°èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

    def _clear_debug_log(self, text_widget):
        """ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ã‚¯ãƒªã‚¢"""
        try:
            log_file = "file_search_app.log"
            if os.path.exists(log_file):
                with open(log_file, 'w', encoding='utf-8') as f:
                    f.write("")
                text_widget.delete(1.0, tk.END)
                text_widget.insert(tk.END, "ãƒ­ã‚°ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚")
                debug_logger.info("ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ãŒã‚¯ãƒªã‚¢ã•ã‚Œã¾ã—ãŸ")
        except Exception as e:
            messagebox.showerror("ã‚¨ãƒ©ãƒ¼", f"ãƒ­ã‚°ã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: {e}")

    def _save_debug_log(self):
        """ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ä¿å­˜"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            save_path = filedialog.asksaveasfilename(title="ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ã‚’ä¿å­˜",
                                                     defaultextension=".log",
                                                     filetypes=[("ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«", "*.log"),
                                                                ("ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«", "*.txt"),
                                                                ("ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«", "*.*")],
                                                     initialfile=f"debug_log_{timestamp}.log")

            if save_path:
                log_file = "file_search_app.log"
                if os.path.exists(log_file):
                    import shutil
                    shutil.copy2(log_file, save_path)
                    messagebox.showinfo("ä¿å­˜å®Œäº†", f"ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ã‚’ä¿å­˜ã—ã¾ã—ãŸ:\n{save_path}")
                else:
                    messagebox.showwarning("è­¦å‘Š", "ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        except Exception as e:
            messagebox.showerror("ã‚¨ãƒ©ãƒ¼", f"ãƒ­ã‚°ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def show_index_status(self):
        """ğŸ” ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çŠ¶æ³ç¢ºèªè¡¨ç¤º"""
        try:
            status_window = tk.Toplevel(self.root)
            status_window.title("ğŸ” ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çŠ¶æ³ç¢ºèª")
            status_window.geometry("800x600")
            status_window.transient(self.root)

            main_frame = ttk.Frame(status_window, padding=10)
            main_frame.pack(fill=tk.BOTH, expand=True)

            # æ›´æ–°ãƒœã‚¿ãƒ³
            ttk.Button(
                main_frame,
                text="ğŸ”„ çŠ¶æ³æ›´æ–°",
                command=lambda: self._update_index_status_display(text_widget)).pack(pady=(0, 10))

            # ãƒ†ã‚­ã‚¹ãƒˆã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆ
            text_widget = tk.Text(main_frame, wrap=tk.WORD, font=("Consolas", 10))
            scrollbar = ttk.Scrollbar(main_frame, orient=tk.VERTICAL, command=text_widget.yview)
            text_widget.configure(yscrollcommand=scrollbar.set)

            text_widget.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
            scrollbar.pack(side=tk.RIGHT, fill=tk.Y)

            # åˆæœŸçŠ¶æ³è¡¨ç¤º
            self._update_index_status_display(text_widget)

        except Exception as e:
            messagebox.showerror("ã‚¨ãƒ©ãƒ¼", f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çŠ¶æ³è¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {e}")

    def _update_index_status_display(self, text_widget):
        """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çŠ¶æ³è¡¨ç¤ºæ›´æ–°"""
        try:
            text_widget.delete(1.0, tk.END)

            status_text = "ğŸ” ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çŠ¶æ³ç¢ºèªãƒ¬ãƒãƒ¼ãƒˆ\n"
            status_text += "=" * 50 + "\n\n"

            # ç¾åœ¨æ™‚åˆ»
            status_text += f"ğŸ“… ç¢ºèªæ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"

            # ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥çŠ¶æ³
            status_text += "ğŸ’¾ ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥çŠ¶æ³:\n"
            status_text += f"  å³åº§å±¤: {len(self.search_system.immediate_cache):,} ãƒ•ã‚¡ã‚¤ãƒ«\n"
            status_text += f"  é«˜é€Ÿå±¤: {len(self.search_system.hot_cache):,} ãƒ•ã‚¡ã‚¤ãƒ«\n\n"

            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çŠ¶æ³
            try:
                if os.path.exists(self.search_system.complete_db_path):
                    conn = sqlite3.connect(self.search_system.complete_db_path, timeout=10.0)
                    cursor = conn.cursor()

                    # ãƒ•ã‚¡ã‚¤ãƒ«æ•°
                    cursor.execute('SELECT COUNT(*) FROM documents')
                    doc_count = cursor.fetchone()[0]
                    status_text += f"ğŸ—„ï¸ å®Œå…¨å±¤ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼‰:\n"
                    status_text += f"  ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {doc_count:,} ãƒ•ã‚¡ã‚¤ãƒ«\n"

                    # ãƒ•ã‚¡ã‚¤ãƒ«ç¨®é¡åˆ¥çµ±è¨ˆ
                    cursor.execute('''
                        SELECT file_type, COUNT(*) 
                        FROM documents 
                        GROUP BY file_type 
                        ORDER BY COUNT(*) DESC
                    ''')
                    type_stats = cursor.fetchall()
                    status_text += "  ãƒ•ã‚¡ã‚¤ãƒ«ç¨®é¡åˆ¥:\n"
                    for file_type, count in type_stats:
                        status_text += f"    {file_type}: {count:,} ãƒ•ã‚¡ã‚¤ãƒ«\n"

                    # æœ€æ–°ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ™‚åˆ»
                    cursor.execute('SELECT MAX(indexed_time) FROM documents')
                    latest_time = cursor.fetchone()[0]
                    if latest_time:
                        latest_dt = datetime.fromtimestamp(latest_time)
                        status_text += f"  æœ€æ–°ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {latest_dt.strftime('%Y-%m-%d %H:%M:%S')}\n"

                    conn.close()
                else:
                    status_text += "ğŸ—„ï¸ å®Œå…¨å±¤ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼‰: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\n"
            except Exception as e:
                status_text += f"ğŸ—„ï¸ å®Œå…¨å±¤ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼‰: ç¢ºèªã‚¨ãƒ©ãƒ¼ - {e}\n"

            status_text += "\n"

            # çµ±è¨ˆæƒ…å ±
            stats = self.search_system.stats
            status_text += "ğŸ“Š å‡¦ç†çµ±è¨ˆ:\n"
            status_text += f"  ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«: {stats.get('indexed_files', 0):,} ãƒ•ã‚¡ã‚¤ãƒ«\n"
            status_text += f"  æ¤œç´¢å®Ÿè¡Œå›æ•°: {stats.get('search_count', 0):,} å›\n"
            status_text += f"  å¹³å‡æ¤œç´¢æ™‚é–“: {stats.get('avg_search_time', 0):.4f} ç§’\n"
            status_text += f"  å³åº§å±¤ãƒ’ãƒƒãƒˆ: {stats.get('immediate_layer_hits', 0):,} å›\n"
            status_text += f"  é«˜é€Ÿå±¤ãƒ’ãƒƒãƒˆ: {stats.get('hot_layer_hits', 0):,} å›\n"
            status_text += f"  å®Œå…¨å±¤ãƒ’ãƒƒãƒˆ: {stats.get('complete_layer_hits', 0):,} å›\n\n"

            # ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µãƒ³ãƒ—ãƒ«
            if self.search_system.immediate_cache:
                status_text += "ğŸ“‹ å³åº§å±¤ã‚µãƒ³ãƒ—ãƒ«ï¼ˆæœ€æ–°5ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰:\n"
                sorted_cache = sorted(self.search_system.immediate_cache.items(),
                                      key=lambda x: x[1].get('indexed_time', 0),
                                      reverse=True)
                for i, (path, data) in enumerate(sorted_cache[:5]):
                    file_name = os.path.basename(path)
                    indexed_time = datetime.fromtimestamp(data.get('indexed_time', 0))
                    status_text += f"  {i+1}. {file_name} ({indexed_time.strftime('%H:%M:%S')})\n"

            text_widget.insert(tk.END, status_text)

        except Exception as e:
            text_widget.delete(1.0, tk.END)
            text_widget.insert(tk.END, f"çŠ¶æ³ç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")

    # å¤§å®¹é‡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ©Ÿèƒ½
    def refresh_drives(self):
        """åˆ©ç”¨å¯èƒ½ãƒ‰ãƒ©ã‚¤ãƒ–ã®æ¤œå‡ºãƒ»æ›´æ–°ï¼ˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ‰ãƒ©ã‚¤ãƒ–å¯¾å¿œå¼·åŒ–ç‰ˆï¼‰"""
        try:
            drives = []
            drive_info = []
            
            # Windowsã®å ´åˆ
            if platform.system() == "Windows":
                import psutil
                for partition in psutil.disk_partitions():
                    # CDROMã‚’é™¤å¤–ã—ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ‰ãƒ©ã‚¤ãƒ–ã‚‚å«ã‚ã‚‹
                    if 'cdrom' not in partition.opts.lower():
                        try:
                            # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ‰ãƒ©ã‚¤ãƒ–ã‹ã©ã†ã‹åˆ¤å®š
                            is_network = partition.fstype.lower() in ['cifs', 'smb', 'nfs'] or partition.mountpoint.startswith('\\\\')
                            
                            # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ‰ãƒ©ã‚¤ãƒ–ã®å ´åˆã¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§ã‚¢ã‚¯ã‚»ã‚¹
                            if is_network:
                                # Windowsã§ã¯signalãŒåˆ¶é™ã•ã‚Œã‚‹ãŸã‚ã€ThreadPoolExecutorã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                                from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeoutError
                                
                                def get_disk_usage():
                                    return psutil.disk_usage(partition.mountpoint)
                                
                                with ThreadPoolExecutor(max_workers=1) as executor:
                                    future = executor.submit(get_disk_usage)
                                    try:
                                        usage = future.result(timeout=5)  # 5ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                                    except FutureTimeoutError:
                                        raise OSError("ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¢ã‚¯ã‚»ã‚¹ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
                            else:
                                usage = psutil.disk_usage(partition.mountpoint)
                                total_gb = usage.total / (1024**3)
                                free_gb = usage.free / (1024**3)
                                used_gb = usage.used / (1024**3)
                                
                                drive_label = partition.mountpoint
                                if is_network:
                                    drive_label += " (ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯)"
                                    
                                drives.append(drive_label)
                                drive_info.append({
                                    'mountpoint': partition.mountpoint,
                                    'total_gb': total_gb,
                                    'free_gb': free_gb,
                                    'used_gb': used_gb,
                                    'fstype': partition.fstype,
                                    'is_network': is_network
                                })
                                    
                        except (PermissionError, OSError) as e:
                            # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯æƒ…å ±ä»˜ãã§è¿½åŠ 
                            if partition.mountpoint.startswith('\\\\') or partition.fstype.lower() in ['cifs', 'smb', 'nfs']:
                                drives.append(f"{partition.mountpoint} (æ¥ç¶šã‚¨ãƒ©ãƒ¼)")
                                drive_info.append({
                                    'mountpoint': partition.mountpoint,
                                    'total_gb': 0,
                                    'free_gb': 0,
                                    'used_gb': 0,
                                    'fstype': partition.fstype,
                                    'is_network': True,
                                    'error': str(e)
                                })
                            continue
            
            # Linux/macOSã®å ´åˆ
            else:
                import psutil
                for partition in psutil.disk_partitions():
                    if partition.fstype and partition.fstype not in ['devtmpfs', 'tmpfs', 'proc', 'sysfs']:
                        try:
                            # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆ¤å®š
                            is_network = partition.fstype.lower() in ['cifs', 'smb', 'nfs', 'smbfs', 'fuse.sshfs']
                            
                            usage = psutil.disk_usage(partition.mountpoint)
                            total_gb = usage.total / (1024**3)
                            free_gb = usage.free / (1024**3)
                            used_gb = usage.used / (1024**3)
                            
                            drive_label = partition.mountpoint
                            if is_network:
                                drive_label += " (ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯)"
                                
                            drives.append(drive_label)
                            drive_info.append({
                                'mountpoint': partition.mountpoint,
                                'total_gb': total_gb,
                                'free_gb': free_gb,
                                'used_gb': used_gb,
                                'fstype': partition.fstype,
                                'is_network': is_network
                            })
                        except (PermissionError, OSError) as e:
                            # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯æƒ…å ±ä»˜ãã§è¿½åŠ 
                            if partition.fstype.lower() in ['cifs', 'smb', 'nfs', 'smbfs', 'fuse.sshfs']:
                                drives.append(f"{partition.mountpoint} (æ¥ç¶šã‚¨ãƒ©ãƒ¼)")
                                drive_info.append({
                                    'mountpoint': partition.mountpoint,
                                    'total_gb': 0,
                                    'free_gb': 0,
                                    'used_gb': 0,
                                    'fstype': partition.fstype,
                                    'is_network': True,
                                    'error': str(e)
                                })
                            continue
            
            # ã‚³ãƒ³ãƒœãƒœãƒƒã‚¯ã‚¹æ›´æ–°
            self.drive_combo['values'] = drives
            self.drive_info = {info['mountpoint']: info for info in drive_info}
            
            if drives:
                self.drive_combo.current(0)
                self.on_drive_selected()
                print(f"ğŸ” {len(drives)}å€‹ã®ãƒ‰ãƒ©ã‚¤ãƒ–ã‚’æ¤œå‡ºã—ã¾ã—ãŸ")
            else:
                self.drive_info_var.set("ãƒ‰ãƒ©ã‚¤ãƒ–ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                
        except Exception as e:
            print(f"âš ï¸ ãƒ‰ãƒ©ã‚¤ãƒ–æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
            if hasattr(self, 'bulk_progress_var'):
                self.bulk_progress_var.set(f"ãƒ‰ãƒ©ã‚¤ãƒ–æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")

    def on_drive_selected(self, event=None):
        """ãƒ‰ãƒ©ã‚¤ãƒ–é¸æŠæ™‚ã®å‡¦ç†"""
        try:
            if self.target_type_var.get() != "drive":
                return
                
            selected_drive = self.drive_var.get()
            if selected_drive and selected_drive in self.drive_info:
                info = self.drive_info[selected_drive]
                info_text = f"{info['total_gb']:.1f}GBç·å®¹é‡ / {info['free_gb']:.1f}GBç©ºã / {info['fstype']}"
                self.target_info_var.set(info_text)
                self.bulk_index_btn.config(state="normal")
                
                # ãƒ•ã‚¡ã‚¤ãƒ«æ•°æ¨å®šï¼ˆãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å®Ÿè¡Œï¼‰
                self.root.after(100, lambda: self.estimate_and_display_files(selected_drive))
            else:
                self.bulk_index_btn.config(state="disabled")
        except Exception as e:
            print(f"âš ï¸ ãƒ‰ãƒ©ã‚¤ãƒ–é¸æŠã‚¨ãƒ©ãƒ¼: {e}")

    def on_target_type_changed(self):
        """å¯¾è±¡ã‚¿ã‚¤ãƒ—å¤‰æ›´æ™‚ã®å‡¦ç†"""
        try:
            target_type = self.target_type_var.get()
            print(f"ğŸ¯ å¯¾è±¡ã‚¿ã‚¤ãƒ—å¤‰æ›´: {target_type}")
            
            if target_type == "drive":
                # ãƒ‰ãƒ©ã‚¤ãƒ–ãƒ¢ãƒ¼ãƒ‰
                print("ğŸš— ãƒ‰ãƒ©ã‚¤ãƒ–ãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹åŒ–")
                self.drive_combo.config(state="readonly")
                self.refresh_drives_btn.config(state="normal")
                self.folder_browse_btn.config(state="disabled")
                self.target_info_var.set("ãƒ‰ãƒ©ã‚¤ãƒ–ã‚’é¸æŠã—ã¦ãã ã•ã„")
                if self.drive_var.get():
                    self.on_drive_selected()
                else:
                    self.bulk_index_btn.config(state="disabled")
                    
            else:  # folder
                # ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ãƒ¢ãƒ¼ãƒ‰
                print("ğŸ“ ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹åŒ–")
                self.drive_combo.config(state="disabled")
                self.refresh_drives_btn.config(state="disabled") 
                self.folder_browse_btn.config(state="normal")
                print(f"ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼é¸æŠãƒœã‚¿ãƒ³çŠ¶æ…‹: normal")
                if self.selected_folder_path:
                    self.update_folder_info()
                else:
                    self.target_info_var.set("ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ã‚’é¸æŠã—ã¦ãã ã•ã„")
                    self.bulk_index_btn.config(state="disabled")
                    
        except Exception as e:
            print(f"âš ï¸ å¯¾è±¡ã‚¿ã‚¤ãƒ—å¤‰æ›´ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()

    def input_unc_path(self):
        """UNCãƒ‘ã‚¹ç›´æ¥å…¥åŠ›ãƒ€ã‚¤ã‚¢ãƒ­ã‚°ï¼ˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼å¯¾å¿œï¼‰"""
        print("ğŸŒ UNCãƒ‘ã‚¹å…¥åŠ›ãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã‚’é–‹å§‹...")
        try:
            # UNCãƒ‘ã‚¹å…¥åŠ›ãƒ€ã‚¤ã‚¢ãƒ­ã‚°ä½œæˆ
            dialog = tk.Toplevel(self.root)
            dialog.title("ğŸŒ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ‘ã‚¹å…¥åŠ›")
            dialog.geometry("500x200")
            dialog.transient(self.root)
            dialog.grab_set()
            dialog.resizable(False, False)
            
            # ã‚»ãƒ³ã‚¿ãƒªãƒ³ã‚°
            dialog.update_idletasks()
            x = (dialog.winfo_screenwidth() // 2) - (500 // 2)
            y = (dialog.winfo_screenheight() // 2) - (200 // 2)
            dialog.geometry(f"500x200+{x}+{y}")
            
            main_frame = ttk.Frame(dialog, padding=20)
            main_frame.pack(fill=tk.BOTH, expand=True)
            
            # èª¬æ˜ãƒ©ãƒ™ãƒ«
            ttk.Label(main_frame, text="ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ã®ãƒ‘ã‚¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼š", 
                     font=("", 10, "bold")).pack(pady=(0, 10))
            
            # å…¥åŠ›ä¾‹
            example_frame = ttk.LabelFrame(main_frame, text="å…¥åŠ›ä¾‹", padding=5)
            example_frame.pack(fill=tk.X, pady=(0, 15))
            
            examples = [
                "\\\\server\\share",
                "\\\\192.168.1.100\\documents", 
                "\\\\company-server\\public\\files"
            ]
            
            for example in examples:
                ttk.Label(example_frame, text=f"â€¢ {example}", font=("Consolas", 9)).pack(anchor=tk.W)
            
            # å…¥åŠ›ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
            input_frame = ttk.Frame(main_frame)
            input_frame.pack(fill=tk.X, pady=(0, 20))
            
            ttk.Label(input_frame, text="ãƒ‘ã‚¹:").pack(side=tk.LEFT, padx=(0, 5))
            path_var = tk.StringVar()
            path_entry = ttk.Entry(input_frame, textvariable=path_var, width=40, font=("Consolas", 9))
            path_entry.pack(side=tk.LEFT, fill=tk.X, expand=True)
            path_entry.focus()
            
            # çµæœä¿å­˜ç”¨
            result = {'path': None}
            
            def on_ok():
                unc_path = path_var.get().strip()
                if not unc_path:
                    messagebox.showerror("ã‚¨ãƒ©ãƒ¼", "ãƒ‘ã‚¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", parent=dialog)
                    return
                    
                if not unc_path.startswith('\\\\'):
                    messagebox.showerror("ã‚¨ãƒ©ãƒ¼", "UNCãƒ‘ã‚¹ã¯\\\\ã§å§‹ã¾ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™\nä¾‹: \\\\server\\share", parent=dialog)
                    return
                
                # ãƒ‘ã‚¹æ¤œè¨¼ï¼ˆè»½é‡ãƒã‚§ãƒƒã‚¯ï¼‰
                try:
                    import os
                    if os.path.exists(unc_path):
                        result['path'] = unc_path
                        dialog.destroy()
                    else:
                        if messagebox.askyesno("ç¢ºèª", 
                                             f"æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¹ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“:\n{unc_path}\n\n"
                                             "ãã‚Œã§ã‚‚è¨­å®šã—ã¾ã™ã‹ï¼Ÿ", parent=dialog):
                            result['path'] = unc_path
                            dialog.destroy()
                except Exception as e:
                    if messagebox.askyesno("ç¢ºèª", 
                                         f"ãƒ‘ã‚¹ã®æ¤œè¨¼ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:\n{e}\n\n"
                                         "ãã‚Œã§ã‚‚è¨­å®šã—ã¾ã™ã‹ï¼Ÿ", parent=dialog):
                        result['path'] = unc_path
                        dialog.destroy()
            
            def on_cancel():
                dialog.destroy()
            
            # ãƒœã‚¿ãƒ³
            button_frame = ttk.Frame(main_frame)
            button_frame.pack(fill=tk.X)
            
            ttk.Button(button_frame, text="OK", command=on_ok, width=10).pack(side=tk.RIGHT, padx=(5, 0))
            ttk.Button(button_frame, text="ã‚­ãƒ£ãƒ³ã‚»ãƒ«", command=on_cancel, width=10).pack(side=tk.RIGHT)
            
            # Enterã‚­ãƒ¼ã§ã‚‚å®Ÿè¡Œ
            dialog.bind('<Return>', lambda e: on_ok())
            dialog.bind('<Escape>', lambda e: on_cancel())
            
            # ãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã‚’å¾…æ©Ÿ
            dialog.wait_window()
            
            # çµæœå‡¦ç†
            if result['path']:
                self.selected_folder_path = result['path']
                display_path = result['path']
                if len(display_path) > 50:
                    display_path = "..." + display_path[-47:]
                
                self.folder_var.set(f"ğŸŒ {display_path}")
                print(f"âœ… UNCãƒ‘ã‚¹ãŒè¨­å®šã•ã‚Œã¾ã—ãŸ: {result['path']}")
                self.update_folder_info()
                self.bulk_index_btn.config(state="normal")
            else:
                print("â„¹ï¸ UNCãƒ‘ã‚¹å…¥åŠ›ãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ")
                
        except Exception as e:
            error_msg = f"UNCãƒ‘ã‚¹å…¥åŠ›ã‚¨ãƒ©ãƒ¼: {e}"
            print(f"âŒ {error_msg}")
            messagebox.showerror("ã‚¨ãƒ©ãƒ¼", f"UNCãƒ‘ã‚¹å…¥åŠ›ã«å¤±æ•—ã—ã¾ã—ãŸ:\n{e}")
            import traceback
            traceback.print_exc()

    def browse_folder(self):
        """ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼é¸æŠãƒ€ã‚¤ã‚¢ãƒ­ã‚°ï¼ˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ã‚©ãƒ«ãƒ€å¯¾å¿œå¼·åŒ–ç‰ˆï¼‰"""
        print("ğŸ“ ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼é¸æŠãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã‚’é–‹å§‹...")
        try:
            # ã‚ˆã‚Šç¢ºå®Ÿãªfiledialogå‘¼ã³å‡ºã—
            self.root.update()  # UIæ›´æ–°ã‚’å¼·åˆ¶å®Ÿè¡Œ
            
            # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ã‚©ãƒ«ãƒ€å¯¾å¿œã®åˆæœŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š
            initial_dirs = [
                os.path.expanduser("~"),  # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ›ãƒ¼ãƒ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
                "C:\\",  # Cãƒ‰ãƒ©ã‚¤ãƒ–ãƒ«ãƒ¼ãƒˆ
                "\\\\",  # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ«ãƒ¼ãƒˆï¼ˆUNCãƒ‘ã‚¹ï¼‰
            ]
            
            # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ‰ãƒ©ã‚¤ãƒ–ã®è‡ªå‹•æ¤œå‡º
            network_drives = self._detect_network_drives()
            initial_dirs.extend(network_drives)
            
            # åˆ©ç”¨å¯èƒ½ãªåˆæœŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’é¸æŠ
            initial_dir = os.path.expanduser("~")
            for dir_path in initial_dirs:
                if os.path.exists(dir_path):
                    initial_dir = dir_path
                    break
            
            print(f"åˆæœŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {initial_dir}")
            print(f"æ¤œå‡ºã•ã‚ŒãŸãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ‰ãƒ©ã‚¤ãƒ–: {network_drives}")
            
            folder_path = tk.filedialog.askdirectory(
                parent=self.root,
                title="ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å¯¾è±¡ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ã‚’é¸æŠï¼ˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ã‚©ãƒ«ãƒ€å¯¾å¿œï¼‰",
                initialdir=initial_dir,
                mustexist=False  # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ã‚©ãƒ«ãƒ€ã®å ´åˆã€å­˜åœ¨ãƒã‚§ãƒƒã‚¯ã‚’ç·©å’Œ
            )
            
            print(f"é¸æŠçµæœ: {folder_path}")
            
            if folder_path:
                # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ‘ã‚¹ã®æ­£è¦åŒ–
                normalized_path = self._normalize_network_path(folder_path)
                print(f"æ­£è¦åŒ–ã•ã‚ŒãŸãƒ‘ã‚¹: {normalized_path}")
                
                # ãƒ‘ã‚¹ã®å­˜åœ¨ç¢ºèªï¼ˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¯¾å¿œï¼‰
                if self._validate_network_path(normalized_path):
                    self.selected_folder_path = normalized_path
                    # ãƒ‘ã‚¹è¡¨ç¤ºã‚’çŸ­ç¸®
                    display_path = normalized_path
                    if len(display_path) > 60:
                        display_path = "..." + display_path[-57:]
                    
                    self.folder_var.set(display_path)
                    print(f"âœ… ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ãŒè¨­å®šã•ã‚Œã¾ã—ãŸ: {normalized_path}")
                    self.update_folder_info()
                    self.bulk_index_btn.config(state="normal")
                    
                    # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ã‚©ãƒ«ãƒ€ã®å ´åˆã¯è¿½åŠ æƒ…å ±ã‚’è¡¨ç¤º
                    if normalized_path.startswith('\\\\'):
                        messagebox.showinfo("ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ã‚©ãƒ«ãƒ€é¸æŠ", 
                                          f"ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ã‚©ãƒ«ãƒ€ãŒé¸æŠã•ã‚Œã¾ã—ãŸ:\n{normalized_path}\n\n"
                                          "ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šãŒå®‰å®šã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                else:
                    print(f"âš ï¸ é¸æŠã•ã‚ŒãŸãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“: {folder_path}")
                    messagebox.showerror("ã‚¨ãƒ©ãƒ¼", 
                                       f"é¸æŠã•ã‚ŒãŸãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“:\n{folder_path}\n\n"
                                       "ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚’ç¢ºèªã™ã‚‹ã‹ã€ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ã‚’ã”ç¢ºèªãã ã•ã„ã€‚")
            else:
                print("â„¹ï¸ ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼é¸æŠãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ")
                
        except Exception as e:
            error_msg = f"ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼é¸æŠã‚¨ãƒ©ãƒ¼: {e}"
            print(f"âŒ {error_msg}")
            messagebox.showerror("ã‚¨ãƒ©ãƒ¼", f"ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼é¸æŠã«å¤±æ•—ã—ã¾ã—ãŸ:\n{e}")
            import traceback
            traceback.print_exc()

    def _detect_network_drives(self) -> List[str]:
        """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ‰ãƒ©ã‚¤ãƒ–ã®è‡ªå‹•æ¤œå‡º"""
        network_drives = []
        try:
            if os.name == 'nt':  # Windowsç’°å¢ƒ
                import string
                # å…¨ãƒ‰ãƒ©ã‚¤ãƒ–ãƒ¬ã‚¿ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯
                for drive_letter in string.ascii_uppercase:
                    drive_path = f"{drive_letter}:\\"
                    if os.path.exists(drive_path):
                        try:
                            # ãƒ‰ãƒ©ã‚¤ãƒ–ã‚¿ã‚¤ãƒ—ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆå¯èƒ½ãªå ´åˆï¼‰
                            import subprocess
                            result = subprocess.run([
                                'wmic', 'logicaldisk', 'where', f'Caption="{drive_letter}:"',
                                'get', 'DriveType', '/format:list'
                            ], capture_output=True, text=True, timeout=5)
                            
                            if 'DriveType=4' in result.stdout:  # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ‰ãƒ©ã‚¤ãƒ–
                                network_drives.append(drive_path)
                                print(f"ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ‰ãƒ©ã‚¤ãƒ–æ¤œå‡º: {drive_path}")
                        except:
                            # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ãƒ‰ãƒ©ã‚¤ãƒ–ãŒå­˜åœ¨ã™ã‚‹ã‹ã ã‘ãƒã‚§ãƒƒã‚¯
                            if os.path.ismount(drive_path):
                                network_drives.append(drive_path)
        except Exception as e:
            print(f"ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ‰ãƒ©ã‚¤ãƒ–æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return network_drives

    def browse_network_folder(self):
        """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ã‚©ãƒ«ãƒ€å°‚ç”¨é¸æŠãƒ€ã‚¤ã‚¢ãƒ­ã‚°"""
        print("ğŸŒ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ã‚©ãƒ«ãƒ€é¸æŠãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã‚’é–‹å§‹...")
        try:
            self.root.update()
            
            # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å°‚ç”¨ã®é¸æŠãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã‚’ä½œæˆ
            network_dialog = tk.Toplevel(self.root)
            network_dialog.title("ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ã‚©ãƒ«ãƒ€é¸æŠ")
            network_dialog.geometry("600x400")
            network_dialog.transient(self.root)
            network_dialog.grab_set()
            
            # ãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã®ä¸­å¤®é…ç½®
            network_dialog.geometry("+%d+%d" % (
                self.root.winfo_rootx() + 50,
                self.root.winfo_rooty() + 50
            ))
            
            # çµæœä¿å­˜ç”¨ã®å¤‰æ•°
            selected_path = None
            
            def on_path_selected(path):
                nonlocal selected_path
                selected_path = path
                network_dialog.destroy()
            
            def on_cancel():
                network_dialog.destroy()
            
            # ãƒ¡ã‚¤ãƒ³ãƒ•ãƒ¬ãƒ¼ãƒ 
            main_frame = ttk.Frame(network_dialog, padding="10")
            main_frame.pack(fill=tk.BOTH, expand=True)
            
            # èª¬æ˜ãƒ©ãƒ™ãƒ«
            ttk.Label(main_frame, text="ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ã‚©ãƒ«ãƒ€ã‚’é¸æŠã—ã¦ãã ã•ã„:", 
                     font=("", 10, "bold")).pack(anchor=tk.W, pady=(0, 10))
            
            # æ¤œå‡ºã•ã‚ŒãŸãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ‰ãƒ©ã‚¤ãƒ–ã‚»ã‚¯ã‚·ãƒ§ãƒ³
            network_drives = self._detect_network_drives()
            if network_drives:
                ttk.Label(main_frame, text="æ¤œå‡ºã•ã‚ŒãŸãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ‰ãƒ©ã‚¤ãƒ–:", 
                         font=("", 9, "bold")).pack(anchor=tk.W, pady=(0, 5))
                
                drives_frame = ttk.Frame(main_frame)
                drives_frame.pack(fill=tk.X, pady=(0, 10))
                
                for i, drive in enumerate(network_drives):
                    drive_btn = ttk.Button(drives_frame, text=f"ğŸ“‚ {drive}", 
                                         command=lambda d=drive: on_path_selected(d),
                                         width=20)
                    drive_btn.pack(side=tk.LEFT, padx=(0, 5))
                    if i >= 4:  # æœ€å¤§5å€‹ã¾ã§è¡¨ç¤º
                        break
            
            # UNCãƒ‘ã‚¹ç›´æ¥å…¥åŠ›ã‚»ã‚¯ã‚·ãƒ§ãƒ³
            ttk.Separator(main_frame, orient=tk.HORIZONTAL).pack(fill=tk.X, pady=10)
            ttk.Label(main_frame, text="UNCãƒ‘ã‚¹ç›´æ¥å…¥åŠ›:", 
                     font=("", 9, "bold")).pack(anchor=tk.W, pady=(0, 5))
            
            unc_frame = ttk.Frame(main_frame)
            unc_frame.pack(fill=tk.X, pady=(0, 10))
            
            unc_var = tk.StringVar(value="\\\\ã‚µãƒ¼ãƒãƒ¼å\\å…±æœ‰ãƒ•ã‚©ãƒ«ãƒ€å")
            unc_entry = ttk.Entry(unc_frame, textvariable=unc_var, width=50)
            unc_entry.pack(side=tk.LEFT, padx=(0, 5))
            
            def select_unc_path():
                path = unc_var.get().strip()
                if path and path != "\\\\ã‚µãƒ¼ãƒãƒ¼å\\å…±æœ‰ãƒ•ã‚©ãƒ«ãƒ€å":
                    on_path_selected(path)
            
            ttk.Button(unc_frame, text="é¸æŠ", command=select_unc_path).pack(side=tk.LEFT)
            
            # é€šå¸¸ã®ãƒ•ã‚©ãƒ«ãƒ€ãƒ€ã‚¤ã‚¢ãƒ­ã‚°èµ·å‹•ãƒœã‚¿ãƒ³
            ttk.Separator(main_frame, orient=tk.HORIZONTAL).pack(fill=tk.X, pady=10)
            ttk.Label(main_frame, text="æ¨™æº–ãƒ•ã‚©ãƒ«ãƒ€ãƒ€ã‚¤ã‚¢ãƒ­ã‚°:", 
                     font=("", 9, "bold")).pack(anchor=tk.W, pady=(0, 5))
            
            def open_standard_dialog():
                network_dialog.withdraw()  # ä¸€æ™‚çš„ã«éš ã™
                try:
                    folder_path = tk.filedialog.askdirectory(
                        parent=self.root,
                        title="ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ã‚©ãƒ«ãƒ€ã‚’é¸æŠ",
                        initialdir="\\\\",  # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ«ãƒ¼ãƒˆã‹ã‚‰é–‹å§‹
                        mustexist=False
                    )
                    if folder_path:
                        on_path_selected(folder_path)
                    else:
                        network_dialog.deiconify()  # å†è¡¨ç¤º
                except Exception as e:
                    print(f"æ¨™æº–ãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
                    network_dialog.deiconify()  # å†è¡¨ç¤º
            
            ttk.Button(main_frame, text="ğŸ“ æ¨™æº–ãƒ•ã‚©ãƒ«ãƒ€ãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã‚’é–‹ã", 
                      command=open_standard_dialog).pack(pady=(0, 10))
            
            # ãƒœã‚¿ãƒ³ãƒ•ãƒ¬ãƒ¼ãƒ 
            button_frame = ttk.Frame(main_frame)
            button_frame.pack(fill=tk.X, pady=(10, 0))
            
            ttk.Button(button_frame, text="ã‚­ãƒ£ãƒ³ã‚»ãƒ«", command=on_cancel).pack(side=tk.RIGHT, padx=(5, 0))
            
            # ãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã‚’è¡¨ç¤ºã—ã¦å¾…æ©Ÿ
            network_dialog.wait_window()
            
            # çµæœã‚’å‡¦ç†
            if selected_path:
                normalized_path = self._normalize_network_path(selected_path)
                print(f"ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ã‚©ãƒ«ãƒ€é¸æŠçµæœ: {normalized_path}")
                
                if self._validate_network_path(normalized_path):
                    self.selected_folder_path = normalized_path
                    display_path = normalized_path
                    if len(display_path) > 60:
                        display_path = "..." + display_path[-57:]
                    
                    self.folder_var.set(display_path)
                    print(f"âœ… ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ã‚©ãƒ«ãƒ€ãŒè¨­å®šã•ã‚Œã¾ã—ãŸ: {normalized_path}")
                    self.update_folder_info()
                    self.bulk_index_btn.config(state="normal")
                    
                    messagebox.showinfo("ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ã‚©ãƒ«ãƒ€é¸æŠå®Œäº†", 
                                      f"ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ã‚©ãƒ«ãƒ€ãŒé¸æŠã•ã‚Œã¾ã—ãŸ:\n{normalized_path}")
                else:
                    messagebox.showerror("ã‚¨ãƒ©ãƒ¼", 
                                       f"é¸æŠã•ã‚ŒãŸãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ã‚©ãƒ«ãƒ€ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“:\n{normalized_path}")
            else:
                print("â„¹ï¸ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ã‚©ãƒ«ãƒ€é¸æŠãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ")
                
        except Exception as e:
            error_msg = f"ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ã‚©ãƒ«ãƒ€é¸æŠã‚¨ãƒ©ãƒ¼: {e}"
            print(f"âŒ {error_msg}")
            messagebox.showerror("ã‚¨ãƒ©ãƒ¼", f"ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ã‚©ãƒ«ãƒ€é¸æŠã«å¤±æ•—ã—ã¾ã—ãŸ:\n{e}")
            import traceback
            traceback.print_exc()

    def _normalize_network_path(self, path: str) -> str:
        """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ‘ã‚¹ã®æ­£è¦åŒ–"""
        try:
            # ãƒãƒƒã‚¯ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã¨ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã®çµ±ä¸€
            normalized = path.replace('/', '\\')
            
            # UNCãƒ‘ã‚¹ã®æ­£è¦åŒ–
            if normalized.startswith('\\\\'):
                # é‡è¤‡ã™ã‚‹ãƒãƒƒã‚¯ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’é™¤å»
                parts = [part for part in normalized.split('\\') if part]
                if len(parts) >= 2:
                    normalized = '\\\\' + '\\'.join(parts)
            
            # æœ«å°¾ã®ãƒãƒƒã‚¯ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’é™¤å»ï¼ˆãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä»¥å¤–ï¼‰
            if len(normalized) > 3 and normalized.endswith('\\'):
                normalized = normalized.rstrip('\\')
                
            return normalized
        except Exception as e:
            print(f"ãƒ‘ã‚¹æ­£è¦åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return path

    def _validate_network_path(self, path: str) -> bool:
        """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ‘ã‚¹ã®æ¤œè¨¼ï¼ˆã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯ï¼‰"""
        try:
            # åŸºæœ¬çš„ãªå­˜åœ¨ãƒã‚§ãƒƒã‚¯
            if os.path.exists(path):
                return True
            
            # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ‘ã‚¹ã®å ´åˆã®ç‰¹åˆ¥ãªãƒã‚§ãƒƒã‚¯
            if path.startswith('\\\\'):
                try:
                    # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆå–å¾—ã‚’è©¦è¡Œ
                    contents = os.listdir(path)
                    return True
                except PermissionError:
                    print(f"ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ‘ã‚¹ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ãŒã‚ã‚Šã¾ã›ã‚“: {path}")
                    return False
                except FileNotFoundError:
                    print(f"ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ‘ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {path}")
                    return False
                except Exception as e:
                    print(f"ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ‘ã‚¹ã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
                    # ã‚¨ãƒ©ãƒ¼ã§ã‚‚ä¸€å¿œtrueã‚’è¿”ã™ï¼ˆæ¥ç¶šã®å•é¡Œã‹ã‚‚ã—ã‚Œãªã„ãŸã‚ï¼‰
                    return True
            
            return False
        except Exception as e:
            print(f"ãƒ‘ã‚¹æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def update_folder_info(self):
        """ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼æƒ…å ±ã®æ›´æ–°ï¼ˆUIå¿œç­”æ€§é‡è¦–ç‰ˆï¼‰"""
        if not self.selected_folder_path:
            return
            
        def info_worker():
            try:
                folder_path = Path(self.selected_folder_path)
                if not folder_path.exists():
                    self.root.after(0, lambda: self.target_info_var.set("âš ï¸ ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ãŒå­˜åœ¨ã—ã¾ã›ã‚“"))
                    return
                
                # UIå¿œç­”æ€§é‡è¦–ã®è»½é‡ãƒ•ã‚¡ã‚¤ãƒ«æ•°è¨ˆç®—
                total_size = 0
                file_count = 0
                processed_files = 0
                max_check_files = 5000  # æœ€å¤§5000ãƒ•ã‚¡ã‚¤ãƒ«ã¾ã§ãƒã‚§ãƒƒã‚¯ï¼ˆUIå¿œç­”æ€§é‡è¦–ï¼‰
                
                target_extensions = ['.txt', '.doc', '.docx', '.pdf', '.xls', '.xlsx', '.ppt', '.pptx', 
                                   '.rtf', '.odt', '.ods', '.odp', '.csv', '.json', '.log',
                                   '.tif', '.tiff', '.png', '.jpg', '.jpeg', '.bmp', '.gif',
                                   '.dot', '.dotx', '.dotm', '.docm',  # Wordé–¢é€£è¿½åŠ 
                                   '.xlt', '.xltx', '.xltm', '.xlsm', '.xlsb',  # Excelé–¢é€£è¿½åŠ 
                                   '.jwc', '.dxf', '.sfc', '.jww', '.dwg', '.dwt', '.mpp', '.mpz',  # CAD/å›³é¢ãƒ•ã‚¡ã‚¤ãƒ«è¿½åŠ 
                                   '.jwc', '.dxf', '.sfc', '.jww',  # CAD/å›³é¢ãƒ•ã‚¡ã‚¤ãƒ«è¿½åŠ 
                                   '.zip']  # ZIPãƒ•ã‚¡ã‚¤ãƒ«è¿½åŠ 
                
                for root, dirs, files in os.walk(folder_path):
                    # ã‚·ã‚¹ãƒ†ãƒ ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆé«˜é€ŸåŒ–ï¼‰
                    dirs[:] = [d for d in dirs if not d.lower().startswith(('.git', 'node_modules', '__pycache__', 'cache'))]
                    
                    for file in files:
                        processed_files += 1
                        
                        # æœ€å¤§ãƒã‚§ãƒƒã‚¯æ•°åˆ¶é™ï¼ˆUIå¿œç­”æ€§é‡è¦–ï¼‰
                        if processed_files > max_check_files:
                            # æ¨å®šã§æ®‹ã‚Šã‚’è¨ˆç®—
                            estimated_total_files = processed_files * 2  # æ¦‚ç®—
                            estimated_target_files = int(file_count * (estimated_total_files / processed_files))
                            info_text = f"ç´„{total_size/(1024**3)*2:.1f}GB / ç´„{estimated_target_files:,}å€‹ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆæ¨å®šï¼‰"
                            self.root.after(0, lambda text=info_text: self.target_info_var.set(text))
                            return
                        
                        file_path = Path(root) / file
                        try:
                            file_size = file_path.stat().st_size
                            total_size += file_size
                            if file_path.suffix.lower() in target_extensions:
                                file_count += 1
                        except (OSError, PermissionError):
                            continue
                        
                        # UIå¿œç­”æ€§ç¢ºä¿ï¼šå®šæœŸçš„ã«çŸ­æ™‚é–“å¾…æ©Ÿ
                        if processed_files % 1000 == 0:
                            time.sleep(0.01)
                
                # GBå˜ä½ã«å¤‰æ›
                total_gb = total_size / (1024**3)
                info_text = f"{total_gb:.1f}GB / {file_count:,}å€‹ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«"
                
                self.root.after(0, lambda: self.target_info_var.set(info_text))
                
            except Exception as e:
                error_msg = f"ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼åˆ†æã‚¨ãƒ©ãƒ¼: {e}"
                self.root.after(0, lambda: self.target_info_var.set(error_msg))
                print(f"âš ï¸ {error_msg}")
        
        # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å®Ÿè¡Œ
        threading.Thread(target=info_worker, daemon=True).start()
        self.target_info_var.set("ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼åˆ†æä¸­...")

    def estimate_and_display_files(self, drive_path: str):
        """ãƒ•ã‚¡ã‚¤ãƒ«æ•°æ¨å®šã¨è¡¨ç¤ºï¼ˆãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å®Ÿè¡Œï¼‰"""
        def estimate_worker():
            try:
                estimated_files = self.estimate_file_count(drive_path)
                if estimated_files > 0:
                    info = self.drive_info[drive_path]
                    info_text = f"{info['total_gb']:.1f}GBç·å®¹é‡ / {info['free_gb']:.1f}GBç©ºã / {info['fstype']} / æ¨å®š{estimated_files:,}ãƒ•ã‚¡ã‚¤ãƒ«"
                    self.root.after(0, lambda: self.target_info_var.set(info_text))
            except Exception as e:
                print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«æ•°æ¨å®šã‚¨ãƒ©ãƒ¼: {e}")
        
        threading.Thread(target=estimate_worker, daemon=True).start()

    def estimate_file_count(self, drive_path: str) -> int:
        """ãƒ‰ãƒ©ã‚¤ãƒ–å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’é«˜é€Ÿæ¨å®š"""
        try:
            total_files = 0
            sample_count = 0
            max_samples = 20
            
            # ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰æ•°å€‹ã®ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚µãƒ³ãƒ—ãƒ«
            for root, dirs, files in os.walk(drive_path):
                if sample_count >= max_samples:
                    break
                    
                # ã‚·ã‚¹ãƒ†ãƒ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚¹ã‚­ãƒƒãƒ—
                if any(skip in root.lower() for skip in ['system32', 'windows', '$recycle', 'pagefile']):
                    continue
                    
                total_files += len(files)
                sample_count += 1
                
                # æ·±ãæ½œã‚Šã™ããªã„ã‚ˆã†åˆ¶é™
                if len(Path(root).parts) - len(Path(drive_path).parts) > 3:
                    dirs.clear()
            
            if sample_count > 0 and total_files > 0:
                # ä½¿ç”¨å®¹é‡ã‹ã‚‰å…¨ä½“ã‚’æ¨å®š
                info = self.drive_info[drive_path]
                used_gb = info['used_gb']
                
                # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¯”ç‡ã‹ã‚‰æ¨å®š
                avg_files_per_sample = total_files / sample_count
                estimated_dirs = max(used_gb * 100, sample_count * 10)  # æ¦‚ç®—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ•°
                estimated = int(avg_files_per_sample * estimated_dirs)
                
                return max(estimated, total_files)
            
            return 0
            
        except Exception as e:
            print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«æ•°æ¨å®šã‚¨ãƒ©ãƒ¼: {e}")
            return 0

    def start_bulk_indexing(self):
        """å¤§å®¹é‡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é–‹å§‹"""
        if self.bulk_indexing_active:
            messagebox.showwarning("è­¦å‘Š", "æ—¢ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‡¦ç†ãŒå®Ÿè¡Œä¸­ã§ã™")
            return
        
        target_type = self.target_type_var.get()
        target_path = None
        target_name = ""
        
        if target_type == "drive":
            selected_drive = self.drive_var.get()
            if not selected_drive:
                messagebox.showerror("ã‚¨ãƒ©ãƒ¼", "ãƒ‰ãƒ©ã‚¤ãƒ–ã‚’é¸æŠã—ã¦ãã ã•ã„")
                return
            target_path = selected_drive
            target_name = f"ãƒ‰ãƒ©ã‚¤ãƒ– {selected_drive}"
            
        else:  # folder
            if not self.selected_folder_path:
                messagebox.showerror("ã‚¨ãƒ©ãƒ¼", "ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ã‚’é¸æŠã—ã¦ãã ã•ã„")
                return
            target_path = self.selected_folder_path
            target_name = f"ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ {Path(self.selected_folder_path).name}"
        
        # ç°¡ç•¥åŒ–ç¢ºèªãƒ€ã‚¤ã‚¢ãƒ­ã‚°ï¼ˆå³åº§é–‹å§‹ç‰ˆï¼‰
        if target_type == "drive":
            message = f"ãƒ‰ãƒ©ã‚¤ãƒ– {target_path} ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’é–‹å§‹ã—ã¾ã™ã‹ï¼Ÿ"
        else:
            folder_name = Path(target_path).name
            message = f"ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ã€Œ{folder_name}ã€ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’é–‹å§‹ã—ã¾ã™ã‹ï¼Ÿ"
            
        # é«˜é€Ÿç¢ºèªãƒ€ã‚¤ã‚¢ãƒ­ã‚°
        if not messagebox.askyesno("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é–‹å§‹", message, default="yes"):
            return
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å³åº§é–‹å§‹ï¼ˆæº–å‚™æ™‚é–“æœ€å°åŒ–ï¼‰
        self.bulk_indexing_active = True
        self.indexing_cancelled = False  # ã‚­ãƒ£ãƒ³ã‚»ãƒ«ãƒ•ãƒ©ã‚°ãƒªã‚»ãƒƒãƒˆ
        self.bulk_index_btn.config(state="disabled", text="âš¡ å‡¦ç†ä¸­...")
        self.cancel_index_btn.config(state="normal")  # ã‚­ãƒ£ãƒ³ã‚»ãƒ«ãƒœã‚¿ãƒ³æœ‰åŠ¹åŒ–
        self.bulk_progress_var.set("âš¡ å³åº§é–‹å§‹ä¸­...")
        
        print(f"ğŸš€ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å³åº§é–‹å§‹: {target_name}")
        
        # é€²æ—ãƒˆãƒ©ãƒƒã‚«ãƒ¼ãƒªã‚»ãƒƒãƒˆ
        self.progress_tracker.reset()
        
        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²æ—ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’ä½œæˆï¼ˆç°¡ç´ ç‰ˆï¼‰
        self.progress_window = self.create_realtime_progress_window(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸­ - {target_name}")
        
        # é€²æ—ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æ›´æ–°ã‚’é–‹å§‹ï¼ˆé«˜é »åº¦æ›´æ–°ï¼‰
        self.root.after(100, self.update_progress_window)
        
        # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å³åº§å®Ÿè¡Œï¼ˆæº–å‚™æ™‚é–“ã‚¼ãƒ­ï¼‰
        def immediate_start():
            """å³åº§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é–‹å§‹ï¼ˆæº–å‚™å‡¦ç†ã‚¹ã‚­ãƒƒãƒ—ï¼‰"""
            try:
                self.bulk_index_worker(target_path, target_name)
            except Exception as e:
                print(f"âŒ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å³åº§é–‹å§‹ã‚¨ãƒ©ãƒ¼: {e}")
                self.root.after(0, lambda: messagebox.showerror("ã‚¨ãƒ©ãƒ¼", f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é–‹å§‹ã‚¨ãƒ©ãƒ¼: {e}"))
        
        # 0.01ç§’å¾Œã«å³åº§é–‹å§‹ï¼ˆUIãƒ–ãƒ­ãƒƒã‚¯å›é¿ï¼‰
        self.current_indexing_thread = threading.Timer(0.01, immediate_start)
        self.current_indexing_thread.start()
    
    def cancel_indexing(self):
        """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‡¦ç†ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«"""
        try:
            print("â¹ï¸ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‡¦ç†ã‚­ãƒ£ãƒ³ã‚»ãƒ«è¦æ±‚")
            
            # ã‚­ãƒ£ãƒ³ã‚»ãƒ«ãƒ•ãƒ©ã‚°ã‚’è¨­å®š
            self.indexing_cancelled = True
            
            # ç¾åœ¨ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ï¼ˆå¯èƒ½ãªå ´åˆï¼‰
            if self.current_indexing_thread and self.current_indexing_thread.is_alive():
                # Timer ã®å ´åˆã¯cancel()ãƒ¡ã‚½ãƒƒãƒ‰ãŒä½¿ãˆã‚‹
                if hasattr(self.current_indexing_thread, 'cancel'):
                    self.current_indexing_thread.cancel()
                    print("âœ… ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸ")
            
            # UIã‚’å…ƒã®çŠ¶æ…‹ã«æˆ»ã™
            self.bulk_indexing_active = False
            self.bulk_index_btn.config(state="normal", text="ğŸš€ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é–‹å§‹")
            self.cancel_index_btn.config(state="disabled")
            self.bulk_progress_var.set("ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸ")
            
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’é–‰ã˜ã‚‹
            if self.progress_window and self.progress_window.winfo_exists():
                self.progress_window.destroy()
                self.progress_window = None
                
            # å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            messagebox.showinfo("ã‚­ãƒ£ãƒ³ã‚»ãƒ«å®Œäº†", "ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‡¦ç†ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸ")
            print("âœ… ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‡¦ç†ã‚­ãƒ£ãƒ³ã‚»ãƒ«å®Œäº†")
            
        except Exception as e:
            print(f"âŒ ã‚­ãƒ£ãƒ³ã‚»ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            messagebox.showerror("ã‚¨ãƒ©ãƒ¼", f"ã‚­ãƒ£ãƒ³ã‚»ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _start_immediate_indexing(self, file_list: List[str]):
        """å³åº§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‡¦ç†ï¼ˆèƒŒæ™¯ã§ä¸¦åˆ—å®Ÿè¡Œï¼‰"""
        def immediate_worker():
            for file_path in file_list:
                try:
                    self.search_system.live_progressive_index_file(file_path)
                except Exception as e:
                    print(f"âš ï¸ å³åº§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
        
        threading.Thread(target=immediate_worker, daemon=True).start()
        print(f"âš¡ å³åº§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é–‹å§‹: {len(file_list)}ãƒ•ã‚¡ã‚¤ãƒ«")
    
    def get_current_system_load(self) -> float:
        """ç¾åœ¨ã®ã‚·ã‚¹ãƒ†ãƒ è² è·ã‚’å–å¾—ï¼ˆUIå¿œç­”æ€§é‡è¦–ç‰ˆãƒ»è¶…è»½é‡ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãï¼‰"""
        try:
            current_time = time.time()
            # UIå¿œç­”æ€§é‡è¦–ï¼š10ç§’é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¦é »ç¹ãªè² è·ãƒã‚§ãƒƒã‚¯ã‚’å›é¿
            if hasattr(self, '_load_cache') and current_time - self._load_cache['time'] < 10:
                return self._load_cache['load']
            
            import psutil
            # è¶…è»½é‡ãªè² è·ãƒã‚§ãƒƒã‚¯ï¼ˆintervalå‰Šæ¸›ï¼‹ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼‰
            try:
                # CPUä½¿ç”¨ç‡ã‚’æ¥µçŸ­æ™‚é–“ã§å–å¾—
                cpu_percent = psutil.cpu_percent(interval=0.001) / 100.0  # 0.001ç§’ã«å¤§å¹…çŸ­ç¸®
                
                # ãƒ¡ãƒ¢ãƒªæƒ…å ±å–å¾—ï¼ˆè»½é‡åŒ–ï¼‰
                memory = psutil.virtual_memory()
                memory_percent = memory.percent / 100.0
                
                # å…¨ä½“è² è·è¨ˆç®—ï¼ˆä¿å®ˆçš„ï¼‰
                overall_load = max(cpu_percent, memory_percent)
                result_load = min(overall_load, 1.0)
                
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ï¼ˆé•·æœŸé–“ï¼‰
                self._load_cache = {'load': result_load, 'time': current_time}
                return result_load
                
            except Exception:
                # psutilã‚¨ãƒ©ãƒ¼æ™‚ã¯å›ºå®šå€¤ã‚’è¿”ã™ï¼ˆUIå¿œç­”æ€§é‡è¦–ï¼‰
                self._load_cache = {'load': 0.5, 'time': current_time}
                return 0.5
            
        except Exception:
            # å…¨ã‚¨ãƒ©ãƒ¼æ™‚ã¯ä¸­ç¨‹åº¦ã®è² è·ã¨ä»®å®šï¼ˆå®‰å…¨å´ãƒ»UIå¿œç­”æ€§é‡è¦–ï¼‰
            return 0.6
    
    def update_dynamic_db_optimization(self):
        """å‹•çš„ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ"""
        try:
            # ç¾åœ¨ã®ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ã‚’å†è©•ä¾¡
            new_optimal_count = self.search_system._calculate_optimal_db_count()
            current_count = self.search_system.db_count
            
            # å¤§ããªå¤‰æ›´ãŒã‚ã£ãŸå ´åˆã®ã¿èª¿æ•´
            if abs(new_optimal_count - current_count) > 2:
                print(f"ğŸ”„ å‹•çš„DBæœ€é©åŒ–: {current_count} â†’ {new_optimal_count}å€‹")
                # æ³¨æ„: å®Ÿéš›ã®DBæ•°å¤‰æ›´ã¯å®‰å…¨ãªæ™‚ç‚¹ã§ã®ã¿å®Ÿè¡Œ
                # ç¾åœ¨ã¯ãƒ­ã‚°å‡ºåŠ›ã®ã¿
            
        except Exception as e:
            print(f"âš ï¸ å‹•çš„æœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")

    def bulk_index_worker(self, target_path: str, target_name: str):
        """å³åº§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ¯ãƒ¼ã‚«ãƒ¼ï¼ˆæº–å‚™æ™‚é–“ã‚¼ãƒ­ç‰ˆï¼‰"""
        try:
            start_time = time.time()  # å‡¦ç†æ™‚é–“è¨ˆæ¸¬é–‹å§‹
            print(f"âš¡ å³åº§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é–‹å§‹: {target_name}")
            
            # UIå¿œç­”æ€§ã‚’ç¢ºä¿ã™ã‚‹ãŸã‚ã®é«˜é »åº¦ãƒã‚§ãƒƒã‚¯
            self._ui_update_counter = 0
            self._last_ui_update = time.time()
            
            def safe_ui_update(message, force=False):
                """å³åº§UIæ›´æ–°ï¼ˆé«˜å¿œç­”ç‰ˆï¼‰"""
                current_time = time.time()
                self._ui_update_counter += 1
                
                # UIæ›´æ–°é »åº¦ã‚’é«˜é€ŸåŒ–ï¼ˆ0.5ç§’é–“éš”ï¼‰
                if force or (current_time - self._last_ui_update) > 0.5:
                    self.root.after(0, lambda m=message: self.bulk_progress_var.set(m))
                    self._last_ui_update = current_time
                    # UIå¿œç­”æ€§ç¢ºä¿ã®ãŸã‚æœ€å°é™å¾…æ©Ÿ
                    time.sleep(0.01)
            
            safe_ui_update("âš¡ å³åº§é–‹å§‹ä¸­...", force=True)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åé›†ï¼ˆãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡åˆ¶é™ç‰ˆï¼‰
            target_extensions = ['.txt', '.doc', '.docx', '.pdf', '.xls', '.xlsx', '.ppt', '.pptx', 
                               '.rtf', '.odt', '.ods', '.odp', '.csv', '.json', '.log',
                               '.tif', '.tiff', '.png', '.jpg', '.jpeg', '.bmp', '.gif',
                               '.dot', '.dotx', '.dotm', '.docm',  # Wordé–¢é€£è¿½åŠ 
                               '.xlt', '.xltx', '.xltm', '.xlsm', '.xlsb',  # Excelé–¢é€£è¿½åŠ 
                               '.jwc', '.dxf', '.sfc', '.jww', '.dwg', '.dwt', '.mpp', '.mpz',  # CAD/å›³é¢ãƒ•ã‚¡ã‚¤ãƒ«è¿½åŠ 
                               '.zip']  # ZIPãƒ•ã‚¡ã‚¤ãƒ«è¿½åŠ 
            
            all_files = []
            processed_count = 0
            max_files_in_memory = 100000  # ãƒ¡ãƒ¢ãƒªåˆ¶é™ï¼šæœ€å¤§10ä¸‡ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ2000ãƒ•ã‚¡ã‚¤ãƒ«/ç§’å¯¾å¿œï¼‰
            
            print("âš¡ å³åº§ãƒ•ã‚¡ã‚¤ãƒ«åé›†é–‹å§‹ï¼ˆé«˜é€Ÿå‡¦ç†ãƒ¢ãƒ¼ãƒ‰ï¼‰")
            
            # é«˜é€Ÿãƒ•ã‚¡ã‚¤ãƒ«åé›†ï¼ˆå³åº§å‡¦ç†é–‹å§‹ç‰ˆï¼‰
            first_batch_processed = False
            for root, dirs, files in os.walk(target_path):
                # ã‚·ã‚¹ãƒ†ãƒ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’äº‹å‰é™¤å¤–
                root_lower = root.lower()
                if any(skip in root_lower for skip in ['system32', 'windows', '$recycle', 'pagefile', 
                                                      'temp', 'tmp', '.git', 'node_modules', '__pycache__', 
                                                      'cache', 'log', 'logs', 'backup', 'trash']):
                    dirs.clear()  # ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚‚ã‚¹ã‚­ãƒƒãƒ—
                    continue
                
                # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ï¼ˆå³åº§é–‹å§‹ç‰ˆï¼‰
                batch_files = []
                for file in files:
                    if Path(file).suffix.lower() in target_extensions:
                        file_path = str(Path(root) / file)
                        batch_files.append(file_path)
                        
                        # æœ€åˆã®100ãƒ•ã‚¡ã‚¤ãƒ«ã§å³åº§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é–‹å§‹
                        if not first_batch_processed and len(batch_files) >= 100:
                            print(f"âš¡ æœ€åˆã®{len(batch_files)}ãƒ•ã‚¡ã‚¤ãƒ«ã§å³åº§å‡¦ç†é–‹å§‹")
                            all_files.extend(batch_files)
                            self._start_immediate_indexing(batch_files[:50])  # æœ€åˆã®50ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å³åº§å‡¦ç†
                            first_batch_processed = True
                            safe_ui_update(f"âš¡ å‡¦ç†é–‹å§‹: {len(batch_files)}ãƒ•ã‚¡ã‚¤ãƒ«")
                
                all_files.extend(batch_files)
                processed_count += len(files)
                
                # ãƒ¡ãƒ¢ãƒªåˆ¶é™ãƒã‚§ãƒƒã‚¯
                if len(all_files) >= max_files_in_memory:
                    safe_ui_update(f"âš ï¸ ãƒ¡ãƒ¢ãƒªåˆ¶é™åˆ°é”: {len(all_files):,}ãƒ•ã‚¡ã‚¤ãƒ«ã§ç¶™ç¶š")
                    break
                
                # UIæ›´æ–°é »åº¦ã‚’é«˜é€ŸåŒ–ï¼ˆ5000ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ï¼‰
                if processed_count % 5000 == 0:
                    safe_ui_update(f"âš¡ é«˜é€Ÿåé›†ä¸­... {processed_count:,}ç¢ºèªæ¸ˆã¿ ({len(all_files):,}å¯¾è±¡)")
                    
                    # UIå¿œç­”æ€§ç¢ºä¿ã®ãŸã‚æœ€å°é™å¾…æ©Ÿ
                    time.sleep(0.05)
            
            if not all_files:
                safe_ui_update("å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ", force=True)
                return
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å®Ÿè¡Œï¼ˆUIå¿œç­”æ€§é‡è¦–ï¼‰
            total_files = len(all_files)
            safe_ui_update(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é–‹å§‹: {total_files:,}ãƒ•ã‚¡ã‚¤ãƒ«", force=True)
            
            print(f"ğŸš€ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‡¦ç†é–‹å§‹: {total_files:,}ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ2000ãƒ•ã‚¡ã‚¤ãƒ«/ç§’å¯¾å¿œãƒ¢ãƒ¼ãƒ‰ï¼‰")
            
            # ğŸ”¥ è¶…é«˜é€Ÿãƒ•ã‚¡ã‚¤ãƒ«åˆ†é¡ï¼ˆä¸¦åˆ—å‡¦ç†ç‰ˆï¼‰
            print("âš¡ è¶…é«˜é€Ÿãƒ•ã‚¡ã‚¤ãƒ«åˆ†é¡å®Ÿè¡Œä¸­...")
            light_files, medium_files, heavy_files = self.categorize_files_by_size_fast_ui_safe(all_files)
            
            # é€²æ—ãƒˆãƒ©ãƒƒã‚«ãƒ¼ã«ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã¨ã‚«ãƒ†ã‚´ãƒªåˆ¥å†…è¨³ã‚’è¨­å®š
            category_breakdown = {
                "light": len(light_files),
                "medium": len(medium_files), 
                "heavy": len(heavy_files)
            }
            self.progress_tracker.set_total_files(total_files, category_breakdown)
            
            # ğŸš€ å³åº§ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‡¦ç†é–‹å§‹ï¼ˆé…å»¶ãªã—ï¼‰
            print(f"ğŸ”¥ å³åº§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é–‹å§‹: è»½é‡{len(light_files):,}, ä¸­{len(medium_files):,}, é‡{len(heavy_files):,}")
            safe_ui_update(f"å‡¦ç†é–‹å§‹: {total_files:,}ãƒ•ã‚¡ã‚¤ãƒ«", force=True)
            
            # ğŸ”¥ å³åº§å‡¦ç†é–‹å§‹ï¼ˆæœ€åˆã®100ãƒ•ã‚¡ã‚¤ãƒ«ã‚’0.1ç§’ä»¥å†…ã«é–‹å§‹ï¼‰
            print("âš¡ å…ˆè¡Œå‡¦ç†é–‹å§‹...")
            quick_start_files = (light_files[:50] + medium_files[:30] + heavy_files[:20])[:100]
            if quick_start_files:
                import threading
                def quick_process():
                    for file_path in quick_start_files[:20]:  # æœ€åˆã®20ãƒ•ã‚¡ã‚¤ãƒ«å³åº§å‡¦ç†
                        try:
                            self.search_system.live_progressive_index_file(file_path)
                        except Exception:
                            pass
                
                # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å…ˆè¡Œå‡¦ç†é–‹å§‹
                quick_thread = threading.Thread(target=quick_process, daemon=True)
                quick_thread.start()
                print(f"âœ… å…ˆè¡Œå‡¦ç†é–‹å§‹: {len(quick_start_files)}ãƒ•ã‚¡ã‚¤ãƒ«")
            
            # UIå¿œç­”æ€§é‡è¦–ã®è¶…è»½é‡ä¸¦åˆ—å‡¦ç†ãƒ¯ãƒ¼ã‚«ãƒ¼
            def process_file_batch_ui_safe_with_progress(file_batch, file_category="light"):
                """UIå¿œç­”æ€§é‡è¦–ã®è¶…è»½é‡ä¸¦åˆ—å‡¦ç†ï¼ˆé€²æ—ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ä»˜ããƒ»2000ãƒ•ã‚¡ã‚¤ãƒ«/ç§’å¯¾å¿œï¼‰"""
                results = []
                
                # ã‚·ã‚¹ãƒ†ãƒ è² è·ãƒã‚§ãƒƒã‚¯ï¼ˆUIå¿œç­”æ€§é‡è¦–ï¼‰
                if not hasattr(self, '_cached_system_load') or time.time() - getattr(self, '_last_load_check', 0) > 10:
                    self._cached_system_load = self.get_current_system_load()
                    self._last_load_check = time.time()
                
                system_load = self._cached_system_load
                current_db_count = getattr(self.search_system, 'db_count', 8)
                
                # UIå¿œç­”æ€§é‡è¦–ã®ä¸¦åˆ—åº¦è¨­å®šï¼ˆç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢ï¼‰
                if file_category == "heavy":
                    optimal_workers = 2  # é‡ã„ãƒ•ã‚¡ã‚¤ãƒ«ã¯2ä¸¦åˆ—ï¼ˆ100%å¢—å¼·ï¼‰
                elif file_category == "medium":
                    # ã‚·ã‚¹ãƒ†ãƒ è² è·ã«å¿œã˜ã¦å‹•çš„èª¿æ•´
                    if system_load > 0.9:
                        optimal_workers = 2
                    elif system_load > 0.7:
                        optimal_workers = 4
                    else:
                        optimal_workers = max(2, current_db_count // 4)
                else:
                    # è»½é‡ãƒ•ã‚¡ã‚¤ãƒ«ã§ã‚‚UIå¿œç­”æ€§é‡è¦–
                    if system_load > 0.8:
                        optimal_workers = max(2, current_db_count // 6)
                    elif system_load > 0.6:
                        optimal_workers = max(4, current_db_count // 3)
                    else:
                        optimal_workers = max(8, current_db_count)
                
                # è¶…æ¥µé™ä¸¦åˆ—æ•°åˆ¶é™ï¼ˆ2000ãƒ•ã‚¡ã‚¤ãƒ«/ç§’ç›®æ¨™é”æˆï¼‰
                if system_load < 0.3:
                    max_workers = min(len(file_batch), optimal_workers, 96)   # è¶…ä½è² è·æ™‚ã¯96ä¸¦åˆ—ã¾ã§ï¼ˆ200%å¢—å¼·ï¼‰
                elif system_load < 0.5:
                    max_workers = min(len(file_batch), optimal_workers, 80)   # ä½è² è·æ™‚ã¯80ä¸¦åˆ—ã¾ã§ï¼ˆ167%å¢—å¼·ï¼‰
                elif system_load < 0.7:
                    max_workers = min(len(file_batch), optimal_workers, 64)   # ä¸­è² è·æ™‚ã¯64ä¸¦åˆ—ã¾ã§ï¼ˆ160%å¢—å¼·ï¼‰
                else:
                    max_workers = min(len(file_batch), optimal_workers, 48)   # é«˜è² è·æ™‚ã¯48ä¸¦åˆ—ã¾ã§ï¼ˆ150%å¢—å¼·ï¼‰
                
                # è¶…æ¥µé™ãƒ¢ãƒ¼ãƒ‰ï¼šãƒ—ãƒ­ã‚»ã‚¹ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’2000ãƒ•ã‚¡ã‚¤ãƒ«/ç§’å¯¾å¿œã«è¶…å¢—å¼·
                if file_category == "light":
                    process_batch_size = min(1000, len(file_batch))  # è»½é‡ãƒ•ã‚¡ã‚¤ãƒ«ã¯1000ãƒ•ã‚¡ã‚¤ãƒ«/ãƒãƒƒãƒï¼ˆ200%å¢—å¼·ï¼‰
                elif file_category == "medium":
                    process_batch_size = min(500, len(file_batch))   # ä¸­ç¨‹åº¦ãƒ•ã‚¡ã‚¤ãƒ«ã¯500ãƒ•ã‚¡ã‚¤ãƒ«/ãƒãƒƒãƒï¼ˆ200%å¢—å¼·ï¼‰
                else:
                    process_batch_size = min(100, len(file_batch))   # é‡ã„ãƒ•ã‚¡ã‚¤ãƒ«ã¯100ãƒ•ã‚¡ã‚¤ãƒ«/ãƒãƒƒãƒï¼ˆ200%å¢—å¼·ï¼‰
                
                # è¶…æ¥µé™æ€§èƒ½ãƒ¢ãƒ¼ãƒ‰ãƒ­ã‚°å‡ºåŠ›
                if len(file_batch) > 0:
                    print(f"ğŸš€ è¶…æ¥µé™2000ãƒ•ã‚¡ã‚¤ãƒ«/ç§’ãƒ¢ãƒ¼ãƒ‰ {file_category}: {max_workers}ä¸¦åˆ— (ãƒãƒƒãƒ:{process_batch_size}ãƒ•ã‚¡ã‚¤ãƒ«) - ç›®æ¨™: 2000ãƒ•ã‚¡ã‚¤ãƒ«/ç§’")
                
                for batch_start in range(0, len(file_batch), process_batch_size):
                    batch_end = min(batch_start + process_batch_size, len(file_batch))
                    current_batch = file_batch[batch_start:batch_end]
                    
                    try:
                        # å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ï¼ˆThreadPoolExecutorä½¿ç”¨ï¼‰
                        with ThreadPoolExecutor(max_workers=max_workers) as executor:
                            # å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å€‹åˆ¥ã«å‡¦ç†ï¼ˆãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æœ€å°åŒ–ï¼‰
                            futures = []
                            for file_path in current_batch:
                                future = executor.submit(
                                    self.process_single_file_with_progress, 
                                    str(file_path), 
                                    file_category
                                )
                                futures.append(future)
                            
                            # çµæœåé›†ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãï¼‰
                            timeout_seconds = {"light": 30, "medium": 60, "heavy": 180}.get(file_category, 45)
                            for future in futures:
                                try:
                                    result = future.result(timeout=timeout_seconds)
                                    if result:
                                        results.append(result)
                                except Exception as e:
                                    continue  # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’å‰Šæ¸›
                    
                    except Exception as e:
                        continue  # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’å‰Šæ¸›
                
                return results
            
            # ğŸ”¥ ãƒ¡ã‚¤ãƒ³ä¸¦åˆ—å‡¦ç†é–‹å§‹ï¼ˆé…å»¶ãªã—ã®å³åº§å®Ÿè¡Œï¼‰
            print("ğŸš€ ãƒ¡ã‚¤ãƒ³ä¸¦åˆ—å‡¦ç†é–‹å§‹...")
            safe_ui_update("ä¸¦åˆ—å‡¦ç†å®Ÿè¡Œä¸­...", force=True)
            
            # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®æœ€é©åŒ–ã•ã‚ŒãŸå‡¦ç†é †åºï¼ˆè»½é‡â†’ä¸­â†’é‡ã®é †ï¼‰
            all_categories = [
                ("light", light_files, 8),      # è»½é‡ãƒ•ã‚¡ã‚¤ãƒ«: 8ä¸¦åˆ—
                ("medium", medium_files, 4),    # ä¸­ç¨‹åº¦ãƒ•ã‚¡ã‚¤ãƒ«: 4ä¸¦åˆ—  
                ("heavy", heavy_files, 2)       # é‡é‡ãƒ•ã‚¡ã‚¤ãƒ«: 2ä¸¦åˆ—
            ]
            
            # ğŸ”¥ å³åº§ã«ä¸¦åˆ—å‡¦ç†å®Ÿè¡Œï¼ˆé…å»¶ãªã—ï¼‰
            total_processed = 0
            for category_name, file_list, max_workers in all_categories:
                if not file_list:
                    continue
                    
                print(f"ğŸ”„ {category_name}ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹: {len(file_list):,}ãƒ•ã‚¡ã‚¤ãƒ« ({max_workers}ä¸¦åˆ—)")
                
                # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å‹•çš„èª¿æ•´ï¼ˆ2000ãƒ•ã‚¡ã‚¤ãƒ«/ç§’å¯¾å¿œï¼‰
                if category_name == "light":
                    batch_size = min(1000, len(file_list))
                elif category_name == "medium":
                    batch_size = min(500, len(file_list))
                else:
                    batch_size = min(100, len(file_list))
                
                # å„ã‚«ãƒ†ã‚´ãƒªã‚’å³åº§ã«ä¸¦åˆ—å‡¦ç†
                for i in range(0, len(file_list), batch_size):
                    batch = file_list[i:i+batch_size]
                    batch_results = process_file_batch_ui_safe_with_progress(batch, category_name)
                    total_processed += len(batch)
                    
                    # é€²æ—æ›´æ–°
                    progress_pct = (total_processed / total_files) * 100
                    safe_ui_update(f"å‡¦ç†ä¸­: {total_processed:,}/{total_files:,} ({progress_pct:.1f}%)")
            
            # å‡¦ç†å®Œäº†
            safe_ui_update(f"å®Œäº†: {total_processed:,}ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†æ¸ˆã¿", force=True)
            print(f"âœ… ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‡¦ç†å®Œäº†: {total_processed:,}/{total_files:,}ãƒ•ã‚¡ã‚¤ãƒ«")
            
        except Exception as e:
            safe_ui_update(f"ã‚¨ãƒ©ãƒ¼: {str(e)}", force=True)
            print(f"âŒ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            
            # UIå¿œç­”æ€§é‡è¦–ã®è¶…è»½é‡ä¸¦åˆ—å‡¦ç†ãƒ¯ãƒ¼ã‚«ãƒ¼
            def process_file_batch_ui_safe(file_batch, file_category="light"):
                """UIå¿œç­”æ€§é‡è¦–ã®è¶…è»½é‡ä¸¦åˆ—å‡¦ç†"""
                results = []
                
                # ã‚·ã‚¹ãƒ†ãƒ è² è·ãƒã‚§ãƒƒã‚¯ï¼ˆUIå¿œç­”æ€§é‡è¦–ï¼‰
                if not hasattr(self, '_cached_system_load') or time.time() - getattr(self, '_last_load_check', 0) > 10:
                    self._cached_system_load = self.get_current_system_load()
                    self._last_load_check = time.time()
                
                system_load = self._cached_system_load
                current_db_count = getattr(self.search_system, 'db_count', 8)
                
                # UIå¿œç­”æ€§é‡è¦–ã®ä¸¦åˆ—åº¦è¨­å®šï¼ˆç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢ï¼‰
                if file_category == "heavy":
                    optimal_workers = 1  # é‡ã„ãƒ•ã‚¡ã‚¤ãƒ«ã¯ç¢ºå®Ÿã«1ã¤ãšã¤
                elif file_category == "medium":
                    # ã‚·ã‚¹ãƒ†ãƒ è² è·ã«å¿œã˜ã¦å‹•çš„èª¿æ•´
                    if system_load > 0.9:
                        optimal_workers = 1
                    elif system_load > 0.7:
                        optimal_workers = 2
                    else:
                        optimal_workers = max(1, current_db_count // 6)
                else:
                    # è»½é‡ãƒ•ã‚¡ã‚¤ãƒ«ã§ã‚‚UIå¿œç­”æ€§é‡è¦–
                    if system_load > 0.8:
                        optimal_workers = max(1, current_db_count // 8)
                    elif system_load > 0.6:
                        optimal_workers = max(2, current_db_count // 4)
                    else:
                        optimal_workers = max(4, current_db_count // 2)
                
                # è¶…æ¥µé™ä¸¦åˆ—æ•°åˆ¶é™ï¼ˆ1000ãƒ•ã‚¡ã‚¤ãƒ«/ç§’ç›®æ¨™é”æˆï¼‰
                if system_load < 0.5:
                    max_workers = min(len(file_batch), optimal_workers, 48)  # ä½è² è·æ™‚ã¯48ä¸¦åˆ—ã¾ã§ï¼ˆ100%å¢—å¼·ï¼‰
                elif system_load < 0.7:
                    max_workers = min(len(file_batch), optimal_workers, 40)  # ä¸­è² è·æ™‚ã¯40ä¸¦åˆ—ã¾ã§ï¼ˆ100%å¢—å¼·ï¼‰
                else:
                    max_workers = min(len(file_batch), optimal_workers, 32)  # é«˜è² è·æ™‚ã¯32ä¸¦åˆ—ã¾ã§ï¼ˆ100%å¢—å¼·ï¼‰
                
                # è¶…æ¥µé™ãƒ¢ãƒ¼ãƒ‰ï¼šãƒ—ãƒ­ã‚»ã‚¹ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’1000ãƒ•ã‚¡ã‚¤ãƒ«/ç§’å¯¾å¿œã«å¢—å¼·
                if file_category == "light":
                    process_batch_size = min(500, len(file_batch))  # è»½é‡ãƒ•ã‚¡ã‚¤ãƒ«ã¯500ãƒ•ã‚¡ã‚¤ãƒ«/ãƒãƒƒãƒï¼ˆ100%å¢—å¼·ï¼‰
                elif file_category == "medium":
                    process_batch_size = min(250, len(file_batch))  # ä¸­ç¨‹åº¦ãƒ•ã‚¡ã‚¤ãƒ«ã¯250ãƒ•ã‚¡ã‚¤ãƒ«/ãƒãƒƒãƒï¼ˆ100%å¢—å¼·ï¼‰
                else:
                    process_batch_size = min(50, len(file_batch))   # é‡ã„ãƒ•ã‚¡ã‚¤ãƒ«ã¯50ãƒ•ã‚¡ã‚¤ãƒ«/ãƒãƒƒãƒï¼ˆ100%å¢—å¼·ï¼‰
                
                # è¶…æ¥µé™æ€§èƒ½ãƒ¢ãƒ¼ãƒ‰ãƒ­ã‚°å‡ºåŠ›
                if len(file_batch) > 0:
                    print(f"ğŸš€ è¶…æ¥µé™1000ãƒ•ã‚¡ã‚¤ãƒ«/ç§’ãƒ¢ãƒ¼ãƒ‰ {file_category}: {max_workers}ä¸¦åˆ— (ãƒãƒƒãƒ:{process_batch_size}ãƒ•ã‚¡ã‚¤ãƒ«) - ç›®æ¨™: 1000ãƒ•ã‚¡ã‚¤ãƒ«/ç§’")
                
                for batch_start in range(0, len(file_batch), process_batch_size):
                    batch_end = min(batch_start + process_batch_size, len(file_batch))
                    current_batch = file_batch[batch_start:batch_end]
                    
                    try:
                        # å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ï¼ˆThreadPoolExecutorä½¿ç”¨ï¼‰
                        with ThreadPoolExecutor(max_workers=max_workers) as executor:
                            # å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å€‹åˆ¥ã«å‡¦ç†ï¼ˆãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æœ€å°åŒ–ï¼‰
                            futures = []
                            for file_path in current_batch:
                                future = executor.submit(
                                    self.search_system.live_progressive_index_file, 
                                    str(file_path)
                                )
                                futures.append(future)
                            
                            # çµæœåé›†ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãï¼‰
                            timeout_seconds = {"light": 30, "medium": 60, "heavy": 180}.get(file_category, 45)
                            for future in futures:
                                try:
                                    result = future.result(timeout=timeout_seconds)
                                    if result:
                                        results.append(result)
                                except Exception as e:
                                    continue  # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’å‰Šæ¸›
                    
                    except Exception as e:
                        continue  # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’å‰Šæ¸›
                
                return results

            # å„ªå…ˆé †ä½ä»˜ãå‡¦ç†: è»½é‡ â†’ ä¸­ç¨‹åº¦ â†’ é‡é‡ï¼ˆé€²æ—ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ä»˜ãï¼‰
            all_categorized_files = [
                (light_files, "light"),
                (medium_files, "medium"), 
                (heavy_files, "heavy")
            ]
            
            indexed_count = 0
            total_files = len(all_files)
            
            for file_list, category in all_categorized_files:
                if not file_list:
                    continue
                    
                safe_ui_update(f"{category}ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹: {len(file_list)}å€‹", force=True)
                print(f"ğŸš€ {category}ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹: {len(file_list)}å€‹ï¼ˆ2000ãƒ•ã‚¡ã‚¤ãƒ«/ç§’å¯¾å¿œãƒ¢ãƒ¼ãƒ‰ï¼‰")
                
                # è¶…æ¥µé™ãƒ¢ãƒ¼ãƒ‰ï¼šãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’2000ãƒ•ã‚¡ã‚¤ãƒ«/ç§’å¯¾å¿œã«è¶…å¼·åŒ–
                if category == "heavy":
                    batch_size = 32   # é‡ã„ãƒ•ã‚¡ã‚¤ãƒ«ã¯32å€‹ãšã¤ã«è¶…å¼·åŒ–ï¼ˆ200%å¢—ï¼‰
                elif category == "medium":
                    batch_size = 100  # ä¸­ç¨‹åº¦ãƒ•ã‚¡ã‚¤ãƒ«ã¯100å€‹ãšã¤ã«è¶…å¼·åŒ–ï¼ˆ200%å¢—ï¼‰
                else:
                    batch_size = 300  # è»½é‡ãƒ•ã‚¡ã‚¤ãƒ«ã¯300å€‹ãšã¤ã«è¶…æ¥µé™å¼·åŒ–ï¼ˆ200%å¢—ï¼‰
                
                for i in range(0, len(file_list), batch_size):
                    batch = file_list[i:i+batch_size]
                    
                    # UIå¿œç­”æ€§é‡è¦–å‡¦ç†å®Ÿè¡Œï¼ˆé€²æ—ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ä»˜ãï¼‰
                    batch_results = process_file_batch_ui_safe_with_progress(batch, category)
                    
                    indexed_count += len(batch)
                    progress = int(indexed_count / total_files * 100) if total_files > 0 else 100
                    
                    # è¶…æ¥µé™ãƒ¢ãƒ¼ãƒ‰æ›´æ–°é »åº¦ï¼šUIè² è·ã‚’2000ãƒ•ã‚¡ã‚¤ãƒ«/ç§’å¯¾å¿œã«æœ€å°åŒ–
                    if indexed_count % 2000 == 0 or indexed_count == total_files:
                        safe_ui_update(f"è¶…æ¥µé™2000ãƒ•ã‚¡ã‚¤ãƒ«/ç§’å‡¦ç†ä¸­: {indexed_count:,}/{total_files:,} ({progress}%) - {category}ãƒ•ã‚¡ã‚¤ãƒ«")
                    
                    # è¶…æ¥µé™ãƒ¢ãƒ¼ãƒ‰ï¼šå‡¦ç†é–“ã®å¾…æ©Ÿæ™‚é–“ã‚’å®Œå…¨é™¤å»ï¼ˆ1000ãƒ•ã‚¡ã‚¤ãƒ«/ç§’å¯¾å¿œï¼‰
                    # å¾…æ©Ÿæ™‚é–“ã¯ã™ã¹ã¦å‰Šé™¤æ¸ˆã¿            # å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆè©³ç´°æƒ…å ±ä»˜ãï¼‰
            end_time = time.time()
            total_time = end_time - start_time
            
            completion_msg = f"âœ… ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å®Œäº†: {total_files:,}ãƒ•ã‚¡ã‚¤ãƒ«"
            if total_time > 0:
                completion_msg += f" ({total_time:.1f}ç§’)"
            
            safe_ui_update(completion_msg, force=True)
            print(f"ğŸ‰ å¤§å®¹é‡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å®Œäº†: {total_files:,}ãƒ•ã‚¡ã‚¤ãƒ« ({target_name}) - æ‰€è¦æ™‚é–“: {total_time:.1f}ç§’")
            
            # å‡¦ç†çµ±è¨ˆè¡¨ç¤º
            if light_files or medium_files or heavy_files:
                print(f"ğŸ“Š å‡¦ç†å†…è¨³: è»½é‡{len(light_files)}å€‹, ä¸­ç¨‹åº¦{len(medium_files)}å€‹, é‡é‡{len(heavy_files)}å€‹")
            
        except Exception as e:
            error_msg = f"âŒ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚¨ãƒ©ãƒ¼: {e}"
            safe_ui_update(error_msg, force=True)
            print(f"âŒ å¤§å®¹é‡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
            
        finally:
            # é€²æ—ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’é–‰ã˜ã‚‹
            self.root.after(0, lambda: self.progress_window.destroy() if self.progress_window and self.progress_window.winfo_exists() else None)
            
            # UIå¾©å…ƒï¼ˆç¢ºå®Ÿã«å®Ÿè¡Œï¼‰
            self.bulk_indexing_active = False
            self.indexing_cancelled = False  # ã‚­ãƒ£ãƒ³ã‚»ãƒ«ãƒ•ãƒ©ã‚°ãƒªã‚»ãƒƒãƒˆ
            self.root.after(0, lambda: self.bulk_index_btn.config(state="normal", text="ğŸš€ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é–‹å§‹"))
            self.root.after(0, lambda: self.cancel_index_btn.config(state="disabled"))  # ã‚­ãƒ£ãƒ³ã‚»ãƒ«ãƒœã‚¿ãƒ³ç„¡åŠ¹åŒ–
            print("ğŸ”§ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²æ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‡¦ç†å®Œäº†ã€UIå¾©å…ƒå®Œäº†")

    def on_closing(self):
        """ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ãŒé–‰ã˜ã‚‰ã‚Œã‚‹ã¨ãã®å‡¦ç†"""
        try:
            print("ğŸ”„ ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³çµ‚äº†å‡¦ç†é–‹å§‹...")
            
            # æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³
            if hasattr(self.search_system, 'shutdown'):
                self.search_system.shutdown()
            
            # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’ç ´æ£„
            self.root.quit()
            self.root.destroy()
            
        except Exception as e:
            print(f"âš ï¸ çµ‚äº†å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            # å¼·åˆ¶çµ‚äº†
            try:
                self.root.quit()
            except:
                pass


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•° - æœ€å¤§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç‰ˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•"""
    try:
        print("ğŸš€ 100%ä»•æ§˜é©åˆ æœ€å¤§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å…¨æ–‡æ¤œç´¢ã‚¢ãƒ—ãƒªèµ·å‹•é–‹å§‹")
        debug_logger.info("æœ€å¤§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç‰ˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•é–‹å§‹")
        
        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±è¡¨ç¤º
        try:
            import psutil
            physical_cores = psutil.cpu_count(logical=False)
            logical_cores = psutil.cpu_count(logical=True)
            memory_gb = psutil.virtual_memory().total / (1024**3)
            print(f"ğŸ’» ã‚·ã‚¹ãƒ†ãƒ ä»•æ§˜: {physical_cores}ç‰©ç†ã‚³ã‚¢/{logical_cores}è«–ç†ã‚³ã‚¢, {memory_gb:.1f}GB RAM")
        except:
            print("ğŸ’» ã‚·ã‚¹ãƒ†ãƒ ä»•æ§˜: è©³ç´°æƒ…å ±å–å¾—ä¸å¯")
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆè¨­å®šï¼ˆEXEåŒ–å¯¾å¿œï¼‰
        if getattr(sys, 'frozen', False):
            # PyInstallerã§EXEåŒ–ã•ã‚Œã¦ã„ã‚‹å ´åˆ
            # å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ.exeï¼‰ã®ã‚ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å–å¾—
            project_root = os.path.dirname(sys.executable)
        else:
            # é€šå¸¸ã®Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨ã—ã¦å®Ÿè¡Œã•ã‚Œã¦ã„ã‚‹å ´åˆ
            current_file_dir = os.path.dirname(os.path.abspath(__file__))
            project_root = current_file_dir
        print(f"ğŸ“ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆ: {project_root}")
        print(f"ğŸ”§ å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰: {'EXEç‰ˆ' if getattr(sys, 'frozen', False) else 'ã‚¹ã‚¯ãƒªãƒ—ãƒˆç‰ˆ'}")
        
        # æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ï¼ˆæœ€å¤§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨­å®šï¼‰
        print("ğŸ”§ æœ€å¤§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ä¸­...")
        search_system = UltraFastFullCompliantSearchSystem(project_root)
        print("âœ… æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        
        # ã‚·ã‚¹ãƒ†ãƒ è¨­å®šã‚µãƒãƒªãƒ¼è¡¨ç¤º
        print(f"âš¡ è¶…æ¥µé™1000ãƒ•ã‚¡ã‚¤ãƒ«/ç§’è¨­å®š:")
        print(f"  - ä½¿ç”¨ã‚¹ãƒ¬ãƒƒãƒ‰æ•°: {search_system.optimal_threads}")
        print(f"  - ãƒãƒƒãƒã‚µã‚¤ã‚º: {search_system.batch_size}")
        print(f"  - å³åº§å±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥: {search_system.max_immediate_cache:,}")
        print(f"  - é«˜é€Ÿå±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥: {search_system.max_hot_cache:,}")
        print(f"  - å¢—åˆ†ã‚¹ã‚­ãƒ£ãƒ³é–“éš”: {search_system.incremental_scan_interval}ç§’")
        
        # UIåˆæœŸåŒ–
        print("ğŸ¨ UIåˆæœŸåŒ–ä¸­...")
        app = UltraFastCompliantUI(search_system)
        print("âœ… UIåˆæœŸåŒ–å®Œäº†")
        
        # OCRã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆUIãŒåˆæœŸåŒ–ã•ã‚ŒãŸå¾Œã«å®Ÿè¡Œï¼‰
        if ocr_setup_needed:
            print("ğŸ” OCRæ©Ÿèƒ½ã®è‡ªå‹•ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚’é–‹å§‹...")
            # UIåˆæœŸåŒ–å¾Œã®é…å»¶å®Ÿè¡Œ
            app.root.after(1000, lambda: threading.Thread(target=auto_install_tesseract_engine, daemon=True).start())
        
        # åˆæœŸçµ±è¨ˆè¡¨ç¤º
        initial_stats = search_system.get_comprehensive_statistics()
        layer_stats = initial_stats.get('layer_statistics', {})
        print(f"ğŸ“Š åˆæœŸçµ±è¨ˆ: immediate={layer_stats.get('immediate_layer', 0)}, "
              f"hot={layer_stats.get('hot_layer', 0)}, complete={layer_stats.get('complete_layer', 0)}")
        
        print("ğŸ¯ è¶…æ¥µé™1000ãƒ•ã‚¡ã‚¤ãƒ«/ç§’ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³æº–å‚™å®Œäº† - UIã‚’è¡¨ç¤ºã—ã¾ã™")
        print("ğŸ’¡ è¶…ä¸¦åˆ—å‡¦ç†ã€ãƒ¡ã‚¬ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–ã€ã‚¼ãƒ­å¾…æ©Ÿæ™‚é–“ãŒæœ‰åŠ¹ã§ã™")
        debug_logger.info("æœ€å¤§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç‰ˆUIãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—é–‹å§‹")
        
        # èµ·å‹•å¾Œã«çµ±è¨ˆã‚’ç¢ºå®Ÿã«æ›´æ–°ï¼ˆå®Œå…¨å±¤ã‚«ã‚¦ãƒ³ãƒˆä¿®æ­£ï¼‰
        print("ğŸ“ˆ å®Œå…¨å±¤çµ±è¨ˆã‚’æœ€æ–°çŠ¶æ…‹ã«æ›´æ–°ä¸­...")
        app.root.after(1000, app.update_statistics)  # 1ç§’å¾Œã«çµ±è¨ˆæ›´æ–°
        app.root.after(3000, app.update_statistics)  # 3ç§’å¾Œã«ã‚‚å†æ›´æ–°ï¼ˆå®‰å…¨æ€§ç¢ºä¿ï¼‰
        
        # UIãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—é–‹å§‹
        app.root.mainloop()
        
    except Exception as e:
        print(f"âŒ ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•ã‚¨ãƒ©ãƒ¼: {e}")
        debug_logger.error(f"ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
