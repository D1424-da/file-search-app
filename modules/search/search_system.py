#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã‚³ã‚¢
3å±¤çµ±åˆæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®ä¸­æ ¸éƒ¨åˆ†
"""

import os
import time
import threading
import hashlib
from pathlib import Path
from typing import Dict, Any, List, Optional, Callable
from concurrent.futures import ThreadPoolExecutor, as_completed

from .database_manager import DatabaseManager
from .cache_manager import CacheManager
from ..extractors import FileContentExtractor
from ..utils import setup_debug_logger, enhanced_search_match, normalize_search_text_ultra

# ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚¬ãƒ¼
debug_logger = setup_debug_logger('UltraFastSearchSystem')


class UltraFastFullCompliantSearchSystem:
    """3å±¤çµ±åˆæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ï¼ˆå³åº§å±¤ãƒ»é«˜é€Ÿå±¤ãƒ»å®Œå…¨å±¤ï¼‰"""
    
    def __init__(self, project_root: str, db_count: int = 8, optimal_threads: int = 8):
        self.project_root = Path(project_root)
        self.db_count = db_count
        self.optimal_threads = optimal_threads
        
        # çµ±è¨ˆæƒ…å ±
        self.stats = {
            "indexed_files": 0,
            "search_count": 0,
            "immediate_layer_hits": 0,
            "hot_layer_hits": 0,
            "complete_layer_hits": 0,
            "total_search_time": 0,
            "avg_search_time": 0,
            "error_count": 0,
            "optimization_count": 0
        }
        
        # çŠ¶æ…‹ç®¡ç†
        self.indexing_in_progress = False
        self.indexing_cancelled = False
        self.shutdown_requested = False
        
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
        self.database_manager = DatabaseManager(project_root, db_count)
        self.cache_manager = CacheManager(project_root)
        self.content_extractor = FileContentExtractor()
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹ï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰
        self.complete_db_paths = self.database_manager.complete_db_paths
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥å‚ç…§ï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰
        self.immediate_cache = self.cache_manager.immediate_cache
        self.hot_cache = self.cache_manager.hot_cache
        self.max_immediate_cache = self.cache_manager.max_immediate_cache
        
        # åˆæœŸåŒ–å®Ÿè¡Œ
        self.initialize_databases()
    
    def initialize_databases(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–"""
        print("ğŸ—„ï¸ 3å±¤çµ±åˆæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ä¸­...")
        debug_logger.info("3å±¤çµ±åˆæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–é–‹å§‹")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–
        self.database_manager.initialize_databases()
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¾©å…ƒ
        self.cache_manager.load_caches()
        
        print(f"âœ… 3å±¤çµ±åˆæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº† (DB:{self.db_count}å€‹, ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¾©å…ƒå®Œäº†)")
        debug_logger.info("3å±¤çµ±åˆæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    def ultra_fast_search(self, query: str, max_results: int = 5500) -> List[Dict[str, Any]]:
        """æœ€é©åŒ–æ¸ˆã¿æ¤œç´¢ãƒ¡ã‚½ãƒƒãƒ‰ - 3å±¤æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ """
        if not query or not query.strip():
            return []

        query = query.strip()
        start_time = time.time()

        # çµ±è¨ˆæ›´æ–°ï¼ˆè»½é‡åŒ–ï¼‰
        self.stats["search_count"] += 1

        try:
            # ç¬¬1å±¤: å³åº§å±¤æ¤œç´¢ï¼ˆæœ€å„ªå…ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
            immediate_results = self._search_immediate_layer(query)
            if immediate_results:
                self.stats["immediate_layer_hits"] += 1
                self.stats["total_search_time"] += time.time() - start_time
                self._update_average_search_time()
                return immediate_results[:max_results]

            # ç¬¬2å±¤: ãƒ›ãƒƒãƒˆå±¤æ¤œç´¢ï¼ˆä¸€æ™‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
            hot_results = self._search_hot_layer(query)
            if hot_results:
                self.stats["hot_layer_hits"] += 1
                self.stats["total_search_time"] += time.time() - start_time
                self._update_average_search_time()
                
                # å³åº§å±¤ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆéåŒæœŸï¼‰
                threading.Timer(0.001, self._cache_search_results, args=[query, hot_results]).start()
                return hot_results[:max_results]

            # ç¬¬3å±¤: å®Œå…¨æ¤œç´¢ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼‰
            complete_results = self._search_complete_layer(query, max_results)
            self.stats["complete_layer_hits"] += 1
            self.stats["total_search_time"] += time.time() - start_time
            self._update_average_search_time()

            # çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«è¿½åŠ ï¼ˆéåŒæœŸï¼‰
            if complete_results:
                threading.Timer(0.001, self._cache_search_results, args=[query, complete_results]).start()

            return complete_results

        except Exception as e:
            error_time = time.time() - start_time
            self.stats["error_count"] += 1
            self.stats["total_search_time"] += error_time
            debug_logger.error(f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e} ({error_time:.3f}s)")
            print(f"âš ï¸ æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def unified_three_layer_search(self,
                                   query: str,
                                   max_results: int = 5500,
                                   file_type_filter: str = "all") -> List[Dict[str, Any]]:
        """æœ€é©åŒ–æ¸ˆã¿3å±¤çµ±åˆæ¤œç´¢ - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹é‡è¦–ç‰ˆ"""
        start_time = time.time()
        results = []

        try:
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸­ã®å‹•ä½œåˆ¶å¾¡ï¼ˆè»½é‡åŒ–ï¼‰
            if self.indexing_in_progress:
                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸­ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥å„ªå…ˆã§é«˜é€Ÿæ¤œç´¢
                results.extend(self._search_immediate_layer(query)[:max_results // 2] or [])
                results.extend(self._search_hot_layer(query)[:max_results // 2] or [])
                
                # çµæœãŒä¸ååˆ†ãªå ´åˆã®ã¿DBæ¤œç´¢
                if len(results) < max_results // 4:
                    try:
                        db_results = self._search_complete_layer(query, max_results // 4)
                        if db_results:
                            results.extend(db_results)
                    except Exception:
                        pass  # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸­ã®DBæ¤œç´¢ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–
                        
            else:
                # é€šå¸¸æ™‚ï¼šæœ€é©åŒ–ã•ã‚ŒãŸ3å±¤æ¤œç´¢
                # å®Œå…¨å±¤å„ªå…ˆæ¤œç´¢ï¼ˆæœ€æ–°ãƒ»æ­£ç¢ºï¼‰
                complete_results = self._search_complete_layer(query, max_results // 2) or []
                results.extend(complete_results)

                # å³åº§å±¤ã§è£œå®Œ
                immediate_results = self._search_immediate_layer(query) or []
                results.extend(immediate_results[:max_results // 4])

                # é«˜é€Ÿå±¤ã§è£œå®Œ
                hot_results = self._search_hot_layer(query) or []
                results.extend(hot_results[:max_results // 4])

            # é‡è¤‡é™¤å»ã¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
            unique_results = self._deduplicate_and_rank_optimized(results)

            # ãƒ•ã‚¡ã‚¤ãƒ«ç¨®é¡ãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨
            if file_type_filter != "all":
                filtered_results = []
                for result in unique_results:
                    file_path = result.get('file_path', '')
                    if file_path.lower().endswith(file_type_filter.lower()):
                        filtered_results.append(result)
                unique_results = filtered_results

            # çµ±è¨ˆæ›´æ–°
            search_time = time.time() - start_time
            self.stats["search_count"] += 1
            self.stats["avg_search_time"] = ((self.stats["avg_search_time"] *
                                              (self.stats["search_count"] - 1) + search_time) /
                                             self.stats["search_count"])

            # è‡ªå‹•æœ€é©åŒ–ãƒã‚§ãƒƒã‚¯ï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸­ä»¥å¤–ï¼‰
            if not self.indexing_in_progress:
                self.check_auto_optimization()

            # æ¤œç´¢çµæœã®å‡ºåŠ›ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            status_msg = "ğŸ“¦ [ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸­]" if self.indexing_in_progress else "âœ… [å®Œäº†]"
            cache_msg = f" ã‚­ãƒ£ãƒƒã‚·ãƒ¥:{len(results) - len(unique_results)}"
            
            # ãƒ¬ã‚¤ãƒ¤ãƒ¼åˆ¥çµæœä»¶æ•°ã‚’è¨ˆç®—ï¼ˆå®Œå…¨å±¤å„ªå…ˆè¡¨ç¤ºï¼‰
            layer_counts = {}
            for result in unique_results:
                layer = result.get('layer', 'unknown')
                if layer.startswith('complete'):
                    layer_key = 'complete'
                else:
                    layer_key = layer
                layer_counts[layer_key] = layer_counts.get(layer_key, 0) + 1
            
            # å®Œå…¨å±¤ã‚’æœ€åˆã«è¡¨ç¤ºã™ã‚‹é †åºã§ä¸¦ã¹æ›¿ãˆ
            ordered_layers = ['complete', 'immediate', 'hot']
            layer_parts = []
            for layer in ordered_layers:
                if layer in layer_counts:
                    layer_parts.append(f"{layer}:{layer_counts[layer]}")
            # ãã®ä»–ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒã‚ã‚Œã°è¿½åŠ 
            for layer, count in layer_counts.items():
                if layer not in ordered_layers:
                    layer_parts.append(f"{layer}:{count}")
            
            layer_msg = " / ".join(layer_parts)
            print(f"ğŸ” {status_msg} 3å±¤çµ±åˆæ¤œç´¢: {len(unique_results)}ä»¶ ({search_time:.4f}ç§’) [ãƒ•ã‚£ãƒ«ã‚¿: {file_type_filter}]{cache_msg} [{layer_msg}]")
            
            return unique_results[:max_results]

        except Exception as e:
            print(f"âŒ çµ±åˆæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def _search_immediate_layer(self, query: str) -> List[Dict[str, Any]]:
        """å³åº§å±¤æ¤œç´¢"""
        return self.cache_manager.search_immediate_layer(query)
    
    def _search_hot_layer(self, query: str) -> List[Dict[str, Any]]:
        """é«˜é€Ÿå±¤æ¤œç´¢"""
        return self.cache_manager.search_hot_layer(query)
    
    def _search_complete_layer(self, query: str, max_results: int) -> List[Dict[str, Any]]:
        """å®Œå…¨å±¤æ¤œç´¢"""
        return self.database_manager.parallel_search(query, max_results)
    
    def _cache_search_results(self, query: str, results: List[Dict[str, Any]]):
        """æ¤œç´¢çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        try:
            # å³åº§å±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¸è¿½åŠ 
            if len(self.immediate_cache) < self.max_immediate_cache:
                self.immediate_cache[query] = results.copy()
            else:
                # LRUçš„å‰Šé™¤ï¼ˆæœ€åˆã®ã‚­ãƒ¼ã‚’å‰Šé™¤ï¼‰
                oldest_key = next(iter(self.immediate_cache))
                del self.immediate_cache[oldest_key]
                self.immediate_cache[query] = results.copy()
                
        except Exception as e:
            debug_logger.warning(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _update_average_search_time(self):
        """å¹³å‡æ¤œç´¢æ™‚é–“ã‚’æ›´æ–°"""
        if self.stats["search_count"] > 0:
            self.stats["avg_search_time"] = self.stats["total_search_time"] / self.stats["search_count"]
    
    def _deduplicate_and_rank_optimized(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æœ€é©åŒ–ç‰ˆé‡è¤‡é™¤å»ã¨ãƒ©ãƒ³ã‚­ãƒ³ã‚° - é«˜é€ŸåŒ–é‡è¦–"""
        if not results:
            return []
            
        seen_paths = set()
        unique_results = []
        
        # ãƒ¬ã‚¤ãƒ¤ãƒ¼å„ªå…ˆåº¦ã‚’äº‹å‰è¨ˆç®—
        priority_map = {
            'complete': 1000,
            'immediate': 100, 
            'hot': 10
        }
        
        # ãƒ¬ã‚¤ãƒ¤ãƒ¼åã‹ã‚‰ã®å„ªå…ˆåº¦å–å¾—ï¼ˆæœ€é©åŒ–ï¼‰
        def get_priority(result):
            if not isinstance(result, dict):
                return (0, 0)
            layer = result.get('layer', 'unknown')
            # complete_db_0ç­‰ã®å ´åˆã¯completeã¨ã—ã¦æ‰±ã†
            layer_base = layer.split('_')[0] if '_' in layer else layer
            priority = priority_map.get(layer_base, 1)
            score = result.get('relevance_score', 0)
            return (priority, score)
        
        # ã‚½ãƒ¼ãƒˆï¼ˆæœ€é©åŒ–ï¼‰
        results.sort(key=get_priority, reverse=True)
        
        # é‡è¤‡é™¤å»ï¼ˆæœ€é©åŒ–ï¼‰
        for result in results:
            if isinstance(result, dict) and 'file_path' in result:
                path = result['file_path']
                if path not in seen_paths:
                    seen_paths.add(path)
                    unique_results.append(result)

        return unique_results
    
    def live_progressive_index_file(self, file_path: str) -> bool:
        """ãƒ©ã‚¤ãƒ–ãƒ—ãƒ­ã‚°ãƒ¬ãƒƒã‚·ãƒ–ãƒ•ã‚¡ã‚¤ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹"""
        debug_logger.debug(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é–‹å§‹: {file_path}")

        # ã‚­ãƒ£ãƒ³ã‚»ãƒ«ãƒã‚§ãƒƒã‚¯
        if hasattr(self, 'indexing_cancelled') and self.indexing_cancelled:
            debug_logger.debug(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‡¦ç†ãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ: {file_path}")
            return False

        try:
            file_path_obj = Path(file_path)

            # macOSéš ã—ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ._ã§å§‹ã¾ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ã‚’ã‚¹ã‚­ãƒƒãƒ—
            if file_path_obj.name.startswith('._'):
                debug_logger.debug(f"macOSéš ã—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—: {file_path_obj.name}")
                return False

            # ãã®ä»–ã®éš ã—ãƒ•ã‚¡ã‚¤ãƒ«ãƒ»ã‚·ã‚¹ãƒ†ãƒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ã‚¹ã‚­ãƒƒãƒ—
            if file_path_obj.name.startswith('.DS_Store') or file_path_obj.name.startswith('Thumbs.db'):
                debug_logger.debug(f"ã‚·ã‚¹ãƒ†ãƒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—: {file_path_obj.name}")
                return False

            if not file_path_obj.exists():
                debug_logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {file_path}")
                return False

            # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±å–å¾—
            stat = file_path_obj.stat()
            file_size = stat.st_size
            modified_time = stat.st_mtime

            debug_logger.debug(f"ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ± - ã‚µã‚¤ã‚º: {file_size}, æ›´æ–°æ™‚åˆ»: {modified_time}")

            # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹æŠ½å‡º
            debug_logger.debug(f"ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºé–‹å§‹: {file_path}")
            content = self.content_extractor.extract_content(file_path)
            if not content:
                debug_logger.warning(f"ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒç©ºã¾ãŸã¯æŠ½å‡ºå¤±æ•—: {file_path}")
                return False

            debug_logger.info(f"ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºæˆåŠŸ: {file_path} ({len(content)}æ–‡å­—)")
            file_hash = hashlib.md5(content.encode('utf-8', errors='ignore')).hexdigest()
            debug_logger.debug(f"ãƒãƒƒã‚·ãƒ¥è¨ˆç®—å®Œäº†: {file_hash[:8]}...")

            # 3å±¤æ§‹é€ æœ€é©åŒ–å‡¦ç†
            file_data = {
                'file_name': file_path_obj.name,
                'file_type': file_path_obj.suffix.lower(),
                'size': file_size
            }

            # å³åº§å±¤ã«è¿½åŠ 
            self.cache_manager.add_to_immediate_cache(str(file_path), content, file_data)

            # å®Œå…¨å±¤ã¸ã®è¿½åŠ ï¼ˆéåŒæœŸï¼‰
            threading.Timer(5.0, self._move_to_complete_layer,
                            args=[file_path, content, file_hash]).start()

            self.stats["indexed_files"] += 1
            debug_logger.info(f"3å±¤æ§‹é€ æœ€é©åŒ–ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å®Œäº†: {file_path}")
            return True

        except Exception as e:
            debug_logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
            return False
    
    def _move_to_complete_layer(self, file_path: str, content: str, file_hash: str):
        """å®Œå…¨å±¤ç§»å‹•ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜ï¼‰"""
        try:
            if self.shutdown_requested:
                return
                
            file_path_obj = Path(file_path)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±å†å–å¾—
            stat = file_path_obj.stat()
            file_data = {
                'file_path': str(file_path),
                'file_name': file_path_obj.name,
                'content': content,
                'file_type': file_path_obj.suffix.lower(),
                'size': stat.st_size,
                'modified_time': stat.st_mtime,
                'indexed_time': time.time(),
                'hash': file_hash
            }
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
            success = self.database_manager.store_document(file_data)
            
            if success:
                debug_logger.debug(f"å®Œå…¨å±¤ä¿å­˜æˆåŠŸ: {os.path.basename(file_path)}")
            else:
                debug_logger.warning(f"å®Œå…¨å±¤ä¿å­˜å¤±æ•—: {os.path.basename(file_path)}")
            
        except Exception as e:
            debug_logger.error(f"å®Œå…¨å±¤ç§»å‹•ã‚¨ãƒ©ãƒ¼: {e}")
    
    def bulk_index_directory_with_progress(self, directory_path: str, 
                                           progress_callback: Optional[Callable] = None) -> Dict[str, Any]:
        """é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯å¯¾å¿œãƒ•ã‚©ãƒ«ãƒ€ä¸€æ‹¬ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹"""
        try:
            self.indexing_in_progress = True
            self.indexing_cancelled = False
            
            start_time = time.time()
            
            # ã‚µãƒãƒ¼ãƒˆã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­
            supported_extensions = {
                '.txt', '.pdf', '.docx', '.xlsx', '.tif', '.tiff',
                '.doc', '.xls', '.ppt', '.pptx',
                '.dot', '.dotx', '.dotm', '.docm',
                '.xlt', '.xltx', '.xltm', '.xlsm', '.xlsb',
                '.zip'
            }
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åé›†
            all_files = []
            for root, dirs, files in os.walk(directory_path):
                for file in files:
                    if any(file.lower().endswith(ext) for ext in supported_extensions):
                        all_files.append(os.path.join(root, file))
            
            if not all_files:
                return {
                    'success_count': 0,
                    'total_files': 0,
                    'total_time': 0,
                    'files_per_second': 0
                }
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åˆ†é¡
            light_files, medium_files, heavy_files = self._categorize_files_by_size(all_files)
            
            # é€²æ—æƒ…å ±åˆæœŸåŒ–
            if progress_callback:
                progress_callback(
                    total_files=len(all_files),
                    category_totals={'light': len(light_files), 'medium': len(medium_files), 'heavy': len(heavy_files)}
                )
            
            # ä¸¦åˆ—å‡¦ç†ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            success_count = 0
            
            # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«é †æ¬¡å‡¦ç†ï¼ˆè»½é‡â†’ä¸­ç¨‹åº¦â†’é‡ã„ï¼‰
            for category, files in [('light', light_files), ('medium', medium_files), ('heavy', heavy_files)]:
                if self.indexing_cancelled:
                    break
                    
                with ThreadPoolExecutor(max_workers=self.optimal_threads) as executor:
                    futures = {executor.submit(self._process_single_file_with_progress, 
                                              file_path, category, progress_callback): file_path 
                              for file_path in files}
                    
                    for future in as_completed(futures):
                        if self.indexing_cancelled:
                            break
                            
                        try:
                            result = future.result(timeout=30.0)
                            if result:
                                success_count += 1
                        except Exception as e:
                            debug_logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            
            total_time = time.time() - start_time
            files_per_second = success_count / total_time if total_time > 0 else 0
            
            return {
                'success_count': success_count,
                'total_files': len(all_files),
                'total_time': total_time,
                'files_per_second': files_per_second
            }
            
        except Exception as e:
            debug_logger.error(f"ä¸€æ‹¬ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'success_count': 0,
                'total_files': 0,
                'total_time': 0,
                'files_per_second': 0
            }
        finally:
            self.indexing_in_progress = False
    
    def _categorize_files_by_size(self, files: List[str]) -> tuple:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã«ã‚ˆã‚‹åˆ†é¡"""
        light_files = []    # <10MB
        medium_files = []   # 10MB-100MB  
        heavy_files = []    # >100MB
        
        for file_path in files:
            try:
                size_bytes = Path(file_path).stat().st_size
                if size_bytes < 10 * 1024 * 1024:  # 10MB
                    light_files.append(file_path)
                elif size_bytes < 100 * 1024 * 1024:  # 100MB
                    medium_files.append(file_path)
                else:
                    heavy_files.append(file_path)
            except:
                light_files.append(file_path)  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯è»½é‡æ‰±ã„
        
        return light_files, medium_files, heavy_files
    
    def _process_single_file_with_progress(self, file_path: str, category: str, 
                                           progress_callback: Optional[Callable] = None) -> bool:
        """é€²æ—ä»˜ãå˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†"""
        try:
            # é€²æ—æ›´æ–°
            if progress_callback:
                progress_callback(current_file=file_path, category=category, success=True)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
            result = self.live_progressive_index_file(file_path)
            
            return result
        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼ã‚‚é€²æ—ã«è¨˜éŒ²
            if progress_callback:
                progress_callback(current_file=file_path, category=category, success=False)
            return False
    
    def check_auto_optimization(self):
        """è‡ªå‹•æœ€é©åŒ–ãƒã‚§ãƒƒã‚¯"""
        try:
            # æ¤œç´¢æ•°ãŒå¤šã„å ´åˆã«æœ€é©åŒ–ã‚’å®Ÿè¡Œ
            if self.stats["search_count"] % 1000 == 0 and self.stats["search_count"] > 0:
                self.stats["optimization_count"] += 1
                debug_logger.info(f"è‡ªå‹•æœ€é©åŒ–å®Ÿè¡Œ ({self.stats['search_count']}å›æ¤œç´¢å¾Œ)")
        except Exception as e:
            debug_logger.error(f"è‡ªå‹•æœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
    
    def get_comprehensive_statistics(self) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„çµ±è¨ˆæƒ…å ±å–å¾—"""
        return {
            "indexed_files": self.stats["indexed_files"],
            "search_count": self.stats["search_count"],
            "layer_hits": {
                "immediate": self.stats["immediate_layer_hits"],
                "hot": self.stats["hot_layer_hits"],
                "complete": self.stats["complete_layer_hits"]
            },
            "search_performance": {
                "avg_search_time": self.stats["avg_search_time"],
                "total_search_time": self.stats["total_search_time"]
            },
            "cache_statistics": self.cache_manager.get_cache_statistics(),
            "database_statistics": self.database_manager.get_statistics()
        }
    
    def get_optimization_statistics(self) -> Dict[str, Any]:
        """æœ€é©åŒ–çµ±è¨ˆæƒ…å ±å–å¾—"""
        return {
            "optimization_count": self.stats["optimization_count"],
            "error_count": self.stats["error_count"],
            "system_status": {
                "indexing_in_progress": self.indexing_in_progress,
                "databases_available": len(self.complete_db_paths),
                "optimal_threads": self.optimal_threads
            }
        }
    
    def save_caches(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜"""
        self.cache_manager.save_caches()
    
    def load_caches(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¾©å…ƒ"""
        self.cache_manager.load_caches()
    
    def clear_cache(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢"""
        self.cache_manager.clear_cache()
    
    def shutdown(self):
        """ã‚·ã‚¹ãƒ†ãƒ ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³"""
        try:
            self.shutdown_requested = True
            self.indexing_cancelled = True
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
            self.cache_manager.shutdown()
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            self.database_manager.shutdown()
            
            debug_logger.info("æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³å®Œäº†")
            
        except Exception as e:
            debug_logger.error(f"ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ã‚¨ãƒ©ãƒ¼: {e}")